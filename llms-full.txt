=======================
Welcome to Binero cloud
=======================

Get your own **virtual data center** today on `Binero cloud <https://binero.com/public-cloud-platform/public-cloud/>`_!

Start by `registering an account with us <https://portal.binero.com/cart.php?a=add&pid=1>`__ and verify your email address
and your account will include 1000 SEK credits to get you started which is enough to run a small instance for about half a year.

.. youtube:: wfsCY6GCQ9I

You can then manage your account by using the :doc:`/getting-started/managing-your-cloud/account-management-portal` and control your
cloud infrastructure by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal` or using :doc:`/getting-started/managing-your-cloud/openstack-horizon`
logging in with an API user created from the cloud portal.

If you are unfamiliar with :doc:`/getting-started/managing-your-cloud/openstack-horizon` we
recommended you use the :doc:`/getting-started/managing-your-cloud/cloud-management-portal` as
it will greatly simplify your user experience.

Advanced features (as described in this documentation) in the platform that will require using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`, the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client` or the :doc:`/openstack-api`.

You can see the OpenStack Horizon dashboard documentation in great detail `here <https://docs.openstack.org/horizon/latest/user/index.html>`__ as
we will not cover all details in this documentation.

.. toctree::
  :caption: General
  :maxdepth: 0
  :hidden:

  general/getting-support
  general/quotas
  general/outages
  general/sla-and-terms-of-service

.. toctree::
  :caption: Getting started
  :maxdepth: 0
  :hidden:

  getting-started/index
  getting-started/managing-your-cloud/index
  getting-started/users
  getting-started/available-services
  getting-started/launching-an-instance
  regions-and-availability-zones

.. toctree::
  :caption: Core services
  :maxdepth: 0
  :hidden:

  compute/index
  networking/index
  storage/index

.. toctree::
  :caption: Other
  :maxdepth: 0
  :hidden:

  backup/index
  secret-store/index
  service-catalog/index
  platform-automation/index
  images/index
  dns
  openstack
  openstack-api
  orchestration/index
  guides/index


===============
Getting support
===============

Our support organization is available via telephone or email ticket
during office hours. Our contact information is available `on our website <https://binero.com/en/contact/>`_.

We always recommend creating a ticket when requesting support as this
will enable us to route your question to the most appropriate resource
in our organization and provide traceability for future reference.

Our support organization will happily you in working with the cloud
platform and share our knowledge about best practices for both using
our cloud platform and for your applications and services.


=======
Outages
=======

If you experience an issue with the platform, you can see `our status page <https://status.binero.com>`__
for details about past and present incidents.

We recommend that you subscribe to emails on the status page or subscribe to a ongoing active incident
to get updates.

..  seealso::

  - :doc:`getting-support`


======
Quotas
======

Public cloud providers are at risk for fraudulent and malicious actors that
creates accounts looking to consume our cloud platform at no cost.

To limit the impact of potential misuse of the cloud platform, we enforce
quotas on the amount of resources per account.

If you need to raise your quotas, :doc:`contacting our support </general/getting-support>`.

Please ensure that you've entered relevant details on your account and has paid
for at least one months of usage.

Default quotas
--------------

The default quotas are as follows:

- :doc:`vCores </compute/flavors>`: 40

- :doc:`RAM </compute/flavors>`: 100 GB

- :doc:`Server Groups </compute/server-groups>`: 10

- :doc:`Volumes </storage/persistent-block-storage/index>`: 20

- :doc:`Snapshots </storage/snapshots/index>`: 10

- Volume size on disk: 2000 GB

- :doc:`Backups </backup/index>`: 250

- Backup size on disk: 20000 GB

- :doc:`Networks </networking/network/index>`: 100

- :doc:`Subnets </networking/subnet/index>`: 100

- :doc:`Ports </networking/ports>`: 500

- :doc:`Routers </networking/router/index>`: 10

- :doc:`Floating IP addresses </networking/floating-ips>`: 50

- RBAC policies: 10

- :doc:`Security groups </networking/security-groups/index>`: 15

- Security group rules: 249

- :doc:`Object storage </storage/object-storage/index>` buckets/container: 1000

..  seealso::

  - :doc:`/general/getting-support`


=========================
SLA and Terms of Services
=========================

See the `terms of service <https://portal.binero.com/documents/binerocloud_tos.pdf>`_
for our public cloud.

Binero provides a general SLA for our public cloud platform.

Please :doc:`contact our support </general/getting-support>` if you want to know more.


==========================
Create account and project
==========================

Get your own **virtual data center** today by `registering an account <https://portal.binero.com/cart.php?a=add&pid=1>`_ and
get 1000 SEK credits to get started.

Continue with the `creating your first instance </getting-started/launching-an-instance>`_ to get started!

.. youtube:: wfsCY6GCQ9I

.. meta::
    :description lang=en:
        How to get started with cloud at Binero. Swedish Openstack services compliant with GDPR
    :keywords: openstack, compliant cloud, swedish cloud


==================
Available services
==================

Binero cloud is a fully fledged infrastructure platform. Below are some of the
main services outlined.

Compute
-------

One of the three core services of the platform, compute is instances (virtual
machines) running on physical hardware, providing CPU and RAM via flavors (which
defines amount of resources and what kind of resources a server or instance gets
access to).

Block storage
-------------

Another core service, block storage is the persistent storage that's used to store
your data.

This block storage service provides volumes that can be either SSD or HDD based
depending on your use-case. Volumes supports snapshot, backups and many other features.

You can boot instances by using a volume, or attach one or more volumes to instances,
move volumes between instances and many other features.

Networking
----------

The last core service is networking. Binero cloud supports networks, routers, load balancers,
security groups (firewall) and other functionality to provide you with fast networking for
your infrastructure.

You can use floating IP addresses connected (public IP addresses on the internet) to a port to
provide access to and from your applications and services.

Load balancer
-------------

To manage load distribution between instances and creating highly available and redundant
services you can use our load balancer service.

This system takes incoming requests and forwards it to an instances that you
configure in a pool.

If one instance is not working our system will automatically remove it from the pool. By using
our load balancer service, you can scale out your applications and services fast and efficiently.

Secret store
------------

Using our secure secret store, you can store information securely that you want to use
for the platform.

For instance certificate keys for terminating SSL/TLS in the load balancer or for object
encryption when using with the object storage service. 

Client VPN
----------

To reach your infrastructure securely, you can use a VPN.

This adds a secure tunnel, by encrypting your traffic before sending it over the internet
to your cloud infrastructure. Binero cloud different types of VPN solutions.

Object storage
--------------

Our object storage service helps you store objects, blobs, files or data of any kind by
talking to our secure HTTPS APIs.

Binero cloud supports both the S3 and Swift API for our object storage service.

NVMe storage
------------

NVMe based storage is the most highly performing storage available with latency, throughput
and I/O close to the speed of memory.

Binero cloud provides different NVMe flavors for applications that require the highest
possible throughput and lowest latency to disk.

Backup
------

Binero cloud has a built-in backup solution that you can enable for your volumes.

Backup is always sent to another availability zone and stored in our object storage
service. 

GPU based compute
-----------------

For certain workloads, a CPU will not provide enough parallelism to provide an efficient
workflow.

A GPU is by design more limited in their use-cases but fast and optimized for graphics
or AI workloads.

Binero cloud provides instances with GPU compute. 

Availability zones
------------------

Binero cloud is available in two physical locations, availability zones, with a metric mile
between them.

..  seealso::

  - :doc:`/compute/index`
  - :doc:`/storage/index`
  - :doc:`/networking/index`
  - :doc:`/networking/load-balancer/index`
  - :doc:`/secret-store/index`
  - :doc:`/networking/client-vpn/index`
  - :doc:`/storage/object-storage/index`
  - :doc:`/storage/nvme-storage`
  - :doc:`/backup/index`
  - :doc:`/compute/gpu-instances`
  - :doc:`/regions-and-availability-zones`


=====================
Launching an instance
=====================

This guide provides a step-by-step guide to get started and launch your first instance
on Binero cloud. The available methods to launch an instance are:

- `The cloud management portal </compute/launching-an-instance/cloud-management-portal>`_

- `OpenStack Horizon </compute/launching-an-instance/openstack-horizon>`_

- `OpenStack CLI </compute/launching-an-instance/openstack-terminal-client>`_

- `API </compute/compute-api>`_

For users starting out, we recommend using our cloud management portal as this provides
an better experience to get to know the cloud platform and will take you from 0-100 in
the shortest time possible with little prior understanding of working with public clouds.

More information on using the other methods are available under the sections for each
service in this documentation.

.. note::

   More information on how to access the portals is
   available :doc:`here </getting-started/managing-your-cloud/index>`.

General first steps
-------------------

When starting out in the platform there are some tasks that you normally only need to
perform once.

While there are many ways for you to perform these one time tasks, we will use the cloud
management portal in the following examples. 

The two main ways to connect your infrastructure (and in this case, your first instance)
to the internet:

- Setup an instances to use a public IP directly (see `directly attached IP addresses </networking/directly-attached-ips>`_,
  which is not the same as floating IP addresses).

- Setup a :doc:`/networking/router/index` and use :doc:`/networking/floating-ips` with
  the router. 

We will cover the second option (router + floating IP) in this quick start guide as it will
provide a more versatile option when growing your infrastructure and add an extra layer of
security.

.. note::

   Some features of the platform, for example load balancing, will **require** a router.

   From a security perspective its also preferable to use floating IP addresses as your instances
   will be less exposed to the internet than with a directly attached IP.

   Finally, when using a router with many instances, doing backend connections (for example from
   a web server to a database) can (and should) on the local network rather than on the public
   IP addresses.

   For this reason, we recommend using a router in all use cases except when using only a single
   instance with a strong local firewall in place. 

We recommend the following tasks to complete before launching your first instance:

- Add your SSH-key to the platform (if provisioning Linux-based instances). 

- Create a network.

- Add a subnet to your network.

- Create a router.

- Connect the router to the subnet by adding a router interface.

Add an SSH-key
^^^^^^^^^^^^^^

Please see `this section </compute/ssh-keys>`_ for information on how to add an SSH key.

Create a network
^^^^^^^^^^^^^^^^

To create a network:

- Press the **+** (plus) icon in the lower right corner.

- Press **Networking** and then **Network** in the sidebar menu.

- Name the network and optionally add a description. 

- **europe-se-1** is pre-selected as region. 

- Under **Availability zone**, select a zone, we recommend **europe-se-1a**

- Press **Create**

Add a subnet
^^^^^^^^^^^^

To add a :doc:`subnet </networking/subnet/index>` to the :doc:`network </networking/network/index>`
you created above.

- Press **Networking** and then **Network** in the sidebar menu. Select the network you created
  in the previous step.

- In the top right corner, select **Create subnet** icon.

- Name the subnet.

- Type a network address in CIDR notation. This is the network your servers will run
  on. It needs to be in one of the RFC1918 ranges (``10.0.0.0/8``, ``172.16.0.0/12``
  or ``192.168.0.0/16``). If you are unsure what range to use we suggest ``192.168.0.0/24``.

- Under **Gateway IP**, select **Set standard gateway IP**, this will select the first
  address in the range for your default route.

- While you don't have to use DHCP, we strongly recommend it for an easier workflow. It
  should be pre-selected.

- Under **Add pool**, select what pool you want in the range you choose in previous step. You
  add a start and stop, for example in the above ``192.168.0.0/24`` example a start might be
  ``192.168.0.100`` and stop ``192.168.0.200`` giving you 101 addresses in the pool. As the
  first address (``192.168.0.1``) is the gateway, you cant include that but can otherwise
  choose as you see your need.

- Under **Add DNS name server**, add the name servers you want your network to use. Binero
  provides DNS name servers on ``83.168.225.225`` and ``83.168.226.226`` and we recommend
  those but you can use any DNS servers.

- Press **Create subnet**

Create a router
^^^^^^^^^^^^^^^

To create a :doc:`router </networking/router/index>`.

- Press **Networking** and then **Routers** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- **europe-se-1** is the pre-selected as region.

- Name the router and optionally add a description.

- Under **Availability zone**, select the same availability zone as the
  network you setup in the previous step.

- Under **Choose external network**, select the network with same name as
  the availability zone you choose.

- Press **Create**

Connect router to subnet
^^^^^^^^^^^^^^^^^^^^^^^^

To connect your :doc:`router </networking/router/index>` on your :doc:`subnet </networking/subnet/index>`
(to enable networking for the subnet through the router).

- Press **Networking** and then **Routers** in the sidebar menu. Select the router
  you created in the previous step.

- In the top right corner, select **Create interface** icon. 

- The subnet you just created should be pre-selected, if not select it.

- In the IP for, enter the first IP in the subnet you just created. If you
  choose ``192.168.0.0/24``, the first usable address is ``192.168.0.1``

- Press **Create**

At this point you are ready to provision your first compute instance!

You have the option between the following methods: 

- `The cloud management portal </compute/launching-an-instance/cloud-management-portal>`_

- `OpenStack Horizon </compute/launching-an-instance/openstack-horizon>`_

- `OpenStack CLI </compute/launching-an-instance/openstack-terminal-client>`_

- `API </compute/compute-api>`_

For users starting out in the platform or users that are not used to working with
public clouds, we recommend our cloud management portal.

..  seealso::

  - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
  - :doc:`/networking/router/index`


=========================
Account management portal
=========================

In the `account management portal <https://portal.binero.com>`__ you will be able to
complete tasks and find information relating to, your account on the platform.

The following are examples of actions you are able to do in the account management portal.

- Get a top down overview of your currently provisioned services.

- Login (using single sign on) to the `cloud management portal <cloud-management-portal>`__.

- Change your company information (for example addresses)

- See your invoices.

- Choose payment options.

- Manage contacts (for the account management portal).

You use the email address you registered your account with to login and are able to setup
users (see `Users </getting-started/users>`_ for more information).

If you've forgotten your password, there is a way for you
to `recover it via the login page <https://portal.binero.com/index.php?rp=/password/reset>`__.

..  seealso::

  - :doc:`/getting-started/users`
  - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`


=======================
Cloud management portal
=======================

.. tip::

   If you need many users with personal accounts to manage your infrastructure, `OpenStack Horizon <openstack-horizon>`__ is
   a good solution for this as the cloud management portal only supports the main user. 

In the `cloud management portal <https://portal.binero.cloud>`__ you will be able to setup your cloud infrastructure effortlessly
and quick! You an access it using single sign on from the `account management portal <account-management-portal>`__.

To use single sign on, press **Login to the cloud portal** on the first page (where you end up directly after logging in) in
the `account management portal <account-management-portal>`__.

The cloud management portal focuses on ease of use and has support for the main features of the platform while leaving out some of
the more advanced features.

For information on how to complete certain tasks in the cloud management portal, see the sections in this documentation.

In our `getting started guide </getting-started/launching-an-instance>`_ we explain how to use the cloud management portal to setup
your first instance with a router and a floating IP which is a good way to start out using the platform.

.. youtube:: fG_RUe4lGN0

..  seealso::

  - :doc:`/getting-started/users`
  - :doc:`/getting-started/managing-your-cloud/account-management-portal`


===================
Managing your cloud
===================

The Binero cloud platform has different interfaces to manage your services each
with its unique features and benefits.

.. important::

   There will be situations where the cloud management portal and OpenStack Horizon
   or the OpenStack terminal client shows different things.

   This is because they use different users. We recommend not alternating
   between tools unless needed.

.. tip::

   If you are starting out from scratch, we recommend using
   `our Cloud management portal <cloud-management-portal>`_ to get going!

- `Account management portal <account-management-portal>`_ - Account and billing,
  intended for the administrator of the account or users working with invoices and costs.

- `Cloud management portal <cloud-management-portal>`_ - Essential cloud infrastructure,
  intended for technical users that want to setup infrastructure with less effort.

- `OpenStack Horizon <openstack-horizon>`_ - Advanced cloud infrastructure, intended for
  technicians and engineers working with more advanced implementation and when needing
  personal accounts.

- `OpenStack terminal client <openstack-terminal-client>`_ - Advanced cloud infrastructure
  via terminal, same as OpenStack Horizon but terminal based.

- `OpenStack API </openstack-api>`_ - Infrastructure as code and third party implementations,
  intended for programmers working with infrastructure as code projects using the platform or
  when using third party applications.

See the walk-through of the cloud management portal.

.. youtube:: fG_RUe4lGN0

.. toctree::
  :caption: Management options
  :maxdepth: 1

  account-management-portal
  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


=================
OpenStack Horizon
=================

If you have previous experience from working with OpenStack (or are an advanced user), there
is a second portal from which to manage your cloud infrastructure which is the OpenStack Horizon
dashboard, available at https://control.binero.cloud.

You can login here by first `creating an API user </getting-started/users.html#api-users>`_ from
the cloud management portal. An API user is a native OpenStack keystone user.

.. note::

   An API-user is not the same as the user you've logged into the cloud management portal. Please
   see `Users </getting-started/users>`_ for more information on the user concepts in
   Binero cloud.

Unlike `the cloud management portal <cloud-management-portal>`_, Horizon **does** support more than
a single user.

If your use-case calls for personal logins, use the main user only for setting up API users and have
your technical staff login to the Horizon interface instead. 

Some advanced features (as described in this documentation) in the platform will require OpenStack
Horizon, the OpenStack terminal client or the API to complete. 

Binero cloud offers a standard installation of the OpenStack Horizon dashboard to be consistent
with other vendors and private clouds running OpenStack.

It's documented in great detail in the official OpenStack documentation available
`here <https://docs.openstack.org/horizon/latest/user/index.html>`_ and will not be
exhaustively covered in this documentation. 

.. note::

   If you are unfamiliar with OpenStack Horizon and don't explicitly need the more advanced features
   available there, we recommended to use our `cloud management portal <https://portal.binero.cloud>`_
   as it's much easier to use when starting out. 

..  seealso::

  - :doc:`/getting-started/users`


=========================
OpenStack Terminal Client
=========================

For terminal oriented users, the OpenStack client offer a speedy way (albeit with a steeper
learning curve) to manage your cloud resources. The OpenStack client is in Python.

Installation
------------

How to install the client will vary based on your operating system. On Linux based systems
and Mac, its possible to install via ``pip`` which is the package installer for Python.

Documentation on installing pip is `here <https://pip.pypa.io/en/stable/installation/>`_ but
it's also possible using ``brew`` on Mac or the Linux package manager included in your Linux
distribution (for example ``apt install python3-pip`` on Debian based distributions).

Once you have ``pip`` installed, you are able to install the terminal clients as such: 

::

    pip install python-openstackclient python-designateclient python-barbicanclient python-octaviaclient python-swiftclient

The above command will install the OpenStack client and the extensions needed to manage all
features in the platform.

Configuration
-------------

To configure the client after installation:

- `Create an API user </getting-started/users.html#api-users>`_ (if you don't already have one)

- On the API user, press the small arrow icon, this will download an openrc file

- Source the file in your terminal by running ``source openrc``

- Enter your password

- You are now ready to use the client in the same terminal that you ran the source command in

.. note::

   When running the ``source`` command, you will get a token on your client which is valid for some time
   but refreshed upon usage. If you have not used the client you might need to run the command again.

The terminal client will use your API user. This is not the same as the main user that you would have
used to create your API user (the same one that can login to the cloud management portal).

This in turn means that the OpenStack terminal client will show you the same information that you would
see when login into Horizon, and not the cloud management portal (as it uses the main user,
which is not an API user).

Usage
-----

To for example see the available images in the platform, you can run

::

    openstack image list

The client will run either as a command with arguments or in interactive mode (at which point the
arguments are the commands). A good way to find the right command is to run:

::

    openstack help | grep <INPUT>

where ``<INPUT>`` is what you are looking for. For instance: 

::

    openstack help | grep image

would show you information on the image command used above.

.. note::

   The OpenStack terminal client uses the different OpenStack APIs. If not specifying the API version
   to use the client might default to an older version. This might hinder you from accessing some
   features. You are able to add an option to command for example ``--os-compute-api-version 2.67``
   which will enable you to use newer features.

Generally speaking, there are the following methods in the terminal client:

- **list** - this lists information about resources that are currently in the cloud.
- **show** - this show information about a single resource that is currently in the cloud.
- **create** - this creates a new resource in the cloud.
- **set** - this edits a current resource in the cloud.

See examples of using ``list`` above, using ``-h`` gets you help and provides a good starting
point to understand each command.

.. _mfa-terminal-label:

Multifactor authentication (MFA)
--------------------------------

If you have enabled :ref:`mfa-users-label` you need to use the ``v3multifactor`` auth type and configure
the auth methods to be ``v3password`` and ``v3totp`` or if you are using an
:ref:`Application Credential <application-credentials-label>` you must use ``v3applicationcredential``
and ``v3totp`` as auth methods.

When you've enabled MFA you need to enter a TOTP passcode every time to authenticate to get a token so
instead of authenticating every request we save the token and use that for commands instead.

Configure two clouds in the ``~/.config/openstack/clouds.yaml`` file, one that uses your password and
TOTP and another one that only uses a token.

Use below as an template and replace with correct information. The project name is your customer
number.

When using an :ref:`Application Credential <application-credentials-label>` with MFA you must give
the user ID, you can find your user ID by issuing a token ``openstack token issue -f value -c user_id``

::

  clouds:
    binero-cloud-mfa:
      auth_type: v3multifactor
      auth_methods:
        - v3password
        - v3totp
      auth:
        auth_url: https://auth.binero.cloud:5000
        username: USERNAME_HERE
        password: PASSWORD_HERE
        project_name: PROJECT_NAME_HERE
        user_domain_name: default
        project_domain_name: default
      region: europe-se-1
      interface: public
      identity_api_version: 3
    binero-cloud-mfa-appcred:
      auth_type: v3multifactor
      auth_methods:
        - v3applicationcredential
        - v3totp
      auth:
        auth_url: https://auth.binero.cloud:5000
        user_id: USER_ID_HERE
        application_credential_id: APP_CRED_ID_HERE
        application_credential_secret: APP_CRED_SECRET_HERE
      region: europe-se-1
      interface: public
      identity_api_version: 3
    binero-cloud-token:
      auth_type: v3token
      auth:
        auth_url: https://auth.binero.cloud:5000
        project_name: PROJECT_NAME_HERE
        project_domain_name: default
      region: europe-se-1
      interface: public
      identity_api_version: 3

.. note::

   The below workflow of using a token with the OpenStack Terminal Client does not
   work when using an Application Credential with MFA enabled and you must authenticate
   every request instead.

You can now run the following command to issue a new token, it will prompt you for a TOTP
passcode.

::

    export OS_TOKEN=$(openstack --os-cloud binero-cloud-mfa token issue -c id -f value)

This token is valid for one hour. You can now use it when running commands.

::

    openstack --os-cloud binero-cloud-token server list

..  seealso::

  - :doc:`/getting-started/users`
  - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
  - :doc:`/getting-started/managing-your-cloud/openstack-horizon`
  - :doc:`/openstack`
  - :doc:`/openstack-api`


=====
Users
=====

The Binero cloud platform has four main user types:

- Main user

- :doc:`/getting-started/managing-your-cloud/account-management-portal` users (same
  as the main user or a newly create one)

- :doc:`/getting-started/managing-your-cloud/cloud-management-portal` user (the
  same as the main user)

- API users (created from the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`)

- :ref:`application-credentials-label` (created from :doc:`/getting-started/managing-your-cloud/openstack-horizon` or
  the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client` with an API user)

The main user is the user you registered when signing up for the service. It will be
an email address, probably of the person that setup the account.

The main user can login to both the :doc:`/getting-started/managing-your-cloud/account-management-portal` and the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal` but cannot use :doc:`/getting-started/managing-your-cloud/openstack-horizon`,
the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client` or the API which all require an API user. 

For more details, see information on each section below.

Account management portal
-------------------------

To create a user in the :doc:`/getting-started/managing-your-cloud/account-management-portal`

* From the home screen click **New contact** under the **Contacts** dialog to the right.

* Click **User management**, again to the right.

* Click **Invite new user**. Here you are able to select what permissions the new user
  should have in the account management portal or select all available permissions. 

* Input the users email address.

* Click **Send invite**

The user will receive and email with instructions on how to enroll themselves. 

.. note::

   Users created in the :doc:`/getting-started/managing-your-cloud/account-management-portal` not synced to the
   :doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

To instead add a contact which is mainly useful for sending out information from the platform (for example invoices to the
billing department) without allowing for logging in to the portal.

* From the home screen click **New contact** under the **Contacts** dialog to the right.

* Input the contact details.

* Under email settings, choose what types of emails the new contact should receive.

* Click **Save changes**

Cloud management portal
-----------------------

The :doc:`/getting-started/managing-your-cloud/cloud-management-portal` has only one user (as opposed to the
:doc:`/getting-started/managing-your-cloud/account-management-portal`) which is the main user.

Users in the :doc:`/getting-started/managing-your-cloud/account-management-portal` can single sign-on directly
to the cloud management portal from the account management portal.

.. tip::

   Because you might want to limit access to your infrastructure, a good first step is to add more users in
   the account management portal and reserve the main login for those that need access to the infrastructure in
   the cloud management portal.

.. _api-users-label:

API users
---------

In the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`, you are able to setup API users
under **Access and Security** in the main menu.

An API user is a native OpenStack Keystone user that has access to your project, you can use it in the
:doc:`/getting-started/managing-your-cloud/openstack-horizon`, with
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client` or to connect to any of the
:ref:`OpenStack API endpoints <openstack-api-endpoints>`.

An API user cannot login directly to the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

To create an API user through the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`, follow
these steps:

* Under **Access and security** in the main menu, click **API users**

* Click the plus sign (+) icon in the lower right corner.

* Select a username (part of the username is automatically selected for you), password and optionally add a description.

* Click **Create**

The new API user has the username such as **mainuser@domain.com_choosenusername** (that is, the username
you choose when creating the user will only be part of the total username).

.. tip::

   Provided you only want to use the OpenStack Horizon interface or work via API, its possible to setup API users
   as personal users for the people managing the infrastructure. The main user is then the super admin which would
   is only needed to provision API users.

.. _application-credentials-label:

Application Credentials
-----------------------

.. important::

   If you enable :ref:`mfa-users-label` for an API user that also applies on all Application Credentials for
   that API user.

Using an Application Credentials makes it possible to grant specific access to your application(s) as a user without
sharing the credentials for that user.

The scope of an Application Credential is the same as the user that created it, you can limit it by selecting
specific roles or access rules, the user that created the Application Credentials owns the resource and it's
tied to the lifetime of the user unless explicitly deleted.

You can read more about Application Credentials in the official OpenStack documentation
`here <https://docs.openstack.org/keystone/latest/user/application_credentials.html>`_.

You can create Application Credentials by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`
or :doc:`/getting-started/managing-your-cloud/openstack-horizon`.

To create an Application Credential through :doc:`/getting-started/managing-your-cloud/openstack-horizon`

* Under **Identity** in the main menu, click **Application Credentials**

* Click the **+Create application credentials** button in the top right

* Input a name and optionally a description.

* In the secret field, input a secret, if you don't we will generate a
  secret for you which we recommended.

* Optionally you can provide a ``expiration date`` and time for when to (automatically) deactivate
  the application credential.

* Under roles, select the appropriate roles. If you don't select a role, the same role as your account will be
  used (member). Creator will allow creation of some objects (secrets) where as the reader role allows read-only
  in most services.

* Under access rules you are able to give even more granular accesses to API calls. See the information on how
  this works in the dialog. If you don't enter anything here, your user is not restricted to any specific API
  calls.

* The **Unrestricted** box will allow the Application Credential to create API users. This is **NOT** recommended.

* Finally, click **Create Application Credential**

.. important::

   Once you've created the Application Credential, you have a one-time opportunity to save the credential by
   copying it or downloading it in the openrc or YAML format. Once you've closed it, you will never be able
   to retrieve the secret again.

Credentials
-----------

.. important::

   The credentials feature is not a secret store and is only used for credentials used for authentication
   tied to a user, see the :doc:`/secret-store/index` service for storing secrets or sensitive information.

Using an credential makes it possible to store and exchange credentials in return for a token or access to
a service that has it's own authentication.

For example the :doc:`/storage/object-storage/s3` implementation for the :doc:`/storage/object-storage/index`
service or storing user related credentials such as a TOTP secret for :ref:`MFA <mfa-users-label>`.

You can only manage Credentials by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

.. warning::

   When listing credentials they are in plain text and contains sensitive information.

You can list all the credentials stored for your API user with ``openstack credential list``.

.. _ec2-credential-label:

EC2 Credential
~~~~~~~~~~~~~~~

.. warning::

   An EC2 credential will continue to work even if you enable :ref:`mfa-users-label` on the API user. This
   means the credential can bypass MFA, make sure that you audit or remove all EC2 credentials if you don't
   need them and have MFA enabled.

A EC2 credential is a credential with type set to ``ec2`` and contains a blob of JSON data with an access
and secret key.

You can then use this access and secret key to retrieve a token scoped to the user that created the EC2
credentials or used it to authenticate against the :doc:`/storage/object-storage/s3` service.

You can list existing EC2 credentials by using ``openstack credential list --type ec2``. If you want to create
a new EC2 credential you can use ``openstack ec2 credentials create``.

TOTP Credential
~~~~~~~~~~~~~~~

A TOTP credential is a credential with type set to ``totp`` that Keystone will use when you
give it a passcode with the ``totp`` auth method when authenticating with your API user.

We **do not recommend** that you manage or touch anything related to TOTP credentials and instead
rely on the flow as described in the :ref:`mfa-users-label` section.

.. warning::

   If you list TOTP credentials it will show your TOTP secret in plain text, this secret
   key is whats used to generate valid TOTP passcodes for your API user when doing MFA,
   you must keep this secret safe.

.. _mfa-users-label:

Multifactor authentication (MFA) for API user
---------------------------------------------

.. note::

   Looking for how to use the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`
   with MFA enabled on your API user? :ref:`Click here to read more <mfa-terminal-label>`.

We support multifactor authentication (MFA) on API users and allow you to self-service
enable it on your API user through :doc:`/getting-started/managing-your-cloud/openstack-horizon`.

Enabling MFA authentication protects your API user by requiring you to present two factors, your
password and a TOTP passcode, for successful authentication.

Before enabling MFA it's important to understand the impact on your API user and any
Application Credentials and EC2 credentials that you've created.

- You will not be able to login to :doc:`/getting-started/managing-your-cloud/openstack-horizon`,
  use the API, terminal client or remove MFA without entering a TOTP passcode.

- If you lose access to your TOTP application or device you will lose access to your API user, we
  recommend that you keep a backup of your TOTP secret.

- All :ref:`application-credentials-label` for your user is also protected and enforced to use MFA.

- :ref:`EC2 credentials <ec2-credential-label>` for this user will continue to work
  and is **NOT** protected or enforced to use MFA, make sure to audit your EC2 credentials.

- Enabling MFA on your API user will invalidate all tokens not issued with MFA enabled.

Please make sure to read through the bullet points above carefully and consider the impact
on your cloud account in the platform before continuing.

- Login to :doc:`/getting-started/managing-your-cloud/openstack-horizon` in the top right click
  your username and in the dropdown go to **Settings**.

- In the menu to the left you will now see a Settings with **User Settings** selected,
  click **MFA Settings**

- Scan the QR code with your TOTP application (such as Google Authenticator) or device, or click
  **View All Details** to show the TOTP secret in plain text.

- Enter a valid passcode and click **Submit**

  - If you enter an incorrect passcode the page refreshes and you will get a new TOTP
    secret and need to go through the same procedure again.

  - If you enter a valid passcode, you're logged out and MFA is now enabled.

If you ever want to remove MFA on your API user you can go back to the **MFA Settings**
page, enter a valid passcode, click **Submit** and MFA is then removed from your API user.

You can read more :ref:`here <mfa-terminal-label>` if you want to use MFA with the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

When using :doc:`/getting-started/managing-your-cloud/openstack-horizon` you will get prompted
for a TOTP passcode when you login.

..  seealso::

  - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
  - :doc:`/getting-started/managing-your-cloud/account-management-portal`


==============================
Regions and availability zones
==============================

Binero cloud is conceptually designed and physically provisioned to provide
segmentation into regions which in turn provides one or more availability zones.

The main reason for wanting many physical locations is to build geo-redundant
installations, meaning infrastructure that is able to withstand a disaster scenario.

For example a properly designed geographically distributed application should in case
of a force majeure event in a data center be able to handle without any disruption. 

Regions
-------

A region is a wider geographic area where resources are available.

A region does not share dependencies with other regions except for certain central services
such as authentication - not relating to the production of infrastructure services.

We design regions to be autonomous. Networking between regions are internet-based as
opposed to availability zones that are interconnect with low latency networking.

Regions have their own APIs and control planes isolated from each other.

Binero cloud has one publicly available region named *europe-se-1* or *eu-se-1* for
short. The region consists of data centers close to or around Stockholm in Sweden. 

A region can contain one or more availability zones.

Availability zones
------------------

While some organizations define a availability zone (AZ) as a physical rack or a room in a
data center, in Binero cloud a availability zone is a separate physical data center in a region.

One or more availability zones together form a region, each availability zone separated by
one metric mile between them.

We design availability zones to have no dependencies between them for the infrastructure
services. That means that all infrastructure is separate from other availability zones in
the same region.

- Physical location, building, data center, power cooling

- Infrastructure networking (internet)

- Networking services

- Storage services

- Compute services

- And much more

This enables you to provision your infrastructure in different locations, allowing you to build
and design highly available and geographically redundant applications in platform.

Availability zones in the same region has reachability between each other over fault-tolerant,
low latency networking (sub 1ms), they do not share any networking infrastructure other than
reachability between them.

We design networking to continue operation regardless of other availability zones in
the same region.

Storage services are also autonomous to other availability zones and do not share anything, we
provide different replication options that can help you copy data between availability zones.

Binero cloud provides two public availability zones (**a** and **b**) in the **europe-se-1**
region and we refer to them as **europe-se-1a** and **europe-se-1b**.

.. tip::

   We recommend using the availability zone **europe-se-1a** to start with (or if you only need
   a single AZ) as the cost is slightly higher in *europe-se-1b*

.. list-table::
   :header-rows: 1

   * - ID
     - Availability Zone
     - Region
     - Geography

   * - a
     - europe-se-1a
     - europe-se-1
     - Stockholm, Sweden

   * - b
     - europe-se-1b
     - europe-se-1
     - Stockholm, Sweden

Availability zones networking
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Its important to understand the networking concepts used to ensure a functioning system
in case of a unforeseen disruption to a single availability zone - which in a geographically
redundant setup is what you need take into account. 

While the network fabric have no dependencies between availability zones to ensure the continued
operation in case of a zone malfunctioning, particularly the internet access will require careful
design to guarantee functionality.

In Binero cloud, the floating and directly attached IP addresses, the public IP space that provides
your infrastructure access to the internet, are local to a single availability zone.

While assigning a floating IP to an instance located in a different AZ would work, in case of a complete
outage of the first AZ (that hosts the floating IP) you would see traffic disruption to both zones since
your traffic would still need to traverse the first zone.

The solution to this is to assign floating IP addresses in both zones, which should only use resources
located in the same zone, and have a DNS record that could be re-pointed or using a global load balancer
to send the request to a zone that is currently working.

More on this in the `router segment </networking/router/index>`_ in this documentation.

Availability zones and storage
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using Binero clouds different replication options for storage would ensure that your data available in
one or more availability zones in a region.

While this is enough to guarantee data integrity in a disaster scenario, if you also want your data *available*
you would need to ensure that you read your data from the same availability zone that your send the request.

Binero cloud provides endpoints for S3 in all zones that host S3, make sure to use the correct one.

When doing database replications between availability zones, make sure your application connects towards a
database that is local to the application making the request. 

Global load balancing
---------------------

A global load balancer is a solution that uses a globally redundant technology to send requests to more than a
single availability zone.

The two most common ways to support this is, either by using BGP (the dynamic routing protocol that powers the
internet) or by using DNS (Domain Name System).

Both are globally distributed by nature and work irrespectively of a single availability zone.

The main reason to use a global load balancer is to ensure that the request does not get stuck in a dependency
towards an AZ that is not working.

Lets say you do load balancing in *europe-se-1a*, it would normally send requests also to backends that you have
running in *europe-se-1b*. If *europe-se-1b* goes down, it would correctly remove them.

If *europe-se-1a* instead go down, the load balancer becomes unavailable and would not be able to send
traffic to the backend in *europe-se-1b* causing an outage for your application.

Using a failover scenario with load balancers in availability zones would still mean that there are dependencies
between them that might cause the failover not to work, particularly in a partial outage scenario.

Using a global load balancer that is truly distributed, that is not local to a single availability zone solves
this by removing traffic to the faulty availability zone entirely. Since it would always have an outside-in
perspective it would make its decisions about sending traffic based on the same information an user would have. 

When setting up copies of your application with replication between them, as is possible with availability
zones in Binero cloud, a single zone should be enough to guarantee a full working service on its own.

A global load balancer would be able to notice an outage in a availability zone and proceed to remove it  by fencing
it from receiving requests until service in that availability zone becomes available again.

Binero cloud does not currently provide a global load balancer service but by using `our DNS system </dns>`__ we do
provide a highly redundant, geo-diverse name server solution that works as a manual GLB.

Please contact us for more information or if you need help designing the fault tolerance that suites
your application best.

.. tip::

   While building geo-diverse highly available systems is difficult, using a platform with support for it greatly
   simplifies the task. We are available to help with design decisions for your application, contact us and we
   can talk more.

..  seealso::

  - :doc:`/networking/router/index`
  - :doc:`/storage/index`


=======
Compute
=======

Compute is one of the core services in Binero cloud. Compute consist of instances (virtual
servers) of a certain :doc:`flavor <flavors>` (performance) running an :doc:`image <../images/index>`
(operating system).

Instances normally use :doc:`/storage/persistent-block-storage/index` to save files and folders
on a filesystem and are reachable via either :doc:`/networking/floating-ips` or
:doc:`/networking/directly-attached-ips`.

You are able to :doc:`launch <launching-an-instance/index>`, :doc:`resize <resizing-an-instance/index>`,
:doc:`shelf <shelving-an-instance>` (offline store) and delete instances via one of the following methods:

- :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- :doc:`/openstack-api`

More information on the many ways to manage your cloud is available in
the :doc:`/getting-started/managing-your-cloud/index` article.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  ssh-keys
  flavors
  launching-an-instance/index
  resizing-an-instance/index
  recreating-an-instance-from-volume
  assign-ip
  console
  server-groups
  hard-reboot
  shelving-an-instance
  boot-from-rescue
  windows-instances
  gpu-instances
  boot-instances-using-uefi
  regions-and-availability-zones
  compute-api


========
SSH-keys
========

Using an SSH-key is generally considered more secure than using passwords.

While you can use passwords in the platform, the initial login of a newly created
Linux instance will require an SSH-key.

If you already have an SSH-key that you use, upload the **public** part to the
platform (see below).

How to use SSH-keys on your particular client operative system are out of scope
for this documentation but there is information readily available on the internet
on this topic.

.. note::

   When using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
   you will invariably use the :doc:`main user </getting-started/users>`.

   SSH-keys created in this portal will not work in the other management tools
   which use API (OpenStack) users.

   Remember that if you create a key using the cloud management portal, you will
   also need to create a key using either Horizon, The OpenStack terminal client or using
   the API if you want to use either of these in the future.

.. tip::

   If you don't have a key, you will be able to generate a new one using one of the below
   methods.

   We do recommend using ``ssh-keygen`` to generate the key on your computer. This
   is because you are then able to choose one of the modern keying methods instead of RSA
   which the various platform options will provide.

   We recommend that you use the new modern ED25519 algorithm when creating your SSH
   key and that you use a passphrase for the key.

   Use the command ``ssh-keygen -t ed25519 -f ~/id_ed25519`` to create your new SSH
   key, use a strong password, your private key is in ``~/id_ed25519`` and your public
   key is in ``~/id_ed25519.pub`` file.

.. tip::

   If you already have a key but with no password we strongly recommend adding a password to
   it, you can add a password to your SSH key with ``ssh-keygen -p -f <your keyfile>``.

   Adding a password will secure the key in case you it's stolen or lost. Remember that the
   private key is as sensitive as a plain text password without a passphrase.

Cloud management portal
-----------------------

To create an SSH-key through the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **SSH-keys** in the side menu. 

- Press the **+** (plus) sign in lower right corner.

- Choose a name.

- Copy/paste your key into the **Key contents** field.

If you don't have a key you can instead choose to generate a new one:

- Press **Generate new key**

- Copy the **private** part of the new key to a file on your workstation as this will **not** be
  saved in the platform. We recommend setting a passphrase on the key according to the above tips. 

OpenStack Horizon
-----------------

To create an SSH-key through :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Key pairs** in the sidebar menu.

- Press **Import public key** in the top right corner.

- Choose a name.

- Choose **SSH-key** under **Key type**

- Paste your **public key** in the **Public key** form (or import the public key
  from file using the **Load public key from file** button)

- Press **Import public key**

If you don't have a key you can instead choose to generate a new one:

- Under **Project**, click **Compute** and then **Key pairs** in the sidebar menu.

- Press **Create key pair** in the top right corner.

- Choose a name.

- Press **Create key pair**

Your new **private key** is automatically downloaded to a ``.pem`` file and the public key
gets saved in the platform. We recommend setting a passphrase on the key according to above tips. 

OpenStack terminal client
-------------------------

You can create a key using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client` by
running the following command ``openstack key create [NAME]``.

It will output your keys **private part** which you should save to your computer, the public part gets
stored in the platform. We recommend setting a passphrase on the key according to above tips. 

.. note::

   If you have issues with an SSH-key not provisioning correctly on your new instance, check your IP
   network configuration. You can see in the instances console log what keys cloud-init has installed.

..  seealso::

    - :doc:`/compute/gpu-instances`
    - :doc:`/storage/nvme-storage`


========================
Assigning IP to instance
========================

General concept
---------------

An IP address is normally assigned the instance upon :doc:`creation <launching-an-instance/index>`.

This is the recommended approach to assigning an IP address (and thus, access to a network)
as it will run cloud-init and (provided you've assigned a :doc:`subnet </networking/subnet/index>`)
setup your ip-configuration for you.

*This is only done on instance provisioning so any extra* :doc:`ports </networking/ports>`
*you assign, will have to be manually configured in the operating system*.

Aside from the single IP that is normally setup during provisioning, two main use-cases for
connecting extra IP addresses to an instance exists:

- If your instances needs connect to different networks, this is possible to do when
  provisioning a new instance as you can add one or more ports.

- If your instance needs to have extra IP addresses from the same network, this is possible when
  launching an instance using OpenStack Horizon or the OpenStack terminal client by first manually
  creating one or more :doc:`ports </networking/ports>` and assigning them.

More information on the concepts of networking in our :doc:`networking section </networking/index>`.

.. tip::

   Adding an extra IP address by using the platform is analogous to adding a :doc:`port </networking/ports>` and
   connecting it to the instance.

   You need to add more ports to your instance to access more then one network, if the network is
   already connected through an existing port you can use that.

Assigning an IP after instance creation
---------------------------------------

To add an IP after creating your instance, the process consists of either creating a :doc:`port </networking/ports>`
and then assigning it to an instance or, as the below guide will detail, connecting an instance to a network (and
by doing so, creating a port in the process).

.. important::

   When adding more ports to your instances, its possible that your instance might loose its network connectivity
   when taking the IP configuration live in the operating system.

   Its also possible that floating IP addresses might need to be re-assigned to the new port depending on if the
   default route on the instance changes. We recommend doing changes to interfaces network configuration during a
   maintenance window. 

.. note::

   Below is an example on how to add networks which will add a port automatically. If you prefer to create the ports
   yourself (more information in the :doc:`/networking/ports` article), another option is to assign an already created
   port. 

Assigning a port using the cloud management portal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a port to an instance by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance you want to assign the port to.

- Press the **Network** tab.

- Press the **Add port** button.

- Choose the network and subnet you want to connect to.

- Press **Add port**

Your port might not be active in your operating system until it's configured there.

Assigning a port using the OpenStack Horizon portal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a port to an instance by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance you want to resize,
  press **Attach Interface**

- Under **Network**, select the network (it will also show subnets) you want to connect to.

- Press **Attach interface**

Your port might not be active in your operating system until it's configured there.

Assigning a port using the OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a port to an instance by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server list``. Save the name of the instance you want to assign
  the port to.

- Run this command: ``openstack network list``. Save the name of the network you want to connect
  to the instance. 

- Run this command: ``openstack server add fixed ip [SERVER NAME] [NETWORK NAME]``, replacing the
  values in angle brackets by the information from the previous steps.

Your port might not be active in your operating system until it's configured there.

.. note::

   If you remove an existing port and add a new one, you might have problems with udev persistent
   rules because of new MAC address depending on the operating system.

   We would recommend making sure that you have :doc:`console access </compute/console>` to your instance
   and a password to login with when changing networking.

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
    - :doc:`/getting-started/managing-your-cloud/openstack-horizon`
    - :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`
    - :doc:`/networking/ports`
    - :doc:`index`


======================
Boot from rescue image
======================

Sometimes you need to boot your instance from an ISO image because the mounted
filesystem on the boot :doc:`volume </storage/persistent-block-storage/index>`
prevents you from fixing certain issues.

You can use a rescue image in these scenarios to boot an alternate operating
system, which can then fix problems with the volume.

The :doc:`/getting-started/managing-your-cloud/cloud-management-portal` includes
a rescue function, but it only lets you mount a rescue image on instances that
don't boot from a volume. We'll focus on how to use the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`
instead to workaround this limitation.

OpenStack Terminal Client
-------------------------

To boot your instance from a rescue image by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- We provide a public `SystemRescue <https://www.system-rescue.org>`_ image that you can use.

- You can use a custom rescue image or use our guide :doc:`here </images/create-rescue-image>`
  to create one.

- To connect the image to the instance run the command ``openstack --os-compute-api-version 2.87 server rescue --image <ISO_NAME> <INSTANCE_NAME>``, replacing
  ``<ISO_NAME>`` with the name from previous step and ``<INSTANCE_NAME>`` with the name of the
  instance that should mount the image.

  - It will take up to a few minutes to add the rescue image and for the instance to start. Use
    the :doc:`console <console>` to manage the instance when it's done.

- When you're done with the rescue remove the rescue image from the instance by running the
  command ``openstack server unrescue <INSTANCE_NAME>``.

.. note::

   If you've created a custom image and have issued ``unrescue`` on the instance remember to
   remove the image or you will pay an hourly fee per GB for the image, see our price list.

..  seealso::

    - :doc:`/images/create-rescue-image`
    - :doc:`index`


=========================
Boot instances using UEFI
=========================

.. note::

   We do not currently support UEFI Secure Boot, :doc:`contact our support </general/getting-support>`
   and let us know you need this feature.

We support booting instances by using ``UEFI`` instead of legacy ``BIOS``. To use it you
need to set the properties ``hw_firmware_type`` to ``uefi`` and ``hw_machine_type`` to
``q35`` on your :doc:`image </images/index>` or in the volume metadata.

The system copies the image properties into the volume metadata when it creates
a volume with an image.


======================================
Manage compute using the OpenStack API
======================================

To manage compute for example managing instances via the OpenStack API you need
to use the Nova API.

We do not provide a complete documentation for how to do API calls but
rather `link to the official documentation <https://docs.openstack.org/api-guide/compute/>`_. 

.. note::

   Read our :doc:`general OpenStack API documentation </openstack-api>` as a good starting
   point for working with the API.


=======
Console
=======

You are able to open the console of your instances by using the instance console in
either the :doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`
or :doc:`OpenStack Horizon </getting-started/managing-your-cloud/openstack-horizon>`.

This is useful if you need to debug or troubleshoot your instance if it's unreachable.

.. note::

   Since you cannot use SSH-keys to login to a console but need to have a password, if you
   have not set a password (but trusted in the keys from provisioning), you will have to reset
   the root or administrator password.

   How to do this will vary by operating system but for the majority of Linux distributions, booting
   into single user mode will enable a password-less login which you can use to reset the password
   for the root user.

Cloud management portal console
-------------------------------

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance you want to view console output for.

- Press the console icon (looks like ``< >``) in the top right.

- A new window will appear showing the console. If the image is blank, this might be because the screen
  is in power save mode (operating system setting). Press the mouse cursor on the screen and press any key
  to wake it up.

OpenStack Horizon console
-------------------------

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance you want
  to view console for, press **Console**.

- A new window will appear showing the console. If the image is blank, this might be because the screen
  is in power save mode (operating system setting). Press the mouse cursor on the screen and press any
  key to wake it up.

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
    - :doc:`/getting-started/managing-your-cloud/openstack-horizon`


=======
Flavors
=======

Binero cloud provides different flavors of compute instances. A flavor defines the compute, memory,
storage capacity for some flavors (see NVMe flavors below) and other hardware configuration that
allows you to customize for performance or features needed by your application.

We provide different groups of flavors and each group has different performance characteristics
and optimizations for different use-cases.

Each group of flavors has a version number meaning the higher version number the newer the
hardware giving a performance advantage on previous versions, for flavors without an explicit
version number in the name it implies version 1.

A flavor can have specific features such as NVMe storage, GPU, Pinned CPU. We add a CPU vendor
identifier as a suffix to the flavor name, see the CPU vendor identifiers list below.

- ``a`` for AMD (amd64)

- ``i`` for Intel (x86_64)  **Version 1 implies Intel**

- ``r`` for ARM (ARM64/AARCH64)

An overview of the flavor groups and their types looks like below.

- General purpose

  - Version 1 (flavors without a version number) implies Intel, example ``gp.2x4``

    - Pinned CPU for CPU intensive workload (dedicated CPU), example ``gp.4x8-pinned``

  - Version 2 with AMD (CPU vendor identifier is ``a``), example ``gp-v2a.2x4``

- High memory

  - Version 1 (flavors without a version number) implies Intel, example ``hm.2x8``

    - Pinned CPU for CPU intensive workload (dedicated CPU), example ``hm.8x32-pinned``

  - Version 2 with AMD (CPU vendor identifier is ``a``), example ``hm-v2a.2x8``

- High performance

  - Version 1 (flavors without a version number) implies Intel, example ``hp.8x32``

    - GPU, example ``hp.8x24-gpu4`` where the number after GPU means 4 GB vRAM

    - NVMe storage, example ``hp.4x8-nvme50`` where the number after NVMe means 50 GB disk

    - GPU and NVMe, example ``hp.12x64-gpu8-nvme250``

The flavor ``hm.8x48`` is a High memory (hm) version 1 using Intel CPUs with 8 vCPUs
and 48 GB memory.

The ``hp.12x64-gpu8-nvme50`` flavor is High performance (hp) version 1 with 12 vCPUs,
64 GB memory, 8 GB GPU vRAM and 50 GB of local NVMe storage.

The ``gp-v2a.8x16`` flavor is General purpose (gp) version 2 with 8 vCPUs and 16 GB
memory.

See the list of flavors more in detail below, you can also see them in the portals or by listing
them using the OpenStack terminal client with ``openstack flavor list --sort-column Name``

.. note::

   All gigahertz (GHz) specifications for processors is the base clock
   frequency in the product specification and does not take into account that
   your workload can be faster due to Intel Turbo boost or AMD Boost clock
   functionality provided by the processors.

General purpose
---------------

This general purpose flavor group has a well rounded combination of CPU and memory and is suitable
for most general application use-cases that does not have heavy or intensive CPU requirements.

This flavor group provides a good cost for performance and we recommend it for use-cases
with low to medium usage requirements.

Version 1 with Intel CPU
~~~~~~~~~~~~~~~~~~~~~~~~

The version number 1 (implied in the flavor names) is using Intel as CPU vendor
with Intel Xeon Gold 6138 2.0 GHz processors from the Intel Xeon Scalable Processors
product family.

.. tip::

   If you are looking for a larger flavor or more performance, see the General purpose
   version 2 flavors with AMD that is larger and has 1-4x the performance based on your
   use-case.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - gp.1x2
     - 1
     - 2048
     - General purpose
     - Version 1
     - Intel

   * - gp.2x4
     - 2
     - 4096
     - General purpose
     - Version 1
     - Intel

   * - gp.3x6
     - 3
     - 6144
     - General purpose
     - Version 1
     - Intel

   * - gp.4x8
     - 4
     - 8192
     - General purpose
     - Version 1
     - Intel

   * - gp.6x12
     - 6
     - 12288
     - General purpose
     - Version 1
     - Intel

   * - gp.8x16
     - 8
     - 16384
     - General purpose
     - Version 1
     - Intel

   * - gp.12x24
     - 12
     - 24576
     - General purpose
     - Version 1
     - Intel

   * - gp.16x32
     - 16
     - 32768
     - General purpose
     - Version 1
     - Intel

   * - gp.24x48
     - 24
     - 49152
     - General purpose
     - Version 1
     - Intel

Pinned CPU
^^^^^^^^^^

These flavors gives you gives you a pinned CPU, this means that we pin your vCPU to a
dedicated CPU core (pCPU) giving you exclusive access to the computation time on that
core.

In certain use-cases guaranteeing low-latency and fast access to CPU computation time is
critical for time sensitive applications that suffers when scheduling of CPU time to shared
CPU cores takes more time causing the application to experience lag or higher tail latencies
due to for example noisy neighbours.

.. vale off

We also guarantee that your allocation also get the CPU core thread siblings so that you're
never exposed to any transient execution CPU vulnerabilities.

.. vale on

When allocating pinned CPUs we reach across all NUMA nodes to optimize CPU usage but due to
memory being local to a NUMA node we recommend ultra sensitive workloads to not run its workload
on different cores if you need low-latency memory access as that will traverse NUMA nodes to
read memory.

.. tip::

   See the High memory version 1 pinned CPU flavors for the same feature but with
   a higher memory to CPU ratio.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - pCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - gp.4x8-pinned
     - 4
     - 8192
     - General purpose with pinned CPU
     - Version 1
     - Intel

   * - gp.8x16-pinned
     - 8
     - 16384
     - General purpose with pinned CPU
     - Version 1
     - Intel

   * - gp.16x32-pinned
     - 16
     - 32768
     - General purpose with pinned CPU
     - Version 1
     - Intel

Version 2 with AMD CPU
~~~~~~~~~~~~~~~~~~~~~~

The version number 2 is using AMD as CPU vendor with AMD EPYC 7742 2.25 GHz processors from the
EPYC 7002 series.

Performance testing has indicated that General purpose version 2 has 1-4x better performance
than version 1 depending on your use-case.

.. note::

   :doc:`windows-instances` is not supported on version 2 flavors.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - gp-v2a.2x4
     - 2
     - 4096
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.4x8
     - 4
     - 8196
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.6x12
     - 6
     - 12288
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.8x16
     - 8
     - 16384
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.12x24
     - 12
     - 24576
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.16x32
     - 16
     - 32768
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.24x48
     - 24
     - 49152
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.48x96
     - 48
     - 98304
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.64x128
     - 64
     - 131072
     - General purpose
     - Version 2
     - AMD

   * - gp-v2a.96x192
     - 96
     - 196608
     - General purpose
     - Version 2
     - AMD

High memory
-----------

The high memory flavor group has optimizations to provide the best value for applications that
needs a larger amount of memory compared to more CPU cores that as provided by general purpose.

The flavor group targets more memory heavy use-cases such as a database application with heavy
caching of the data set in memory that doesn't have a high or intensive computation need.

Version 1 with Intel
~~~~~~~~~~~~~~~~~~~~

The version number 1 (implied in the flavor names) is using Intel as CPU vendor
with Intel Xeon Gold 6138 2.0 GHz processors from the Intel Xeon Scalable Processors
product family.

.. tip::

   If you are looking for a larger flavor or more performance, see the High memory
   version 2 flavors with AMD that is larger and has 1-4x the performance based on your
   use-case.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPU
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - hm.1x4
     - 1
     - 4096
     - High memory
     - Version 1
     - Intel

   * - hm.2x8
     - 2
     - 8192
     - High memory
     - Version 1
     - Intel

   * - hm.3x12
     - 3
     - 12288
     - High memory
     - Version 1
     - Intel

   * - hm.4x16
     - 4
     - 16384
     - High memory
     - Version 1
     - Intel

   * - hm.6x24
     - 6
     - 24576
     - High memory
     - Version 1
     - Intel

   * - hm.6x32
     - 6
     - 32768
     - High memory
     - Version 1
     - Intel

   * - hm.8x48
     - 8
     - 49152
     - High memory
     - Version 1
     - Intel

   * - hm.8x96
     - 8
     - 98304
     - High memory
     - Version 1
     - Intel

   * - hm.12x64
     - 12
     - 65536
     - High memory
     - Version 1
     - Intel

   * - hm.12x128
     - 12
     - 131072
     - High memory
     - Version 1
     - Intel

   * - hm.16x64
     - 16
     - 65536
     - High memory
     - Version 1
     - Intel

   * - hm.16x128
     - 16
     - 131072
     - High memory
     - Version 1
     - Intel

   * - hm.24x64
     - 24
     - 65536
     - High memory
     - Version 1
     - Intel

   * - hm.24x128
     - 24
     - 131072
     - High memory
     - Version 1
     - Intel

Pinned CPU
^^^^^^^^^^

See the pinned CPU section in General purpose version 1 for a detailed explanation
of these flavors. These are the same but with a higher memory to CPU ratio.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - pCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - hm.4x16-pinned
     - 4
     - 16384
     - High memory with pinned CPU
     - Version 1
     - Intel

   * - hm.8x32-pinned
     - 8
     - 32768
     - High memory with pinned CPU
     - Version 1
     - Intel

   * - hm.16x64-pinned
     - 16
     - 65536
     - High memory with pinned CPU
     - Version 1
     - Intel

Version 2 with AMD CPU
~~~~~~~~~~~~~~~~~~~~~~

The version number 2 is using AMD as CPU vendor with AMD EPYC 7742 2.25 GHz processors from the
EPYC 7002 series.

Performance testing has indicated that High memory version 2 has 1-4x better performance
than version 1 depending on your use-case.

.. note::

   :doc:`windows-instances` is not supported on version 2 flavors.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - hm-v2a.2x8
     - 2
     - 2048
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.4x16
     - 4
     - 4096
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.6x24
     - 6
     - 6144
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.8x32
     - 8
     - 32768
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.8x192
     - 8
     - 196608
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.12x64
     - 12
     - 65536
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.16x96
     - 16
     - 98304
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.16x256
     - 16
     - 262144
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.18x128
     - 18
     - 131072
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.20x192
     - 20
     - 196608
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.22x256
     - 22
     - 262144
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.24x384
     - 24
     - 393216
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.32x512
     - 32
     - 524288
     - High memory
     - Version 2
     - AMD

   * - hm-v2a.64x768
     - 64
     - 786432
     - High memory
     - Version 2
     - AMD

High performance
----------------

The high performance flavor group has optimizations for applications with either or both heavy
and intensive CPU requirements. This results in faster task completion, a faster and more predictable
access to both memory and storage giving you a lower latency.

The flavor group targets more CPU intensive applications causing your workload to run and complete
faster improving serial throughput, use-cases such as caching servers or heavily utilized database
applications or other application that needs more performance and lower latency.

The flavor sizes is more rounded to being equal in CPU and memory.

Version 1 with Intel
~~~~~~~~~~~~~~~~~~~~

The version number 1 (implied in the flavor names) is using Intel as CPU vendor
with Intel Xeon Gold 6154 3.0 GHz processors from the Intel Xeon Scalable Processors
product family.

.. tip::

   The High performance version 1 has a 50% higher base clock frequency than version 1
   of the General purpose and High memory flavor groups and which greatly improves
   performance for CPU heavy or latency sensitive workloads.

.. list-table::
   :widths: 25 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Flavor group
     - Version
     - CPU vendor

   * - hp.2x4
     - 2
     - 4096
     - High performance
     - Version 1
     - Intel

   * - hp.2x6
     - 2
     - 6144
     - High performance
     - Version 1
     - Intel

   * - hp.2x8
     - 2
     - 8192
     - High performance
     - Version 1
     - Intel

   * - hp.4x6
     - 4
     - 6144
     - High performance
     - Version 1
     - Intel

   * - hp.4x8
     - 4
     - 8192
     - High performance
     - Version 1
     - Intel

   * - hp.4x16
     - 4
     - 16384
     - High performance
     - Version 1
     - Intel

   * - hp.6x12
     - 6
     - 12288
     - High performance
     - Version 1
     - Intel

   * - hp.8x24
     - 8
     - 24576
     - High performance
     - Version 1
     - Intel

   * - hp.8x32
     - 8
     - 32768
     - High performance
     - Version 1
     - Intel

   * - hp.8x64
     - 8
     - 65536
     - High performance
     - Version 1
     - Intel

   * - hp.8x128
     - 8
     - 131072
     - High performance
     - Version 1
     - Intel

   * - hp.12x32
     - 12
     - 32768
     - High performance
     - Version 1
     - Intel

   * - hp.12x64
     - 12
     - 65536
     - High performance
     - Version 1
     - Intel

   * - hp.12x128
     - 12
     - 131072
     - High performance
     - Version 1
     - Intel

NVMe storage
^^^^^^^^^^^^

These flavors provides local ephemeral :doc:`NVMe based storage </storage/nvme-storage>` with
high performance and low-latency access times, with the limitation of being local there
is no data redundancy.

.. caution::

   The NVMe based storage is local to the hypervisor that is running your
   instance and is using a single physical disk, though NVMe based enterprise
   solid state drives has an high lifetime expectancy it's important that
   you consider this fact and **backup your data** regularly.

   See :doc:`/storage/nvme-storage` for more information.

This flavor provides the best possible storage performance for IO intensive workloads
that needs to write to disk and is great for ephemeral storage or as disk if you
have an application with data replication or redundancy already built-in.

.. list-table::
   :widths: 25 20 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Local disk (GB)
     - Flavor group
     - Version
     - CPU vendor

   * - hp.4x8-nvme50
     - 4
     - 8192
     - 50
     - High performance with NVMe
     - Version 1
     - Intel

   * - hp.4x8-nvme250
     - 4
     - 8192
     - 250
     - High performance with NVMe
     - Version 1
     - Intel

   * - hp.8x24-nvme50
     - 8
     - 24576
     - 50
     - High performance with NVMe
     - Version 1
     - Intel

   * - hp.8x24-nvme250
     - 8
     - 24576
     - 250
     - High performance with NVMe
     - Version 1
     - Intel

   * - hp.12x64-nvme50
     - 12
     - 65536
     - 50
     - High performance with NVMe
     - Version 1
     - Intel

   * - hp.12x64-nvme250
     - 12
     - 65536
     - 250
     - High performance with NVMe
     - Version 1
     - Intel

GPU
^^^

These flavors provides :doc:`GPU based compute <gpu-instances>` and allows gives you a
GPU allocated to your instance with the specified vRAM / vMEM memory.

This gives you access to a NVIDIA A10 GPU that you can use for rendering, image
processing, AI, ML and inference workloads that can run 50-200x faster on a GPU
than a CPU.

.. tip::

   If you also need local NVMe storage for your GPU workload, see the GPU with NVMe
   flavors further down.

.. list-table::
   :widths: 25 20 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - GPU vRAM/vMEM (GB)
     - Flavor group
     - Version
     - CPU vendor

   * - hp.4x8-gpu4
     - 4
     - 8192
     - 4
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.4x8-gpu8
     - 4
     - 8192
     - 8
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.4x8-gpu24
     - 4
     - 8192
     - 24
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.8x24-gpu4
     - 8
     - 24576
     - 4
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.8x24-gpu8
     - 8
     - 24576
     - 8
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.8x24-gpu24
     - 8
     - 24576
     - 24
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.12x64-gpu4
     - 12
     - 65536
     - 4
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.12x64-gpu8
     - 12
     - 65536
     - 8
     - High performance with GPU
     - Version 1
     - Intel

   * - hp.12x64-gpu24
     - 12
     - 65536
     - 24
     - High performance with GPU
     - Version 1
     - Intel

GPU and NVMe storage
^^^^^^^^^^^^^^^^^^^^

These flavors provides both GPU and NVMe storage and is great when you need
to read or write fast to local NVMe storage for your GPU workload.

.. list-table::
   :widths: 25 20 20 20 20 40 40 20
   :header-rows: 1

   * - Name
     - vCPUs
     - Memory (MB)
     - Disk (GB)
     - GPU vRAM/vMEM
     - Flavor group
     - Version
     - CPU vendor

   * - hp.4x8-gpu4-nvme50
     - 4
     - 8192
     - 50
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.4x8-gpu4-nvme250
     - 4
     - 8192
     - 250
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.4x8-gpu8-nvme250
     - 4
     - 8192
     - 250
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.4x8-gpu8-nvme50
     - 4
     - 8192
     - 50
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.4x8-gpu24-nvme50
     - 4
     - 8192
     - 50
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.4x8-gpu24-nvme250
     - 4
     - 8192
     - 250
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu4-nvme50
     - 8
     - 24576
     - 50
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu4-nvme250
     - 8
     - 24576
     - 250
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu8-nvme50
     - 8
     - 24576
     - 50
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu8-nvme250
     - 8
     - 24576
     - 250
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu24-nvme50
     - 8
     - 24576
     - 50
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.8x24-gpu24-nvme250
     - 8
     - 24576
     - 250
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu4-nvme50
     - 12
     - 65536
     - 50
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu4-nvme250
     - 12
     - 65536
     - 250
     - 4
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu8-nvme50
     - 12
     - 65536
     - 50
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu8-nvme250
     - 12
     - 65536
     - 250
     - 8
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu24-nvme50
     - 12
     - 65536
     - 50
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

   * - hp.12x64-gpu24-nvme250
     - 12
     - 65536
     - 250
     - 24
     - High performance with GPU and NVMe
     - Version 1
     - Intel

..  seealso::

    - :doc:`/compute/gpu-instances`
    - :doc:`/storage/nvme-storage`


=============
GPU instances
=============

GPU based compute enables you to run computing tasks on a GPU (graphics
processing unit).

Binero cloud uses NVIDIA A10 GPUs in our GPU instance flavors.

The key differences between GPUs and CPUs are:

.. list-table::
   :widths: 50 50
   :header-rows: 1

   * - CPUs
     - GPUs

   * - Generally work in sequence. Many cores and good task switching give
       the impression of parallelism but a CPU is fundamentally designed to run
       one task at a time.
     - Designed to work in parallel. A large amount of cores and threading
       managed in hardware enables GPUs to perform many calculations at once. 

   * - Designed for task parallelism.
     - Designed for data parallelism.

   * - Have a small amount of cores that can complete a single complex tasks at
       a high speed.
     - Have a large amount of cores that work in tandem on computing many tasks
       or calculations in parallel.

   * - Have access to a large amount of (by comparison) slow RAM at a low
       latency. CPUs are latency (operation) optimized.
     - Have access to a (by comparison) small amount of fast RAM at a higher
       latency. GPUs are throughput optimized.

   * - Have a versatile instruction set, allowing it to perform complex tasks
       in fewer cycles but is bad at some other tasks.
     - Have a limited (but highly optimized) instruction set which can perform
       the tasks its designed for efficiently.

   * - Task switching and general use-case decreases performance. 
     - Tasks switching is not used, processes serial data streams in
       parallel from A to B.

   * - Would always work for any given use-case but might not provide good enough
       performance for some tasks.
     - Would only be a valid choice for some use-cases but would in those cases
       provide good performance.

In summary, for applications such as machine learning (ML), artificial intelligence (AI) or
image processing, a GPU would likely provide a 50x to 200x increase in performance over a typical
CPU doing the same work.

Using a GPU, requires adaptation to the APIs available from the GPU manufacturer.

Binero cloud provides GPUs from NVIDIA, supporting among others OpenCL and CUDA running
on our high performance instance types.

Setting up a GPU instance
-------------------------

You :doc:`launch <launching-an-instance/index>` a GPU instance the same way as any other
compute instance with a few things to remember:

- When launching a GPU, select one of the :doc:`flavors` that include GPUs. 

- You have the option to choose a GPU with NVMe backed storage for high performing storage. This
  is not a must for GPU based compute, see :doc:`/storage/nvme-storage` for more information.

- We recommend using Ubuntu 24.04 as :doc:`image </images/index>` for your GPU based instance. This is
  because we have tested the NVIDIA driver with this image with good result. That said, its possible to
  run a multitude of images.

- When the image is up and running, you will only get 10% of the GPUs performance without a license
  installed. See below section on installing license for more information.

Installing the driver
---------------------

To use the GPU functionality, you need to install a driver from NVIDIA on the instance that has
access to the GPU. See the below instructions to install the driver.

The current NVIDIA vGPU Software Version that we are running is: **18.1**

The current latest driver we support is **Linux: 570.133.20** and **Windows: 572.83** 

For full list of supported versions click `here <https://docs.nvidia.com/vgpu/18.0/grid-vgpu-release-notes-red-hat-el-kvm/index.html>`__.

.. important::

   After installing the driver you must reboot your instance. Schedule the upgrade to allow for a
   reboot to take place. 

Linux
^^^^^

Follow the below steps to install the Linux driver in your instance.

.. note::

   The example below is using Ubuntu 24.04 as operating system. For any other Linux based operating system, the
   steps would be equal but some commands might not be identical.

   :doc:`Contact our support </general/getting-support>` if you need help installing the driver on other
   operating systems than the example below.

- Verify that the instance is able to see the graphics adapter by running ``lspci | grep -i nvidia`` which
  would return something like ``00:05.0 VGA compatible controller: NVIDIA Corporation Device 2236 (rev a1)``

- You need to install the dependencies ``g++``, ``make`` and ``dkms`` to installation and build the driver. This
  depends on your operating system, for example ``sudo apt update && sudo apt -y install build-essential dkms``

- Fetch the driver by running: ``curl -O https://binero.com/downloads/nvidia-linux-grid-570_570.133.20_amd64.deb``

- Set executable permission by running ``chmod +x nvidia-linux-grid-570_570.133.20_amd64.deb``

- Install the driver by running ``dpkg -i nvidia-linux-grid-570_570.133.20_amd64.deb``

- Verify a successful installation by reading ``/var/log/nvidia-installer.log``. The command ``nvidia-smi`` would
  give you more useful output.

- At this point, you need a valid license which `our support </general/getting-support>`_ provides you with. Its
  included in the instance cost but not assigned until requested.

- The license should added to file ``/etc/nvidia/ClientConfigToken/client_configuration_token.tok`` making sure
  to not have any extra spaces or newlines.

- Restart ``nvidia-gridd`` by running ``sudo systemctl restart nvidia-gridd``

- The command ``nvidia-smi -q | grep License`` should now return a valid license

- Install CUDA toolkit and CuDNN (optional, note that below instruction is for Ubuntu, other operating systems might
  require different packages)

::

    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-dev-cuda-12_9.10.1.4-1_amd64.deb -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-cuda-12_9.10.1.4-1_amd64.deb -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-headers-cuda-12_9.10.1.4-1_amd64.deb
    dpkg -i libcudnn9-headers-cuda-12_9.10.1.4-1_amd64.deb libcudnn9-cuda-12_9.10.1.4-1_amd64.deb libcudnn9-dev-cuda-12_9.10.1.4-1_amd64.deb

- Install TensorFlow (optional)

::

    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-dev-cuda-12_9.10.1.4-1_amd64.deb -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-cuda-12_9.10.1.4-1_amd64.deb -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/libcudnn9-headers-cuda-12_9.10.1.4-1_amd64.deb
    dpkg -i libcudnn9-headers-cuda-12_9.10.1.4-1_amd64.deb libcudnn9-cuda-12_9.10.1.4-1_amd64.deb libcudnn9-dev-cuda-12_9.10.1.4-1_amd64.deb

You are now able to run GPU based computations on your instance!

Windows
^^^^^^^

- Download the driver `here <https://binero.com/downloads/572.83_grid_win10_win11_server2022_dch_64bit_international.exe>`__.

- Run the file with administrative privileges and click through the installation.

- When the installation finishes, reboot the instance. 

- Open the device manager by running ``devmgmt.msc``.

- Under **Display adapters** the device should now be available.

- At this point, you need a valid license which `our support </general/getting-support>`_ can provide. Its included in the
  instance monthly cost but not assigned until requested.

- The license file should go in this folder: ``%SystemDrive%:\Program Files\NVIDIA Corporation\vGPU Licensing\ClientConfigToken``. More information
  from NVIDIA is available `here <https://docs.nvidia.com/grid/latest/grid-licensing-user-guide/#configuring-nls-licensed-client-on-windows>`_.

- Open services by running ``services.msc`` and restart the service ``NvDisplayContainer``.

You are now able to run GPU based computations on your instance!

Upgrading the driver
--------------------

From time to time, NVIDIA will release (and Binero will provide) and upgraded version of the GPU driver.

This is to correct potential bugs and keep the software secure. When this happens, Binero strongly recommends (and in some cases, you must upgrade
to maintain a working system) that you upgrade the driver on the your instances.

See below instructions to upgrade the driver.

The latest version of the driver that we support is **Linux: 570.133.20** and **Windows: 572.83**

.. important::

   After upgrading the driver you must reboot your instance. Schedule the upgrade to allow for a
   reboot to take place. 

Linux
^^^^^

Follow below steps to upgrade the NVIDIA + CUDA driver on a Linux based platform:

- Download the driver ``curl -O https://binero.com/downloads/nvidia-linux-grid-570_570.133.20_amd64.deb``

- Install the driver by running ``dpkg -i nvidia-linux-grid-570_570.133.20_amd64.deb``

- Reboot the system.

- Verify version by running ``nvidia-smi``.

- To upgrade CUDA, first uninstall it by running ``sudo /usr/local/cuda/bin/cuda-uninstaller`` and checking
  all options.

::

    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb
    sudo dpkg -i cuda-keyring_1.1-1_all.deb
    sudo apt-get update
    sudo apt-get -y install cuda-toolkit-12-9

Windows
^^^^^^^

Follow below steps to upgrade the NVIDIA driver on a Windows based platform:

- Download the driver `here <https://binero.com/downloads/572.83_grid_win10_win11_server2022_dch_64bit_international.exe>`__.

- Run the file with administrative privileges.

- Follow the installation instructions. 

- Reboot the system.

..  seealso::

    - :doc:`/general/getting-support`
    - :doc:`/images/index`
    - :doc:`/compute/flavors`


===========
Hard reboot
===========

General concept
---------------

If an instance is unresponsive (does not reply to commands), a hard reboot of the instance
is the same as pulling the power cord on a physical computer, a soft reboot sends a
signal to the operating system and waits for it to try to reboot on its own.

.. tip::

   Aside from actually rebooting the instance, a hard reboot will also redefine the instance
   on the hypervisor.

   This means that the instance will get the latest hardware version in the platform which is
   good for keeping compatible with the evolving ecosystem of OpenStack.

   We recommend doing an occasional hard reboot when the instance is already powered off.

Doing a hard reboot
-------------------

The hard reboot option is only available via :doc:`/getting-started/managing-your-cloud/openstack-horizon`
or :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

We recommend checking out our guides on working with these platforms if you normally only work
with the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

.. important::

   Be careful when doing a hard reboot as it will not care about the state of the operating system or
   your applications, for example any writes to the disk in the middle of a hard reboot will get
   interrupted, potentially causing data loss.

   We recommend this action as a last resort, or that you hard reboot after the operating system has halted.

Hard reboot using the OpenStack Horizon portal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To hard reboot an instance by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the dropdown button **Actions**

- Press **Hard reboot instance**

- Press the confirmation button (**Hard reboot instance**).

- Wait until the progress bar has finished.

The instance will start again (even if hard rebooted from a powered off state).

Hard reboot using the OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To hard reboot an instance by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server list``. Save the name of the instance you want to
  hard reboot.

- Run this command: ``openstack server reboot --hard [SERVER NAME]``, replacing the values
  in angle brackets by the information from the previous steps.

The instance will start again (even if hard rebooted from a powered off state).


=====================================================
Launching instances using the cloud management portal
=====================================================

To launch an `instance <../index>`_ from the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

.. note::

   If you don't have a :doc:`network </networking/network/index>` or
   :doc:`../ssh-keys` available in some of the steppes below, you might need to do
   some :doc:`initial configuration </getting-started/launching-an-instance>` first.

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Name your instance.

- Under region, **europe-se-1** is pre-selected.

- Under **Boot source**, choose the :doc:`image </images/index>` you want (the operative
  system). Here you should also choose the storage type you want (SSD or HDD).

  - If you don't select a storage type, it defaults to SSD.

  - When choosing a type, you also select a volume size for your persistent storage. 20 GB
    is the smallest size available, anything under that will still default to 20 GB but you
    are able to increase the size. See `persistent storage <storage/persistent-block-storage>`_
    for more information.

  - Press **Select** when done.

- Under **Choose configuration**, select the :doc:`flavor <../flavors>` you want.

- Under **Availability zone**, select an availability zone. We recommend **europe-se-1a** if you are
  creating you first instance (or generally if you are not sure you need to be in europe-se-1b).

- Under **SSH-keys**, select your key.

- Under **Root password**, select a complicated password. Make sure to write it down as the platform
  will not save it after provisioning.

- Under **Network**, select the network you want to connect to. This could be either a :doc:`subnet </networking/subnet/index>`
  (if you have one setup, if so the name is whatever you chose) or a :doc:`directly attached IP </networking/directly-attached-ips>` (the name contains
  the availability zone name you chose earlier). We recommend using a subnet, for more information see
  our :doc:`/getting-started/launching-an-instance` guide.

- Press **Create** when done.

The instance will take some time to deploy. When done, it will say running or active on the next page.

When the instance is running, click it and review the IP configuration under **Networking details**. There you are able
to see its IP address (both IPv4 and IPv6).

To connect use either SSH or RDP (depending on image). If you are using a subnet (and not a directly attached
IP, see above), you might need to add a floating IP to be able to connect. For more information see
our :doc:`/networking/reaching-your-instances` article.

.. note::

   The cloud management portal will not setup the :doc:`security groups </networking/security-groups/index>`
   needed to allow traffic to the instance.

..  seealso::

    - :doc:`/regions-and-availability-zones`
    - :doc:`/compute/flavors`
    - :doc:`/getting-started/launching-an-instance`


=====================
Launching an instance
=====================

To launch a new compute instance in Binero cloud, you have four main options
as outlined in the links below.

Each option have its advantages and disadvantages:

- :doc:`The cloud management portal <cloud-management-portal>` is what we recommended
  and will get a user with limited prior knowledge from A to B quickly. The tradeoff
  is that advanced features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in
  OpenStack. Some advanced features might only have a GUI implementation here.

- :doc:`OpenStack terminal client <openstack-terminal-client>` is a command line
  implementation giving terminal oriented users a quick way to access the cloud. The
  learning curve is steeper than the GUI implementation but the workflow will be
  efficient.

- :doc:`The API </compute/compute-api>` is the full OpenStack REST API. Its intended
  for users that are either writing infrastructure as code or integrate with
  third-party applications that needs to reach the APIs directly.

.. note::

   To create an instance, you will first need to know what :doc:`flavor <../flavors>` you
   want and what :doc:`image </images/index>` to boot from.

   If you have not configured SSH-keys (for Linux) or networking yet, you might want to
   do :doc:`some initial configuration first </getting-started/launching-an-instance>`.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


===========================================
Launching instances using OpenStack Horizon
===========================================

This will walk you through launching an :doc:`instance <../index>`
from :doc:`/getting-started/managing-your-cloud/openstack-horizon`.

.. note::

   If you don't have a :doc:`network </networking/network/index>` or :doc:`../ssh-keys`
   available in some of the steps below, you might need to do some
   :doc:`initial configuration </getting-started/launching-an-instance>` first.

   An SSH-key created or added in the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
   is not available in OpenStack Horizon as they use `API users </getting-started/users.html#api-users>`__
   and SSH-keys is per user account and not the project.

- Under **Project**, click **Compute** and then **Instances** in the left side menu.

- Click the **Launch an instance** button in the upper right corner.

.. caution::

   If you want to create an instance in the ``europe-se-1b`` availability zone (any availability zone that is not the default
   ``europe-se-1a``) youneed to :doc:`create the volume <../../storage/persistent-block-storage/create-volume>` in that availability
   zone beforehand and in the **Sources** step select **Boot Source** as **Volume**.

   This is because we do not support attaching volumes across availability zones and the instance and volume must be in the
   same availability zone.

- Under the **Details** tab in the dialog.

  - Enter an **Instance Name** for your instance and optionally provide a description.

  - Select an **Availability Zone**, see :doc:`Regions and Availability Zones <../regions-and-availability-zones>`.

    - We recommend the default ``europe-se-1a`` availability zone if you are creating
      your first instance and if you are not sure that you need to use a secondary
      availability zone.

- Under the **Sources** tab in the dialog.

  - Select **Image** as **Boot Source**.

  - Select **Yes** for **Create New Volume** to boot your instance from a
    volume, we only recommend selecting **No** here if you intend to use
    an NVMe-based flavor.

  - Select **No** for **Delete Volume on Instance Delete**. If you want the system
    to implicitly delete the volume together with the instance, select **Yes**.

  - Select the :doc:`image </images/index>` you want to boot your instance from
    by pressing the arrow on the right side.

- Under the **Flavour** tab in the dialog.

  - Select the :doc:`flavor <../flavors>` you want the instance to have by pressing
    the arrow on the right side.

- Under the **Networks** tab in the dialog.

  - Select the network you want to connect your instance to by pressing the arrow on the right side. This could be
    a :doc:`network </networking/network/index>` if you have one or a
    :doc:`directly attached IP </networking/directly-attached-ips>` in which case the availability zone is in the name of the network.
    We recommend using a subnet, for more information see our :doc:`/getting-started/launching-an-instance` guide.

- Under the **Key Pair** tab in the dialog.

  - Select the SSH key you want added to the instance by pressing the arrow on the
    right side. This is optional and only relevant for images and operating systems
    containing a SSH server.

- Click the **Launch Instance** button in the lower right in the dialog.

When the instance creation is complete the status will be **Active**. You can view more
details about the instance by clicking the name in the list.

To reach your instance see the :doc:`documentation </networking/reaching-your-instances>`
for that.

..  seealso::

    - :doc:`/regions-and-availability-zones`
    - :doc:`/compute/flavors`
    - :doc:`/getting-started/launching-an-instance`


===================================================
Launching instances using OpenStack terminal client
===================================================

To launch an :doc:`instance <../index>` from the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

.. note::

   If you don't have a :doc:`network </networking/subnet/index>` or :doc:`../ssh-keys` available in
   some of the steps below, you might need to do some :doc:`initial configuration </getting-started/launching-an-instance>` first.
   An SSH-key setup by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal` is not available in Horizon or
   the terminal client as they use `API users </getting-started/users.html#api-users>`__ to login and SSH-keys are account bound.

- Identify which :doc:`network </networking/network/index>` you
  want to use by running ``openstack network list``, save the name.

- Identify which :doc:`SSH-key <../ssh-keys>` you want to use by
  running ``openstack key list``, save the name.

- Identify which :doc:`flavor <../flavors>` you want to use by
  running ``openstack flavor list``, save the name.

- Identify which :doc:`image </images/index>` you want to boot from
  by running ``openstack image list``, save the name.

- Once you have all above information you are able to run the following
  command that will create and start an instance similarly as
  from :doc:`/compute/launching-an-instance/openstack-horizon`:

::

     openstack server create \
     --flavor [FLAVOR NAME] \
     --availability-zone europe-se-1a \
     --image [IMAGE NAME] \
     --boot-from-volume [VOLUME SIZE] \
     --network [NETWORK NAME] \
     --security-group default \
     --key-name [KEY NAME] \
     [NAME OF SERVER]

It will create a server with a persistent volume that **does not** get deleted along with
the instance (as would have been the case from Horizon).

You need to replace the values within brackets, as provided by previous steps. Note the disk
size value that states how big the servers disk will be.

To get the status of the provisioned server, you can use ``openstack server list`` which will
show all servers. Once the server gets status **ACTIVE**, its booting up.

Above method will create a disk with standard options. It will end up on our standard storage
tier (which is SSD) and it will not get deleted along with the instance.

.. important::

   Verify that your instance (with ``openstack server list``) have booted from a *volume* and
   not an ephemeral disk. The **Image** column in the output should say **N/A (booted from volume)**.

   Booting with a ephemeral disk by using the image as source will constraint you to the default
   size of the flavor disk.

When running ``openstack server list`` you will notice that your newly created server has an
IP address from the a subnet on the network you specified.

You might need to add a floating IP to be able to connect. For more information see our
:doc:`/networking/reaching-your-instances` article. To assign it a floating IP via the
OpenStack terminal client, see :doc:`/networking/floating-ips`.

.. note::

   The cloud management portal will not setup :doc:`security groups </networking/security-groups/index>`
   which you need to access the instance if you are using a floating IP.

.. tip::

   The OpenStack terminal client will give you helpful feedback when ending a command with ``--help``.

   For example ``openstack server create -h`` will show you the available options when creating
   an instance.

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`


==================================
Recreating an instance from volume
==================================

.. note::

   This assumes that you have booted your instance from a volume and
   not using local storage such as with the NVMe :doc:`flavors <flavors>`.

You can re-create an instance that's spawned from a volume by creating a snapshot
on the volume, creating a new volume from that snapshot and then create a new
instance from that volume.

.. caution::

   Do not delete anything before first verifying that it works.

.. note::

   The below instructions walk-through using
   :doc:`/getting-started/managing-your-cloud/openstack-horizon`.

.. note::

   The new instance will have a new IP address, you need to update any internal DNS or
   configuration where this you might reference this address.

- Shutdown the instance, preferably by logging into the instance and doing a clean shutdown
  from the operating system.

- Create a :doc:`snapshot </storage/snapshots/create-snapshot>` of the volume the instance
  boots from (also called root volume).

- Create a new :doc:`volume from the snapshot </storage/snapshots/create-volume-from-snapshot>`.

- Create a :doc:`new instance </storage/persistent-block-storage/creating-an-instance-from-a-volume>`
  from the new volume.

- :doc:`Detach </storage/persistent-block-storage/detach-volume>` any extra volumes from the
  original instance and attach them to the new instance, refer to the documentation 
  :doc:`here </storage/persistent-block-storage/create-volume>`.

- If you have a floating IP assigned to the original instance move it to the new instance.

  - Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

  - In the dropdown menu next to the old instance, press the small arrow and select ``Disassociate floating IP``.

  - Select the floating IP (if you have many, repeat these steps for all).

  - **Do not** check **Release Floating IP** as this will release the floating IP from your project.

  - Click ``Disassociate``.

  - Add the floating IP(s) to the new instance by following the instructions :doc:`here </networking/floating-ips>`.

- You are now ready to start your new instance.

.. caution::

   When deleting the old instance, first verify that it has no extra volumes
   attached (only the root volume).

- When you have verified that it works you can proceed to delete the old instance.


======================================
Compute regions and availability zones
======================================

Compute is a core service and is available in all Binero clouds
regions and availability zones.

A compute instance is local to a single availability zone with no
way to replicate between zones.

Its possible to move or copy an instance to another zone by first
creating and :doc:`image </images/index>` and then provisioning a
new instance in the new zone based on that image.

.. note::

   While its possible to use stretched networking and run images in many
   availability zones, make sure that your networking isn't reliant on a
   floating IP that is local to a single zone.

   See the :doc:`/networking/regions-and-availability-zones` documentation.

..  seealso::

    - :doc:`/regions-and-availability-zones`


==================================================
Resizing an instance using Cloud management portal
==================================================

To resize an `instance <../index>`_ by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

.. important::

   Resizing an instance will cause the instance to shut off during the
   process. We recommend doing a backup of your data before proceeding.

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you want to resize.

- In the top right corner, press **Resize** (its a small dotted square).

- Select the :doc:`flavor <../flavors>` you want.

- Press **Resize** and press **Yes**

- You will see the status in the top left corner. 

- When it says **Verify resize** (and a small triangle), login to the instance
  and verify that it worked as intended (meaning the instance is working as
  before but with the new flavor). 

- If you are happy with the result, press the small checkmark in the top
  right corner. This completes the process and make the resize permanent.

- If you are unhappy (or for some reason the resize was unsuccessful), you
  are able to instead click the small **x** (cross). This will revert the
  process.

- Accept the change by pressing **Yes**

The process is now complete. 

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
    - :doc:`/compute/flavors`


====================
Resizing an instance
====================

If your instance is not sized correctly, wether it has to little resources or
to much, you are able to resize it.

The typical situation might be that your load has increased (maybe you have added
customers to your service or maybe there is a temporary influx in visitors during
for example a campaign) to a point where your application is behaving
sluggish (or not working at all). 

Methods
-------

Resizing (which is the Binero cloud way to scale up) is changing the :doc:`flavor <../flavors>`
on your instance.

You have four main options for how to resize an instance. Each option has its advantages
and disadvantages:

- :doc:`The cloud management portal <cloud-management-portal>` is recommend and will get a
  user with limited prior knowledge from A to B quickly. The tradeoff is that advanced
  features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in OpenStack. Some
  advanced features might only have a GUI implementation here.

- :doc:`OpenStack terminal client <openstack-terminal-client>` is a command line implementation
  of OpenStack Horizon giving terminal oriented users a quick way to access the cloud. The learning
  curve is steeper than the GUI implementation but the workflow will be efficient.

- :doc:`The API </compute/compute-api>` is the full OpenStack REST API. Its intended for users that
  are either writing infrastructure as code or using a third-party application (for example Terraform)
  that needs to reach the cloud provisioning layer directly.

.. note::

   Storage performance is also important. If you've opted for our HDD based storage (which is more cost
   effective and a good choice for use cases that require storage space that is infrequently accessed)
   and notice that your application is experiencing IO wait (which translates to load and slow
   performance), a :doc:`retype </storage/retype-a-volume>` might be the solution.

Scaling methods
---------------

The two main ways are scaling by increasing performance by increasing resources such as CPU or memory
by resizing instances or scaling by adding parallel performance by creating more instances and spreading
out the workload with for example a load balancer.

Scaling up
^^^^^^^^^^

Scaling up (or vertical scaling) is resizing your instance to instantly add more resources to your
existing infrastructure.

While being quick to perform (as its purely reliant on the infrastructure platform and not the
application design), the downside is that there is an effective performance limit before diminishing
returns on adding more performance would kick in. What the limit is, depends on the use-case. 

Scaling out
^^^^^^^^^^^

The opposite method is adding parallel resources (for example more instances) and
using `load balancing </networking/load-balancers>`_) to distribute load between them, this method
is scaling out (or horizontal scaling).

For systems that expects the need to scale many times their initial size, this method is
likely required.

While there are normally bottlenecks in scale out systems as well, they are generally related to
the specific components that make up the system.

A single application (a monolith) might for example still be able to scale well by using load balanced
web servers as frontends and replicated database servers as backends. The web servers will scale pretty
much indefinitely but the database would bottleneck at some point (depending on solution). 

The same system broken into micro services instead scales by adding more resource for the specific
component in the system that needs it. A single micro service would (likely) not hit a performance
ceiling given the proper limitations in its scope. The performance of the application as a whole
is then dependent on the performance of its micro services. 

While microservices is the modern approach to application design (also providing management upsides),
systems that are not expected to grow to users in the millions, might still work well with only load
balancing or on a single server, that instead scales up.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


============================================
Resizing an instance using OpenStack Horizon
============================================

To resize an :doc:`instance <../index>` by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

.. important::

   Resizing an instance will cause the instance to shut off during the
   process. We recommend doing a backup of your data before proceeding.

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance
  you want to resize, press **Resize instance**.

- Under **New flavor**, select the :doc:`flavor <../flavors>` you want.

- Press **Resize** button.

- You will see the status of the resize in the following page.

- When it says **Confirm or Revert Resize/Migrate**, login to the instance and verify
  that it worked as intended (meaning the instance is working as before but with the
  new flavor). 

- If you are happy with the result, press **Confirm Resize/Rigrate** in dropdown to
  the far right (it should be pre-selected). This completes the process and make the
  resize permanent.

- If you are unhappy (or for some reason the resize was unsuccessful), you are able
  to instead click **Revert resize/Migrate** (available in the dropdown). This will
  revert the process.

The process is now complete. 

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
    - :doc:`/compute/flavors`


====================================================
Resizing an instance using OpenStack terminal client
====================================================

To resize an :doc:`instance <../index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

.. important::

   Resizing an instance will cause the instance to shut off during the
   process. We recommend doing a backup of your data before proceeding.

- Identify the :doc:`flavor <../flavors>` you want. You can list all
  flavors with ``openstack flavor list``.

- Identify the instance you want to resize. You can list all your
  instances with ``openstack server list``.

- To resize, run the following command: ``openstack server resize --flavor [FLAVOR NAME] [INSTANCE NAME]`` where
  you replace the items in brackets to the flavor you choose in previous step and the instance name.

- Run the command: ``openstack server list`` and you'll see the instance is now in Status **VERIFY_RESIZE**. Login
  to the instance and verify that it worked as intended (meaning the instance is working as before but with the new flavor).

- If you are happy with the result, confirm the resize with ``openstack server resize confirm [INSTANCE NAME]``,
  this completes the process and make the resize permanent.

- If you are unhappy (or for some reason the resize was unsuccessful), you are able to instead
  run the command ``openstack server resize revert [INSTANCE NAME]``. This will revert the process.

The process is now complete. 

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`
    - :doc:`/compute/flavors`


=============
Server groups
=============

Server groups provide a mechanism for indicating the locality of instances relative to
other instances. Allowing you to specify whether instances should run on the same
hardware (affinity) or different hardware (anti-affinity).

Policies
--------

We support four different policies on a server group.

- **affinity**

  - Restricts instances belonging to the server group to the same host, affinity can
    decrease network latency since instances will be on the same hardware but will also
    decrease your fault tolerance in case of an outage.

- **anti-affinity**

  - Restricts instances belonging to the server group to separate hosts, anti-affinity can
    improve your load distribution and fault tolerance in case of an outage.

- **soft-affinity**

  - Same as **affinity** but when it's not possible to schedule then together will use as
    few different hardware as possible.

- **soft-anti-affinity** 

  - Same as **anti-affinity** but when it's not possible to schedule then on different
    hardware will use as many as possible.

.. important::

   We **strongly** recommend using **soft-affinity** or **soft-anti-affinity**.

   We do regular maintenance in our platform and when using hard policies for your server
   group the system might need to power off your instance if scheduling is not possible
   due to your policy.

.. note::

   The anti-affinity policies in themselves does not provide high availability but make the
   platform aware on how you want your instance placed during scheduling, your application
   need to handle high availability by using for example many instances.

Creating a server group
-----------------------

Server groups can only be create using :doc:`/getting-started/managing-your-cloud/openstack-horizon` or
the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

.. note::

   You cannot add instances to a server group after creation. If you have an existing
   instance you can for example :doc:`re-create <recreating-an-instance-from-volume>` it.

OpenStack Horizon
^^^^^^^^^^^^^^^^^

See :doc:`/getting-started/managing-your-cloud/openstack-horizon`.

- Under **Project**, click **Compute** and then **Server groups** in the sidebar menu.

- Press **Create server group** in the top right corner.

- Choose a name for the server group.

- Choose a policy for the server group, see above.

- Press **Submit**.

The server group is now available when :doc:`creating an instance </compute/launching-an-instance/openstack-horizon>`. 

OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^

See :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

- Create the server group with ``openstack server group create --policy [POLICY] [NAME]``

- Get the UUID of the server group by listing them with ``openstack server group list``
  or showing the group you created with ``openstack server group show [NAME]``.

The server group is now available when :doc:`creating an instance </compute/launching-an-instance/openstack-terminal-client>`
using the ``--hint group=[UUID]`` parameter when using the CLI.


====================
Shelving an instance
====================

Shelving an instance allows you to save the data for your instance but release
the allocated resources required to run the instance and decrease the price you
pay for resource you're don't currently need.

Shelving an instance saves your data by uploading it to the image service and
storage such as volumes are not touched. The system saves the metadata about
your instance and releases any compute allocations.

You can at any time unshelve the instance and start it again.

.. tip::

   Shelving also works for our GPU and NVMe based flavors.

.. note::

   Shelved instances reduces your cost for allocated resources for the
   instance but you still pay for the used storage, such as root disk
   and volumes.

Shelve an instance using Cloud portal
-------------------------------------

To shelve an `instance <index>`_ from the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Click **Compute** then **Instances** in the sidebar menu.

- On the card for the instance click the dropdown menu and click the **Shelve** action

Shelve an instance using OpenStack Horizon
------------------------------------------

- Click **Project**, then **Compute**, then **Instances** in the sidebar menu.

- In the dropdown menu to the right for the instance, click **Shelve instance**.

If the instance is running, the system will power it off and then start the
shelving process.

To unshelve the instance repeat the same process but click **Unshelve instance**
in the dropdown menu.

Shelve an instance using OpenStack Terminal client
--------------------------------------------------

To shelve an `instance <index>` by using the OpenStack terminal client.

- Identify the instance you want to shelve. You can find it by listing existing
  instance with ``openstack server list``

- To shelve the instance run ``openstack server shelve [NAME]``. Replace the values
  within brackets with the name of the instance from the previous step.

If the instance is running, the system will power it off and then start the
shelving process.

To unshelve the instance repeat the same process but instead
run ``openstack server unshelve [NAME]``

.. seealso::

  - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
  - :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`


=================
Windows instances
=================

Windows runs great on Binero cloud! Some considerations when using a Windows
based :doc:`image </images/index>` on the platform. We outline them below. 

- As Windows generally heavy on memory usage, we recommend using a
  :doc:`high memory flavor version 1 (hm) <flavors>` to run your windows
  instance on.

- While Microsoft says Windows server will run on the smallest instance, for a
  good user experience we recommend at least 3 cores and 8 GB of memory.

- You can expect reasonable performance at 4 cores and 12 to 16 GB of memory. 

- As Windows Server is not open source, there is a license cost associated with
  provisioning Windows Server. See our `price list <https://binero.com/en/it-services/cloud-services/public-cloud/price/>`_
  for more information.

- While Microsoft says 32 GB is the lower limit of disk required to run Windows
  Server, we recommend using at least 64 GB. This does not include your actual
  use-case, but for the operating system itself. 

Supported flavors
-----------------

Only version 1 :doc:`flavors <flavors>` support Windows.

Provisioning
------------

Provisioning a Windows Server based instance works the same as in our
:doc:`launching an instance <launching-an-instance/index>` documentation with the
exception that Windows Server does not support SSH-keys for login.

Because of this, when you've provisioned your instance and the status becomes
``ACTIVE`` you need to set the password by using the console, see below.

Cloud management portal
^^^^^^^^^^^^^^^^^^^^^^^

To set the initial password through the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you recently provisioned.

- In the top right corner, press **Console** (its the two angle brackets ``< >``).

- You will now get the visual screen output of the server. The image might be blank because
  the server puts it in power save mode when not used. Moving the cursor or pressing the
  screen to wake it up. 

- Press CTRL + ALT + DELETE

- Press (using the cursor) the **Sign in** button.

- You will get a prompt to select a password.

You are now able to login using your new password via an RDP client by using for example
a :doc:`floating IP </networking/floating-ips>`.

Don't forget that you might need to add a :doc:`security group </networking/security-groups/index>`
for RDP access.

OpenStack Horizon
^^^^^^^^^^^^^^^^^

To set the initial password through
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance
  you want to resize, press **Console**

- You will now get the visual screen output of the server. The image might be blank
  because the server puts it in power save mode when not used. Moving the cursor or
  clicking on the screen will wake it up. 

- Press CTRL + ALT + DELETE

- Press (using the cursor) the **Sign in** button.

- You will get a prompt to select a password.

You are now able to login using your new password via an RDP client by using for example 
a :doc:`floating IP </networking/floating-ips>`.

Don't forget that you might need to add a :doc:`security group </networking/security-groups/index>`
for RDP access.

.. note::

   If you cannot see the sign-in button for any reason, restart the server.

Upgrading Windows VirtIO driver
-------------------------------

From time to time, the underlying infrastructure behaviour will change.

This could result in bad IO-performance and even IO-stalls on windows instances with
old versions of VirtIO drivers installed. Upgrading the VirtIO driver from time to
time is advisable.

To perform the upgrade, follow this below process.

.. important::

   Its advisable to backup (or at least :doc:`snapshot </storage/snapshots/create-snapshot>`)
   your volume before upgrading the VirtIO driver.

.. note::

   The process involves a reboot, we recommend that you proceed during a maintenance
   window if your instance is mission critical.

- Stop any application write IO towards the local disc. This means for example shutting down
  a database service. You do not need to quiesce all IO towards the disk.

- Download the most recent VirtIO ISO driver
  from `here <https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/>`__.

- Mount the ISO in Windows.

- Run the ``virtio-win-gt-x64.msi`` installer with administrative privileges.

- Click through it.

- When installer completes successfully, reboot your server.

..  seealso::

    - :doc:`/getting-started/managing-your-cloud/cloud-management-portal`
    - :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`


==========
Networking
==========

Networking is a core function in the In Binero cloud platform. It features a complete suite of virtual
networking functions ranging from routing, security groups (firewall) and high speed internet access to
VPNs (Virtual Private Networks) and load balancers.

The two main methods to connect to the internet is to either assign a :doc:`directly attached IP <directly-attached-ips>`
to your instance or setup a :doc:`router <router/index>` and connect a :doc:`subnet <subnet/index>`
to the router and your instance, then use a :doc:`floating IP <floating-ips>` to your instance (which is
the recommended approach).

The general design of networking in the platform is that :doc:`instances </compute/index>` use :doc:`ports <ports>` to
connect to :doc:`networks <network/index>`.

Networks in turn, have :doc:`subnets <subnet/index>` that assigns IP addresses to :doc:`ports <ports>` used by
:doc:`instances </compute/index>`.

:doc:`Floating IP addresses <floating-ips>` are publicly routed IP addresses mapped (1:1) to a port to either
publish services on the internet or provide servers with access to the internet. 

See the sections in this article to get a good understanding of the networking possibilities
in the platform.

If you are looking to get started with networking, we recommend our :doc:`getting started guide <../getting-started/launching-an-instance>`
that will guide you through setting up a versatile solution by using a single subnet behind a router. 

.. toctree::
  :caption: Available services
  :maxdepth: 2

  network/index
  subnet/index
  ports
  security-groups/index
  router/index
  floating-ips
  directly-attached-ips
  mtu
  load-balancer/index
  reaching-your-instances
  client-vpn/index
  site-to-site-vpn/index
  custom-firewall-router
  reverse-dns-ptr
  regions-and-availability-zones
  network-api


========
Networks
========

General concept
---------------

A network in Binero cloud is a software-defined network.

The network will connect to :doc:`ports </networking/ports>` on routers or
instances just as a physical server might connect to a port in a switch.

Networks are the carriers of traffic in your infrastructure and would normally
enter or leave through a :doc:`router </networking/router/index>` (for
routing to another network or the internet) or an :doc:`instance </compute/index>`. 

Networks are what connects the infrastructure together. Assuming you want to use
only manual IP assignments on your infrastructure, creating networks and ports would
be enough.

You add one or more :doc:`subnets <../subnet/index>` to a network to provide IP
addressing for ports.

.. note::

   You can manage IP addressing manually with a single network, but using OpenStack
   networks with configured subnets provides a better user experience and some platform
   services depend on it.

Managing networks
-----------------

You are able to manage networks by using either of the below tools.

- :doc:`The cloud management portal <cloud-management-portal>` is what we recommended
  and will get a user with limited prior knowledge from A to B quickly. The tradeoff
  is that advanced features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in
  OpenStack. Some advanced features might only have a GUI implementation here.

- :doc:`OpenStack terminal client <openstack-terminal-client>` is a command line
  implementation giving terminal oriented users a quick way to access the cloud. The
  learning curve is steeper than the GUI implementation but the workflow will be
  efficient.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


===================================================
Managing networks using the cloud management portal
===================================================

Create a network
----------------

To create a :doc:`network <index>` from the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Networks** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Under region, *europe-se-1* should be pre-selected.

- Name your network and optionally provide a description.

- Under **Availability zone (hints)**, select the :doc:`availability zone </networking/regions-and-availability-zones>` that
  your router or instance to which you want to connect the network to.

- Under **Select external network**, choose the network that corresponds to the availability zone you selected. 

- You should set **Admin state** to up.

- Press **Create** in the lower right corner.

.. note::

   To connect an instance or router to the network, you will also need to create a :doc:`subnet <../subnet/index>`
   which you can then attach to an interface on a router or an instance.

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`../index`


===========
Network API
===========

To manage networking functions to for example launch a :doc:`router/index` via the OpenStack
API you need to use the Neutron API.

We do not provide a complete documentation for how to do API calls but
rather `link to the correct official documentation <https://docs.openstack.org/api-ref/network/>`_. 

.. note::

   Please read our `general OpenStack API documentation :doc:`/openstack-api` as a good starting
   point for working with the API.


==========
Client VPN
==========

A client VPN is a VPN that connects individual computers to a VPN server and then
routes some (or all) network traffic through a tunnel between the computer and the
VPN server.

Your VPN traffic is safe from third parties as they cannot read the encrypted
data while its in transit.

Generally speaking, a client VPN serves a single client (user) but you can also
configure it to work as a :doc:`../site-to-site-vpn/index` instead, depending on
how the client in the other end of the tunnel is setup (the server would work
the same way).

In Binero cloud, you are able to provision a client VPN server to which you could
then connect and as such, reach the inside of your network through a tunnel.

The VPN service uses OpenVPN technology, which is an open source based VPN suite
that runs on an instance on your cloud. At no extra cost, other than the actual
instance hourly cost, to setup a VPN service in our cloud. 

The VPN service will come with its own management interface (web based) that enables
you to manage users, change security settings, add 2FA authentication and so on.

Complete documentation of all the features our VPN solution provides is out of scope for
our support pages but more information is available
`here <https://www.pfsense.org/get-involved/>`__ and on the web in general.

See the subsections of this section for the most common information relating to the
service. Our :doc:`support </general/getting-support>` can also help you out.

.. toctree::
  :caption: Available services
  :maxdepth: 2

  setting-up
  initial-configuration
  user-modes
  managing-users
  logging-in
  networks
  security-settings
  maintenance


=====================
Setting up client VPN
=====================

You can only provision VPN from the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

Follow these steps to setup client VPN by using the cloud management portal:

- Press **VPN** and then **OpenVPN** in the sidebar menu.

- Click the **+** (plus) symbol in the lower right corner.

- Give your VPN service a name and optionally a description.

- Select your :doc:`SSH-keys </compute/ssh-keys>`.

- Under **Admin IP ranges**, enter the IP ranges (comma-delimited or a single
  range) that should be able to do initial VPN configuration, see below.

  - This should be one ore more subnets in :doc:`CIDR notation <../subnet/subnet-format>`. We
    recommend your current public IP address. If you use a single IP address, it needs to use
    a  ``/32`` CIDR suffix.

- Under **Private network**, select the :doc:`subnet <../subnet/index>` that you want to access.

  - This is also the network added to the VPN instance. If you want to reach networks, you can
    :doc:`add more networks <networks>` later on. The network you choose needs to have a router
    connected as the instance will use a :doc:`floating IP <../floating-ips>`.

- Under **pfSense instance flavor**, select the compute flavor you want for your VPN. We recommend
  the default ``gp.1x2`` for implementations up to 10 users for management use. For more users,
  consider adding an extra core per 10 users. For performance oriented users, we recommend the
  ``hp.4x6`` flavor.

- If you want backup, press the checkbox and enter the amount of days.

- Press **Create**. VPN creation takes considerably longer than setting up a standard instance. This
  is due to the (comparably) long first time bootstrap of the VPN instance when generating ciphers
  and so on.

- You are now ready to proceed to do the :doc:`initial configuration <initial-configuration>` of the
  service.

.. note::

   The **admin IP range** value will setup a :doc:`security group <../security-groups/index>` that will
   allow access to the new instance for initial web-management (for example downloading the VPN client
   configuration).

   Once done, you can further manage changes after first logging into the VPN.

   You can alter the security group from the portals in case you would originate from another IP in the
   future and for some reason cannot start the tunnel before login into the VPN interface. Note that this
   should be an IP (or network) in CIDR notation.

.. note::

   Some steps included in the setup might hit a predefined quota (for example if you cant create more
   security groups or floating IP addresses), :doc:`contact our support </general/getting-support>` to
   get your quota increased.

   If the installation fails, you are able to get the reason by clicking the service and checking
   error messages. 


===================================
Initial configuration of client VPN
===================================

When the VPN service stack says completed (``CREATE COMPLETE``), open its
info view by clicking the service card or list entry.

You will see the URL to the VPN WebUI presented. Copy the value from the
``mgmt_url`` output and open the URL in your web browser.

As the WebUI uses a self-signed certificate by default, your browser will
display a warning before opening the site. The default username and password
for the WebUI is ``admin`` and ``pfsense``.

.. note::

   You can only access the WebUI from the IP address or network that configured
   in **Admin IP ranges** during the :doc:`setup of the VPN <setting-up>`.

   If you cannot reach the WebUI, its likely that you are connecting from another IP
   address than the one you choose during setup.

   Proceed to edit the :doc:`security group <../security-groups/index>` on the VPN
   :doc:`instance </compute/index>` so that it includes your current IP address (or
   **temporarily** remove the security group, making sure to re-add it once you're
   done and have downloaded the client configuration).

Once logged in, read and accept pfSense EULA. After this, your first task should be
to change the admin accounts password.

There should be a warning on the top of the screen with a link to change the
password. If not, its available under the menu **System** then **User manager**,
press the small pen icon next to the admin user. 

.. important::

   Not changing the default user password is a **major** security risk as this login
   combination is widely known as default credentials.

Your next task is to choose a user mode. The default mode is using a certificate
and TLS key to secure traffic with no user authentication.

An alternative mode (also available) will allow for individual users to authenticate. We
strongly recommend reading the :doc:`user-modes` article before proceeding as changing the
mode later will require new client configurations for the VPN users. 

If you want to continue with the default user mode and login to your new VPN, see the
:doc:`logging-in` article.


========================
Logging in to client VPN
========================

To use your client VPN you will need a client software running on your computer.

Which software will vary based on what operating system your computer is running as
well as your preferences.

Below we will walk-through ways to get up and running, the general idea is to import
a client configuration file that the VPN server provides into the client VPN application.

The client configuration will contain the necessary keys, certificates and settings to
be able to authenticate and start the tunnel

.. important::

   The exported client configuration file contains sensitive information that an attacker
   can use to gain access to your VPN.

   You should treat it as sensitive information according to your policy. We recommend
   that you do **not** email the configuration to users but rather put it onto a flash
   drive or using a secure file transfer service to distribute it.

Downloading configuration
-------------------------

To download configurations from the VPN server.

- :doc:`Login <initial-configuration>` to the WebUI of the client VPN. If you don't already
  have a VPN account that works, you will need to login from the admin IP address used in
  the initial configuration. See :doc:`setting-up` for more information.

- Press **VPN** and then **OpenVPN** in the main menu.

- Press the **Client Export** tab.

- Scroll down to **OpenVPN clients**.

- Download the appropriate client or configuration (see below). If you run the VPN with user
  based authentication, download the configuration for the proper user - they will contain
  different keys and certificates. You are able to use the search feature directly above to
  search for a specific user.

Once you have a client configuration setup, you should be able to login to your VPN and
:doc:`reach your cloud network <networks>` directly from your client computer.

Windows based clients
---------------------

The VPN server will provide a pre-configured installer for Windows computers.

This installer will install a client software with an already included configuration setup
which makes it more pleasant for a user to get the VPN connection up and running.

To download this configuration, follow the instructions above and select the appropriate
(most likely 64 bit) version under **Current Windows Installer** section.

Assuming you already have a OpenVPN client installed and prefer to use that software, instead
download the configuration named **Most clients** under **Inline configurations** section.

You can then import it into the client software by double clicking the file or right clicking
it and selecting **import**.

If neither of those works, see relevant documentation on your specific client for how to import
configurations into the client.

Mac clients
-----------

Clients available for Mac that should work well, one client is `OpenVPN Connect <https://openvpn.net/client/>`__
and another client is `Tunnelblick <https://tunnelblick.net>`__.

Other clients
-------------

Generally, the most clients configuration will work on most clients. To download it, follow the
instructions under the **Downloading configuration** and select it, after which you should
be able to import it by following the documentation from your software provider.


======================
VPN server maintenance
======================

To remain a secure solution and to keep working as intended, we recommend
maintaining and updating the VPN server.

Below we show some important tasks and how to manage them.

Updating the VPN server
-----------------------

The VPN server software is `pfSense <https://www.pfsense.org>`__ which in turn
is running on the FreeBSD operating system.

pfSense will provide updates that will include security fixes and new functionality.

We strongly recommend updating the VPN server once new releases are available to keep
it secure over time.

If you do not update your installation you risk falling to far behind to be able to
update and be vulnerable to security vulnerabilities.

To update the VPN server by using the VPN WebGUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **System** and then **Update** in the main menu.

- Ensure that the Branch is **Latest stable version**.

- Wait for the Status field to say that the server is up to date or not.

- If the server is not up to date, follow the instructions to update. 

.. note::

   The server will reboot after the update. Its advisable to first :doc:`snapshot </storage/snapshots/index>` the
   server in case the update should fail or something should not work afterwards. 

Re-issue certificates
---------------------

When you created the VPN server it generated new certificates but because some clients
doesn't allow certificates valid for longer than a year, you need to reissue these on
a yearly basis.

To re-issue certificates by using the VPN WebGUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **System** and then **Cert manager** in the main menu. 

- Press the **Certificates** tab under the main heading (the default menu is the CA
  menu, do **not** re-issue the CA).

- By default there are three certificates installed on the system; **OpenVPN Server**,
  **OpenVPN User** and ``webConfigurator default``. Aside from this, if you've added
  users these will also be in the list. You need to reissue the first two certificates.

- Press the small round arrow (re-issue / renew) to the right of each (one at the time)
  of the OpenVPN server and OpenVPN User.

- Press the **Renew/Reissue** button.

- Verify the **Valid until** date.

.. important::

   The **OpenVPN User** certificate is for the :doc:`default mode <user-modes>` user
   setup. When renewing this certificate, new :doc:`client configurations <logging-in>`
   will need to be setup in the VPN users client software.

   The **OpenVPN Server** certificate will not require new client configurations but
   renewing it will **restart the service** which will disconnect active clients. 

.. note::

   User certificates is valid for 10 years. When they expire, you will also need to
   renew them. This will also require new VPN configuration for the users who use the
   certificates.


============================
Managing users of client VPN
============================

You are able to use different authentication mechanisms with the client VPN service.

It supports both LDAP and Radius (which in turn will support many other mechanisms
such as 2FA and so on). The scope of this support article will only include local
users. 

Add a user
----------

To add a user, using the VPN WebGUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **System** and then **User manager** in the main menu.

- Press the **+Add** button. 

- Enter a username (firstname.lastname is common).

- Enter a full name. 

- Enter a password and verify it. 

- Check the **Click to create a user certificate** checkbox.

- Copy the username to the **descriptive name** field under **create certificate
  for user**.

- Press **Save**.

Remove a user
-------------

To remove a user, using the VPN WebGUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **System** and then **User manager** in the main menu.

- Mark the checkbox in front of the user you want to remove.

- Press **Delete** button on the bottom.

.. note::

   A deleted user will not disconnect immediately from the VPN. If that's needed for
   any reason, do so by pressing **Status** and then **OpenVPN** in the main menu, and
   then pressing the cross (``x``) next to the user. 


======================================
Reaching cloud networks via client VPN
======================================

General concept
---------------

When connecting to a VPN, the VPN server will push routes for the networks configured
to the client. The client will install these routes into the clients routing table so
that traffic for the networks get sent over the VPN tunnel.

Exactly how the client handles routes will vary between clients and operating
systems. The default behaviour of the VPN service is to push a route for the
network that you've chosen during the :doc:`setup process <setting-up>`.

Adding more networks
--------------------

If your cloud infrastructure contains more than a single :doc:`subnet <../subnet/index>`
that you want to reach over the VPN, you need to manually add that subnet to the VPN
servers configuration.

To add a network to the VPN server, using the WebUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **VPN** and then **OpenVPN** in the main menu.

- Click the small pen symbol (edit server) on the only row (assuming you have not added
  more servers) under **OpenVPN servers**. 

- Scroll down to the **Tunnel settings** section. 

- Change the value in the field **IPv4 Local network(s)** to include all networks comma-delimited
  and in :doc:`CIDR notation <../subnet/subnet-format>`. Your initial subnet should be in this
  field already.

- Press **Save** at the bottom. 

- When users login again, they can reach all the new networks assuming its correctly
  :doc:`routed <../router/routing-between-networks>` in the cloud.

.. important::

   Changing the VPN server configuration will **restart** the VPN service. This will in turn
   disconnect all active sessions. The procedure is quick but will be disruptive. 

VPN-server network design
-------------------------

The VPN server only has a single interface. This interface, in turn, has a floating IP connected
to it.

Ingress from the internet is via the floating IP which will :doc:`DNAT <../router/nat>` traffic
to the IP address on the subnet.

Egress from the VPN server to the internet will SNAT traffic to originate from the floating IP. 

For internal traffic within the network(s), the VPN server will use the same interface that it
uses for public traffic but it will not egress the platform.

The VPN server will SNAT all VPN traffic, which by default will originate from the ``10.0.8.0/24``
range that the VPN server uses for VPN clients, so that the source IP is the VPN servers own
IP address on the subnet.

This means that it will appear as if the traffic from the VPN clients originates from the VPN
servers IP on the subnet. 

.. note::

   This behaviour is changeable for users wanting to expose what VPN client IP the connection was
   from but we do not recommend it, as it will mean setting up routes for the ``10.0.8.0/24``
   network on all hosts and on the router. 

To reach other networks in the cloud, you use routing.

The VPN server will use its default route (which is the upstream router) for all traffic that is
outside of its own subnet.

The router will then route this traffic the same way it would any internal traffic in the
cloud, this is a good way to reach your entire cloud infrastructure over VPN.

.. note::

   Because the VPN server uses NAT some applications can work badly with NAT due to the
   design application or protocol itself, for example active mode FTP can have issues over
   the VPN. 


===============================
Security settings in client VPN
===============================

The default configuration allows any number of clients to access the VPN
service by using the same credentials (one user certificate + the TLS key).

All required certificates are automatically generated (and are thus unique)
for each deployment of the service.

Cipher selections and algorithms used for the generated keys follow the
guidelines as outlined in the `CNSA suite <https://apps.nsa.gov/iaarchive/programs/iad-initiatives/cnsa-suite.cfm>`_
for protecting data at *TOP SECRET* level.

For even higher security, you might choose to add user specific credentials
through the pfSense control panel, see :doc:`managing-users` for instructions.


==========
User modes
==========

While there are many modes of operation possible on the VPN, there are two main modes that
we recommend, the difference being wether to require individual user authentication.

Both modes will provide :doc:`strong encryption <security-settings>` but aims to provide
solutions for different use-cases. 

Default configuration
---------------------

In its default configuration, the VPN service does not rely on user accounts but rather a
certificate and a TLS key shared among its clients.

This is a good starting point as you can share the VPN configuration among users and is identical
for all users, you don't need any user management and getting one or more users enrolled to the
VPN is as fast as importing the configuration into the client and connecting.

Once connected, it will provide a secure communication to the inside network where authentication
would (most likely) still enforce a username and password (or key).

Having a VPN greatly simplifies :doc:`connecting to your instances <../reaching-your-instances>`
and communication is secure.

A potential downside to this setup is that its impossible to deactivate a user from the VPN that
already have the configuration without changing the certificates or the TLS key for all users.

In case the internal authentication protocols (once connected to the VPN) are not considered a
strong enough security level, this then becomes a cumbersome process when for example an employee
leaves or a consultant stops working on a project where, had there been user authentication, you would
normally remove the user and disconnect any existing sessions.

.. note::

    Remember that it's best practice to renew certificates and keys at a regular interval which
    further makes this a cumbersome solution.

Individual users
----------------

An alternative way to authenticate users is by setting up individual users with passwords and unique
certificates to increase the layers of security.

This allows you to remove a user should the need arise and there is then no need to replace the
certificates and keys for all users. 

To change the operating mode to include user authentication, using the VPN WebGUI.

- :doc:`Login <initial-configuration>` to the VPN management interface.

- Press **VPN** and then **OpenVPN** in the main menu.

- Click the small pen symbol (edit server) on the only row (assuming you have not added more servers)
  under **OpenVPN servers**. 

- Under section **Mode configuration** (close to the top), change **Server mode**
  to ``Remote Access ( SSL/TLS + User Auth )``

- Scroll down to **Duplicate Connection** under section **Tunnel settings** and disable it (by removing the
  checkbox) if you only want to allow one simultaneous connection per user. This will enforce that a single
  user is not sharing their login (willingly or otherwise) as just a single login at the time per user account
  is possible.

- Press **Save** at the bottom.

You are now ready to :doc:`create some users <managing-users>` to login to your VPN. 

.. important::

   If you've changed from using the default mode, you will need to ensure that you
   also :doc:`update your client configuration <logging-in>` before logging in again.

   The configuration will be different on the client side with new per user certificates.

.. note::

   When removing a user, the user will not disconnect or logout automatically but rather
   cannot log back in once disconnected. It's possible to manually disconnect a user via
   the web interface under **Status** and then **OpenVPN**.


=========================
Custom firewall or router
=========================

Sometimes its preferred to use a network stack that is familiar to an admin or that provides
a function that's needed not available in a :doc:`router <router/index>`.

You have many options both open source and proprietary that can run as a router or firewall
with good performance and with rich feature-sets. These systems are normally installed as
instances in a cloud and Binero cloud supports this approach.

.. tip::

   We do not recommend mixing a custom router or firewall with :doc:`router <router/index>`. While
   its possible to do so, this will cause to make the network more difficult to understand.

   As such, rather give the instance running as your own router a port to each of the
   subnets that you want to reach and manage everything within the your router.

Concepts
--------

When working with a custom router, the main difference from the platform perspective is the
:doc:`port <ports>` configuration, particularly the port security setting.

Since the platform is (normally) aware of what IP addresses are existing behind each port, it
filters traffic based on what it knows.

This enables it to protect from spoofing attempts and its also a prerequisite for
using :doc:`security groups <security-groups/index>`. 

When using a custom firewall or router, the port security feature does not work as destination
traffic is no longer meant for the instance (a router receives traffic and forwards it - the
destination is something other than the router itself). 

Aside from the above its important to note that the usual manner of handling IP address in the
platform is :doc:`DHCP <subnet/dhcp>` on a :doc:`network <network/index>`.

It might be tempting to run a DHCP server on the custom router instead of using manual
networking. While this works, this will remove cloud-init support, resulting in instances
no longer being correctly provisioned. 

Setting up a custom router or firewall
--------------------------------------

Below is a guide on how to setup a router with a public (internet facing) outside network and a
subnet (which will work the same as with a router) for hosting instances on the inside
of the router. 

.. note::

   Since some features below are only available using :doc:`/getting-started/managing-your-cloud/openstack-horizon`
   (or :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`), we will use
   OpenStack Horizon below.

- Create your :doc:`networks <network/index>` and :doc:`subnet <subnet/index>`
  for the internal network(s) unless you have already done so. 

- Create a network :doc:`port <ports>` on the subnet. 

  - Select fixed IP and add input the same IP as you selected as default route on the
    subnet in the previous step.

  - Under :doc:`security groups <security-groups/index>`, add the ``all-open`` group (as the
    traffic will not be destined to the instance because of routing). A firewall to the
    router/firewall-instance will have to be setup on the instance itself.

  - Edit the new port and select the **Allowed address pairs** tab

  - Add ``0.0.0.0/0`` under the **IP Address or CIDR** field.

- Launch the instance using an image or install it via console. 

  - Add a :doc:`directly attached IP <directly-attached-ips>` as the outside network.

  - Connect to the port(s). Do not connect to the inside subnet, just the port(s) that
    you already created. 

  - Select the ``all-open`` security group unless you want to filter traffic to the router in
    the platform (its recommended to use this feature in the router/firewall as that will
    simplify working with it.

When you have launched the instance:

- The outside (external) interface should use DHCP and get a statically directly assigned IP-address.

- The internal interface could use either a static address (use the fixed IP on the port) or a
  DHCP provided address, we recommend using a static address here.

- Setup SNAT (see :doc:`router/nat` for more info) on the instance. This depends on your solution
  and is out of scope for this article.

Your instances located behind this instance, and that use an IP on this instance as default route, should
now be able to access the internet through it and you should be able to redirect (DNAT) traffic into the
instances.

If its not working, the following are some tips to check connectivity:

- Verify that the router can reach the instances directly and vice versa. 

- Verify the security groups on all instances. The firewall/router instance should have ``all-open`` on
  all ports and ``default`` on the internal ports). If the firewall does not have ``default``, other instances
  will not accept traffic from it and if it does not have the ``all-open`` group, it will not access traffic
  from the other instances. 

.. note::

   The platform uses a smaller MTU (1450) than is standard. You might need to change this, see
   the :doc:`mtu` documentation.


==============================
Directly attached IP addresses
==============================

Binero cloud provides two main ways to use public IP addresses, directly attached IP
addresses and :doc:`floating-ips`.

You assign directly attached IP addresses directly on the :doc:`port </networking/ports>`
for :doc:`instances </compute/index>`.

This means that when you run for example ``ip addr show`` or ``ipconfig /all`` inside your instance
operating system, you would (with a directly attached IP) see the public IP assigned on the interface.

When using a floating IP, the result would instead be the IP address from the :doc:`subnet <subnet/index>`
that you assigned to the instance. The floating IP is then redirected using NAT to and from the instance in
the router.

Another key difference is that a directly attached IP is the only way to consume a public IPv4 *without* using
a :doc:`router <router/index>`. A router cannot use a directly attached IP or route traffic for one.

Its possible to connect a router to an instance that has a directly attached IP but since they will both provide a
default route, it will require configuration of the instance to use a manual IP and also use static routes on the
instance facing the router. This is not a recommended approach.

.. note::

   Directly attached IP addresses are not designed to work well with :doc:`routers </networking/router/index>`
   but for a single instance that want a direct internet connection and nothing else. 

.. warning::

   A port with a directly attached IP address has its lifecycle tied to the instance, when you detach
   the interface or delete the instance the port gets released back into the pool.

   If need a more permanent IP address allocation use a :doc:`Floating IP <floating-ips>`.

Key differences to floating IP addresses
----------------------------------------

- Since a port with a directly attached IP is setup on the instance, there is no need for NAT. This might be an upside
  for some applications that does not work well through NAT or admins that want to manage their entire network stack
  on the instance.

- Directly attached IP addresses have a slight performance improvement over :doc:`routers </networking/router/index>`
  because of no NAT and no :doc:`network </networking/network/index>`. This performance will be negligible in most
  use-cases.

- Directly attached IP addresses does not combine with the majority of the networking functions in the platform which
  will rely on a router to work, for instance the :doc:`load-balancer/index` service.

- Instances associates directly attached IP addresses with its lifetime, so removing the interface or deleting the
  instance releases the port and IP allocation, use a :doc:`floating IP </networking/floating-ips>` if you a more
  permanent IP allocation.

.. note::

   For a more versatile approach to networking where you keep the IP allocation, we recommend using :doc:`floating-ips`.

   The primary use-case for directly attached IP addresses are single instances that need a less complicated method to
   reach the internet and publish services from an instance to be available on the internet.

Setting up a directly attached IP
---------------------------------

The process for setting up a directly attached IP on an instance is not differing from any other
:doc:`method of setting up an IP </compute/assign-ip>` in the platform except you would chose an
IP from one of our external ranges instead of selecting one from a :doc:`subnet <subnet/index>`.

Which network to choose would depend on in which :doc:`availability zone <regions-and-availability-zones>`
your instance is running in.

- ``europe-se-1-1a-net0`` for instances placed in *europe-se-1a* availability zone

- ``europe-se-1-1b-net0`` for instances placed in *europe-se-1b* availability zone

Doing so, would place a public IP directly on the interface (NIC) of the instance and you would be able to see it by
running for example ``ip addr show`` in Linux.

..  seealso::

    - :doc:`floating-ips`
    - :doc:`regions-and-availability-zones`
    - :doc:`router/index`


=====================
Floating IP addresses
=====================

Binero cloud provides two ways to use public IP addresses , using
:doc:`directly attached IP addresses <directly-attached-ips>` or
Floating IP addresses.

Directly attached IP addresses is a public IP addresses assigned directly on
the :doc:`port </networking/ports>` of an :doc:`instances </compute/index>`. This
means that when list the IP address in your operating system you would see the
public IP address assigned to the network interface.

When using a floating IP, the result would instead be the IP from the
:doc:`subnet <subnet/index>` that you assigned to the instance.

The floating IP is then diverted to and from the instance through the
router that performs :doc:`router/nat`.

Floating IP addresses are, as the name implies, not tied to a single instance but mapped to
a port (which has an IP from a :doc:`subnet <subnet/index>`).

They are floating in the sense that you can move them between instances with zero
configuration required on the instance.

A floating IP is local to and owned by the project, even if its not actively used on
any instance it's still allocated to you.

Using a floating IP makes the IP address portable within the platform and if you have
third party dependencies such as using the IP address in remote firewall configurations
it's the preferred way to use public IP addressing as you keep the floating IP until you
delete the allocation.

Floating IP addresses require a :doc:`router <router/index>` as its the router that does
the mapping. On a technical level the router uses :doc:`router/nat` in both directions:

- Incoming traffic from the internet would be a destination NAT (DNAT) to the
  subnet IP from the floating. 

- Outgoing traffic to the internet would be a source NAT (SNAT) to appear as
  if originating from the floating IP. 

Key differences to directly attached IP addresses
-------------------------------------------------

- A floating IP is not visible (or indeed configured) on an instance. This means any application
  that use a floating IP, will have to work with NAT (which the majority of applications do).

- A floating IP uses a :doc:`router <router/index>` where directly attached IP addresses does not
  giving a small performance penalty that is negligible and insignificant.

- Some features in the platform require floating IP addresses and will not work with directly attached
  IP addresses such as the :doc:`load balancer </networking/load-balancer/index>` service.

Setting up a floating IP
------------------------

You assign floating IP addresses to instances (and in particular, to a subnet IP on an instance). Depending
on what tool used, you might need to add floating IP addresses to the project first.

.. important::

   When assigning a new floating IP, make sure you assign one from the same :doc:`availability zone <regions-and-availability-zones>`
   that the instance or service you want to use it on.

.. note::

   A floating IP is a service and a incurs a cost per IP address allocated to your project. If you have floating
   IP addresses no longer used, make sure to remove them to release the IP back to the pool. 

.. note::

   If you have many internal IP addresses on an instance, you need to assign the floating IP to the internal IP of
   the port that is the default route for the instance. If you are unsure, assign the floating IP and if it does not
   work, remove it and try another internal IP.

Adding floating IP addresses using the cloud management portal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a floating IP to an instance by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Network** and then ``Floating IPs`` in the sidebar menu.

- Verify that you have a free floating IP. It should say **Down** under the IP.

- If not, assign a new IP by clicking the **+** (plus) symbol in the lower right corner.

  - Under **Choice of network**, select a network from the same availability zone as the instance you want to assign the IP to. 

  - Optionally give the IP a description.

  - Press **Create**.

  - Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you want to add a floating IP to.

- Press the **more** icon (looks like three small dots) in the top right.

- Press **Add floating IP**.

- Select your new (or old, if you had one already) IP under ``fixed IPs``.

- Under **Available ports**, select the internal IP to map the floating IP to. 

- Press **Associate IP**.

- Your IP should now be visible under the **Networking** tab. Remember that you might need
  to add :doc:`security groups <security-groups/index>` to the instance if you cannot
  reach it.

Adding floating IP addresses using OpenStack Horizon
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a floating IP to an instance by using the
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then ``Floating IPs`` in the sidebar menu.

- Verify that you have a free floating IP. It should say ``-`` under the column **mapped fixed IP address**.

- If not, assign a new IP by clicking the **+** (plus) symbol in the lower right corner.

  - Press ``Allocate IP to project`` in the top right corner.

  - Under **Pool**, select a network from the same availability zone as the instance you want to assign the IP to. 

  - Optionally give the IP a description.

  - Press ``Allocate IP``.

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance you want to add the floating
  IP to, press **Associate floating IP**.

- Select your new (or old, if you had one already) IP under **IP Address**.

- Under ``Ports to be associated``, select the internal IP to map the floating IP to. 

- Press **Associate**

- Your IP should now be visible under the **IP address** column. Remember that you might need
  to add :doc:`security groups <security-groups/index>` to the instance if you cannot reach it.

Adding a floating IP using OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a floating IP to an instance by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack floating ip list``

- If there is an IP that says ``none`` under the **fixed IP address** column then that's available.

  - We recommend also figuring out if the floating IP is from the right availability zone, you can do
    this by running ``openstack network show [ID]`` where ID is the UUID from the **Floating Network**
    column in the previous command. Its important that you use a floating IP from the same availability zone. 

- If there was no available IP, assign one to the project: 

  - Run this command: ``openstack network list --external``. Note the name of the network that is in the
    availability zone that you want to use the floating IP (for example *europe-se-1-1a-net0*). 

  - Run this command: ``openstack floating ip create [NETWORK NAME]``, replacing [NETWORK NAME] with the
    name of the network from the previous step.

- Run this command: ``openstack floating ip list``, note the new IP address.

- Run this command: ``openstack server list``, note the name of the instance you want to
  assign the floating IP to. 

- Run this command: ``openstack server show [NAME]``, replacing [NAME] with the name of the server from
  previous step. Note which address from the **addresses** field you want to connect the floating IP to. 

- Run this command: ``openstack server add floating ip --fixed-ip-address [INTERNAL IP] [INSTANCE NAME] [FLOATING IP]``, replacing
  [INTERNAL IP] with the instances IP from the previous step, [INSTANCE NAME] with the name of the instance and [FLOATING IP] with
  whichever floating IP you added as per earlier step.

- Your IP should now be visible under the **addresses** field when running the command ``openstack server show [NAME]``. Remember
  that you might need to add :doc:`security groups <security-groups/index>` to the instance if you cannot reach it.

..  seealso::

    - :doc:`directly-attached-ips`
    - :doc:`regions-and-availability-zones`
    - :doc:`router/index`


===============
Health monitors
===============

Health monitors checks if a service on a member works as intended.

As such, they are protocol aware (for example by checking a website
by using the HTTP protocol).

You can configure monitors in many ways:

- Protocol type (you cannot change it after deployment).

- Maximums and limits before considering the service down and removed
  from the pool.

- Protocol aware settings such as URLs, paths and codes.

You define a ``health monitor`` per listener. We recommend ensuring that the
``health monitor`` closely matches the listener type.

If you are for example by using load balancing for an HTTP based service, don't
do health checking by pinging the members as this will have almost no bearing on
whether the individual members are working or not and will probably cause faulty
members to remain in the pool.

..  seealso::

    - :doc:`../recommendations`


==============================
Load balancing in Binero cloud
==============================

Load balancers consists of the following constructs:

- The **Listener** is the load balanced service as its presented to
  users. It listens for connections on a port, for example TCP port
  80. You can add one or more listeners to a load balancer after its
  created.

- The **Pool** is the a collection of member instances. It also sets
  the *common configuration* for the pool members included in the pool.

- The **Pool members** are also called backend (or real servers). These
  are compute instances that run a service (such as HTTP for web
  application) to which the load balancer sends requests if the
  member is healthy.

- **Health monitors** checks that a member is healthy, if a member is
  healthy the load balancer will send traffic to the member.

You can read more about the different parts of a load balancer in
their own section.

Load balancers in Binero cloud are highly available and redundant.

You are only able to setup configuration for one protocol (meaning
service type or a TCP or UDP port) at a time, so if you for example
want both HTTP and HTTPS setup, you would start with one (probably
HTTPS, maybe using our :doc:`../ssl-termination` guide) and then setup
the other (probably HTTP with HTTPS redirection) as a *separate listener*
with (if applicable) a separate pool (that could well distribute load to
the same pool members).

.. toctree::
  :caption: Available services
  :maxdepth: 2

  listeners
  pools
  pool-members
  health-monitors

..  seealso::

    - :doc:`../recommendations`


=========
Listeners
=========

A load balancer can have many listeners, a common setup would be to listen on
both TCP port 80 and TCP port 443 when load balancing connections to a web
application that runs on both HTTP and HTTPS protocols.

Each listener will forward requests to a pool, the members of the pool can be
the same but defined on each pool as they might use different health monitors.

The main option for the listener is what protocol to use. The protocols
available:

- ``TCP``  this is a standard TCP connection (port proxy). Its not application
  aware but will rather just forward traffic.

- ``UDP``  same as above but for UDP.

- ``HTTP``  these are protocol aware and will be able to provide som layer 7
  features relating to the HTTP protocol.

- ``HTTPS``  same as HTTP but encrypted.

- ``Terminate HTTPS``  same as HTTPS but will take a HTTPS request and forward it to
  members as HTTP (unencrypted), thus terminating the SSL/TLS connection and negating
  the need to manage certificates on the members servers. To do this, the platform will
  need access to an SSL/TLS certificate via out :doc:`secret store </secret-store/index>`.

.. note::

   HTTP and HTTPS are so called Layer 7 aware (application aware). This means they will also
   take the HTTP protocol into consideration and not just listen on the assigned port.

   While load balancing using protocol TCP and port 80 could work the same way as load balancing
   using protocol HTTP, the difference is that when using HTTP, the load balancer are able to
   use :doc:`../layer7-policies/index` and insert headers, features that work on the application
   layer.

On the listener, you are able to configure settings such as:

- Timeouts for the connection to the listener, we recommend to leave the default values for this. 

- Layer 7 (mainly HTTP) policies enabling for example redirects or rejections directly on the
  listener, negating the need to send requests to members.

- Allowed CIDR prefixes, meaning who can connect. ``0.0.0.0/0`` is the default and matches all networks.

- HTTP headers, for example ``x-forwarded-for``.

To the listener, you would normally (unless the service is internal) connect a
floating IP for accessing the service from the internet. The listener then becomes
the service endpoint for your application from the users perspective.

..  seealso::

    - :doc:`../recommendations`


============
Pool members
============

A member is a compute instance that runs a service (for example Apache).

When load balancing, a member receives proxied requests from the listener
through its membership in the pool.

Members in a pool have their health monitored and are only included in the
load balancer if their health is up, if the health is down they are not
included and thus get no traffic.

Members can have some configuration:

- Explicit monitoring IP addresses or ports if you want to do health
  monitoring on something other than the members own IP address.

- Backup flag - this means that the member is a fallback server if all
  the normal members are down. For instance when doing maintenance on an
  application you can instead show a notification that your site is down
  for maintenance and will be back later.

- You can set a weight so that a member receives a certain amount of the
  requests. A higher weight means more requests, the max value is 256. We
  recommend keeping all weights on members identical for the best request
  distribution.

All members should have an IP on a :doc:`subnet </networking/subnet/index>`.

While the subnet does not have to be the same as the one the listener
uses, its recommended for simplicity as well as performance.

..  seealso::

    - :doc:`../recommendations`


=====
Pools
=====

Pools constitute a group of members (compute instances) that provide the
actual service that is being load balanced. Each listener will normally
forward traffic to a member of the pool.

The pool will provide a common configuration for the members:

- Load balancing algorithms which decides how to distribute the requests
  between the members. Three algorithms to choose between:

  - Least connections, traffic will end up on the pool members that
    currently has the least connections.

  - Round robin, spreading traffic evenly among pool members. 

  - Source IP, traffic from the same source will end up on the same pool
    member when possible. This is a form of persistence (see below) but
    a more crude implementation with less guarantees. This is also
    called affinity.

- Session persistence will try to ensure that requests from the same
  source will end up on the same member instance. Depending on what
  listener protocol you will want to use, there are three variants
  of session persistence:

  - Source IP, this uses the source IP of the request, same as the source
    IP algorithm. Many users can share a source IP which makes this a crude
    way to do persistence.

  - HTTP cookie, the load balancer will create a standard HTTP cookie on
    the client computer and use that information to send the user to the
    same member. It does not take the applications session management
    into consideration.

  - Application cookie, this will use your applications (for example PHP)
    session management. You would reference to the cookie name, for example 
    ``PHPSESSIONID``, ``ASP.NET_SessionId`` or ``JSESSIONID``. This is the
    most exact way to do persistence.

The pool utilises health monitors to decide if a members should be active
in the pool or not at any given time.

.. tip::

   We recommend avoiding using session persistence but rather to build your
   application stateless, that is, save any session information in for example
   a database that will always be accessible no matter on which pool member
   the request ends up on.

..  seealso::

    - :doc:`../recommendations`


=============
Load Balancer
=============

Binero cloud provides a managed load balancer service that you can
use when `scaling </compute/resizing-an-instance/index.html#scaling-methods>`_
and making your application more resilient and highly available with
little effort for you.

The service handles the ingress traffic and requests to your application
with support for healthchecks. See the :doc:`general-concept/index` for more
details on how the service works.

The load balancer service builds on the well-established open source projects
`HAProxy <https://www.haproxy.org>`_ and `Keepalived <https://www.keepalived.org>`_
and all load balancers created is highly available and redundant.

Binero handles security and patching of your load balancer infrastructure at
no extra cost and it's included in the price.

You can manage your load balancers by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`,
:doc:`/getting-started/managing-your-cloud/openstack-horizon`,
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client` or the API.

The load balancer service is available in all our
:doc:`availability zones </regions-and-availability-zones>`.

.. note::

   If availability zone is not selected it will always default to the
   **europe-se-1a** availability zone.

.. note::

   The load balancer service requires a :doc:`router <../router/index>`
   and members on a :doc:`subnet <../subnet/index>` to use.

.. caution::

   The availability zone on a :doc:`index` only selects where to place your load
   balancer, incoming traffic from the internet will still go through the availability
   zone of the :doc:`/networking/router/index`.

.. toctree::
  :maxdepth: 2

  general-concept/index  
  recommendations 
  launching-a-loadbalancer/index
  ssl-termination
  layer7-policies/index


=============================
Load Balancer recommendations
=============================

On this page, we give som recommendations for achieving the best possible
load balancing setup in the platform and in general.

- Listeners

  - The Layer 7 policy can help you manage more actions in the load balancer. This
    requires the listener to be in either HTTP or HTTPS mode. A common scenario might
    be to setup a redirect from HTTP -> HTTPS for sites that use HTTPs.

  - A single load balancer can have many listeners. This is useful for when you
    want to have many services published. The load balancer can only have one service
    per TCP or UDP port so if you for some reason need to have more than one application
    that use the same port, then that would require more than a single load balancer.

  - Aside from above, if you want to use different backend pools, it's possible to create
    a Layer 7 rule that forward traffic from a certain domain to a different pool. That
    way, using domain names you could host many web applications using different backends
    on the same load balancer.

  - Terminating HTTPS in the load balancer is a good way to simplify configuration on the
    instances since you do certificate management in the load balancer only. See our guide
    :doc:`ssl-termination` for more information.

- Pools

  - If your application relies on persistence (for example sessions for users) you are able
    to configure this on the pool. We do recommend using shared storage (for example NFS)
    or to use a database for sessions. This ensures you wont disrupt your users during
    for example maintenance on instances in the pool.

  - From a performance perspective, assuming you don't use local caching on the members,
    using the round robin algorithm and not using persistence will likely be the best option.

  - If you use some kind of caching locally on the member, using persistence or ``source_ip``
    as algorithm would be preferable as you would then hit a warm cache on the same member
    on each request.

- Pool-members

  - Setup identical members. From a performance perspective but also ensure that the members
    run identical software (versions and so on). Automation is a good way to achieve this.

  - If the members are not identical, you can end up in situations where issues are present
    one member making it difficult to diagnose as only a subset of the requests end up on that
    member.

  - Its generally better to use many general purpose members than a few with high performance
    but finding the sweet spot will depend on your exact use-case, but remember that:

    - If one out of two high performance members disappear, you lose 50% of the capacity of
      the pool This might overwhelm the remaining server causing a complete outage. If one
      out of ten general purpose members disappear, the performance impact becomes negligible.

    - Software tends to work better with fewer requests (irrespectively of performance of
      the compute instance) than with many.

  - Make sure you have enough free capacity to loose a member (or even better, two) during
    peak load. A common issue with load balancing is that when the load rise and something
    happens with a single member resulting in its removal, the remaining members are
    overwhelmed from picking up its load, resulting in more removals due to high load leading
    to slower responses and the load balancer timing out. This soon spirals out of control
    where servers flaps in and out of the pool as the member recovers when removed but then
    gets overwhelmed again.

  - Our recommendation is to have a (Linux) load 15 value of less than 1 per CPU core on the
    members (ideally 0,5) during peak with one member offline (two if high availability is
    important).

  - A good idea might be to add a member to each pool with the backup feature enabled. That
    way, it will receive traffic if all other members are down and could inform of a website
    outage (planned or otherwise).

- Health checking

  - Ensure that your monitors are as closely setup (protocol wise) to the listener type as
    possible. If you are for example using load balancing for an HTTP based service, don't
    do health checking by pinging the members. Its not uncommon for a server that is
    malfunctioning to still reply to ping (or even still listen on a particular port).

  - Never use ping based health checking. You should at least check the transport protocol
    such as TCP or UDP but it's even better if you check upper layer 7 protocols such as
    the status code return for HTTP.

  - Don't use separate health checking (as in separate ports or IP addresses ). Listening to
    the same application as is being load balanced is preferable.

  - Its possible to create scripts that take many underlying considerations into account and
    having the load balancer test these scripts. This might give even better insight into
    the application.

.. note::

   We recommend using the round-robin algorithm and sizing your system with excess performance
   if availability and end-user experience is important to you.

..  seealso::

    - :doc:`general-concept/index`


=========================================================
Launching load balancer using the Cloud management portal
=========================================================

Preparations
------------

.. note::

   Before launching your first load balancer, we strongly recommend
   reading our :doc:`concepts <../general-concept/index>` guide to
   gain a better understanding of the parts.

We also recommend setting the correct :doc:`security groups </networking/security-groups/index>`
on the instances that should be members in the load balancer pool.

Traffic from the load balancer will not come from the default group
as the load balancer is not an instance - this means that explicit
rules needs to be setup on the members.

Configuration
-------------

To launch a :doc:`load balancer <../index>` from the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

.. note::

   If you want to create a load balancer that terminates SSL/TLS, you need
   to use :doc:`OpenStack Horizon <openstack-horizon>` or
   :doc:`OpenStack Terminal Client <openstack-terminal-client>`. We also
   recommend reading our :doc:`../ssl-termination` guide first.

- Press **Networking** and then **Load balancers** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- The first step is to configure the general options for the load balancer. 

  - Name your load balancer. We recommend calling it ``[NAME]_lb`` (replace
    the name with something that's relevant for you). Optionally provide a
    description.

  - Leave the IP field empty, as an IP is automatically allocated.

  - Select the :doc:`availability zone </networking/regions-and-availability-zones>`
    or leave empty to use the default europe-se-1a.

  - Select a :doc:`subnet </networking/subnet/index>` to use for hosting the load
    balancer. We recommend using the same as the subnet where you have your instances
    to load balancer but its not a must.

  - Press **Next**

- The second step sets up the *Listener*. More information :doc:`here <../general-concept/listeners>`.

  - Name your listener. We recommend calling it ``[NAME]_listener_[PORTNUMBER]`` to
    differentiate it from the other parts. Replace the name with the name you chose
    for your load balancer and the port to that of the service you want to load
    balance. Optionally provide a description.

  - Select the load balancer protocol. Note you cannot provision HTTPS with SSL/TLS termination
    from the cloud management portal, but will require OpenStack Horizon (even though its available
    in the menu).

  - *Verify the port number* (it might set automatically but depending on protocol
    you might need to set it manually). Without it, creation will fail.

  - Press **Next**

- The third step is to create the *Pool*. More information :doc:`here <../general-concept/pools>`.

  - Name your pool. We recommend calling it ``[NAME]_pool_[PORTNUMBER]`` to differentiate
    it from the other parts. Replace the name with the name you chose for your load
    balancer and the port to that of the service you want to load balance. Optionally
    provide a description.

  - Select your preferred load balancing algorithm.

  - If you want session persistence, select what type (and if applicable, enter a session name).

  - Press **Next**

- The fourth step assigns *members* (instances) to the pool. More
  information :doc:`here <../general-concept/pool-members>`.

  - Either press **Add external node** and then input the data manually or 

  - Press the small down-arrow next to **Expand to see instances** and then press the
    small **+** plus sign next to the instances you want to add.

  - Select what (IP) port on each instance the service listens on. For example for
    a web server, this would be either 80 or 443.

  - When you've added all instances to the pool, press **Next**. 

- In the fifth and final step, *health monitoring* will be setup. More
  information :doc:`here <../general-concept/health-monitors>`.

  - Name your ``health monitor``. We recommend calling it ``[NAME]_monitor_[PORTNUMBER]`` to
    differentiate it from other the parts. Replace the name with the name you chose for
    your load balancer and the port to that of the service you want to load
    balance. Optionally provide a description.

  - Select the type. This value would depend on what kind of listener you created
    (which protocol). We recommend choosing a type that is as granular (go with HTTP
    than TCP port 80 if you want to check a web server) as possible.

  - Choose the options relating to your ``health monitor``, the defaults will likely suffice
    but this is much dependant on the application.

  - Press **Create load balancer**. 

.. note::

   The load balancer will take some time to start as its a complex process to create it.

Verification
------------

To verify that the health checking has added the members to the pool, follow this procedure:

- Press **Networking** and then **Load balancers** in the sidebar menu.

- Verify that the **Provisioning status** says **Active** under its name and then click it.

- Press the **Pools** tab and then press the name of your pool.

- Press the **Nodes** tab.

- In the list, you should now see the members you've added. The column **Operational status**
  should show you if they are online or not.

.. tip::

   If the members are not online, make sure you have the proper :doc:`/networking/security-groups/index`
   configured on the them. Traffic from the load balancer will not come from the default group as the load
   balancer is not part of your instances - this means that explicit rules needs to be setup on the
   members. If you still cant get the members online, verify (by using for example ``tcpdump`` or by
   reading access logs, that the traffic hits the member servers from the load balancers IP.

..  seealso::

    - :doc:`../general-concept/index`
    - :doc:`../recommendations`
    - :doc:`../index`


=========================
Launching a load balancer
=========================

.. note::

   Before launching your first load balancer, we strongly recommend reading
   our :doc:`concepts <../general-concept/index>` guide to get an understanding
   of the parts.

We also suggest our :doc:`recommendations for load balancing <../recommendations>` article
for some general tips that might improve your load balancing implementation.

To create a load balancer in Binero cloud, you have three main options as outlined in
the links below. Each option have its advantages and disadvantages:

- :doc:`The cloud management portal <cloud-management-portal>` is recommend and will get
  a user with limited prior knowledge from A to B quickly. The tradeoff is that
  advanced features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in
  OpenStack. Some advanced features might only have a GUI implementation here.

- :doc:`The OpenStack terminal client <openstack-terminal-client>` is a command line
  giving terminal oriented users a quick way to access the cloud. The learning curve
  is steeper than the GUI implementation but the workflow will be efficient.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


===============================================
Launching load balancer using OpenStack Horizon
===============================================

.. note::

   Before launching your first load balancer, we strongly recommend reading
   our :doc:`concepts <../general-concept/index>` guide to gain a better
   understanding of the parts.

We also recommend setting the correct :doc:`security groups </networking/security-groups/index>`
on the instances that should be members in the load balancing. Traffic from the load balancer
will not come from the default group as the load balancer is not an instance - this means that
explicit rules needs to be setup on the members.

Configuration
-------------

To launch a :doc:`load balancer <../index>` from the
:doc:`OpenStack Horizon portal </getting-started/managing-your-cloud/openstack-horizon>`

.. note::

   If you want to create a load balancer that terminates SSL/TLS, you first need to
   :doc:`create the requisite certificate </secret-store/create-cert-for-loadbalancing>` in
   our secret store. We recommend reading our :doc:`../ssl-termination` guide before proceeding.

- Under **Project**, click **Network** and then **Load balancers** in the sidebar menu.

- Click **Create load balancer** in the right upper corner.

- The first step is to configure the general options for the load balancer. 

  - Name your load balancer. We recommend calling it ``[NAME]_lb`` (replace the name with something
    that's relevant for you). Providing a description is optional.

  - Leave the IP field empty, an IP address is automatically allocated.

  - Select the :doc:`availability zone </networking/regions-and-availability-zones>` or leave empty
    to use the default europe-se-1a.

  - Leave the Flavor field empty as selecting a flavor is not supported.

  - Select a :doc:`subnet </networking/subnet/index>` to use for hosting the load
    balancer. We recommend using the same as the subnet where you have your instances to load
    balancer but its not a must.

  - Press **Next**.

- The second step sets up the *listener*. More information :doc:`here <../general-concept/listeners>`.

  - Name your listener. We recommend calling it ``[NAME]_listener_[PORTNUMBER]`` to differentiate
    it from the other parts. Replace the name with the name you chose for your load balancer and
    the port to that of the service you want to load balance. Optionally provide a description.

  - Select the load balancer protocol.

  - *Verify the port number* (it might set automatically but depending on protocol you might need to
    set it manually). Without it, creation will fail.

  - Press **Next**.

- The third step is to create the *pool*. More information :doc:`here <../general-concept/pools>`.

  - Name your pool. We recommend calling it ``[NAME]_pool_[PORTNUMBER]`` to differentiate it from
    the other parts. Replace the name with the name you chose for your load balancer and the port
    to that of the service you want to load balance. Optionally provide a description.

  - Select your preferred load balancing algorithm.

  - If you want session persistence, select what type (and if applicable, enter a session name).

  - Press **Next**.

- The fourth step assigns *members* (instances) to the pool. More
  information :doc:`here <../general-concept/pool-members>`.

  - Either press **Add external member** and then input the data manually or 

  - press the **Add** button next to the instances you want to add in the list.

  - Select what (IP) port on each instance the service listens on. For instance for a web
    service, this would be either 80 or 443.

  - When you have added all instances to the pool, press **Next**. 

- In the fifth step, *health monitoring* will be setup. More information
  :doc:`here <../general-concept/health-monitors>`.

  - Name your ``health monitor``. We recommend calling it ``[NAME]_monitor_[PORTNUMBER]`` to differentiate
    it from other the parts. Replace the name with the name you chose for your load balancer and the
    port to that of the service you want to load balance. Optionally provide a description.

  - Select the type. This value would depend on what kind of listener you created (which protocol). We
    recommend choosing a type that is as granular (go with HTTP than TCP port 80 if you want to check a
    web server) as possible.

  - Choose the options relating to your ``health monitor``, the defaults will likely suffice but this
    is much dependant on the application.

  - If you've opted for ``TERMINATED SSL`` when setting up the listener details, you will need to
    proceed to next step to add your certificates and should then be able to click **Next**. If
    not, the **Next** option is unavailable and you should instead click **Create Load Balancer**.

- The sixth step is only relevant when creating an SSL/TLS terminating load balancer. In this step, you
  would choose the certificate that the load balancer should use for SSL/TLS connections. To do this, you
  first need to have a :doc:`certificate </secret-store/create-cert-for-loadbalancing>` added to the secret
  store.

.. note::

   The load balancer will take some time to start as its a complex process to create it.

Verification
------------

To verify that the health checking has added the members to the pool, follow this procedure:

- Under **Project**, click **Network** and then **Load balancers** in the sidebar menu.

- Verify that the **Operating status** says **Online** and **Provisioning status**
  says **Active** on the load balancer and then press its name.

- Press the **Pools** tab and then press the name of your pool.

- Press the **Members** tab.

- In the list, you should now see the member(s) you've added. The column **Operating status** should
  show you if they are online or not.

.. tip::

   If the members are not online, make sure you have the proper :doc:`/networking/security-groups/index`
   configured on the them. If you still cant get the members online, verify (by using for example ``tcpdump``
   or by reading access logs, that the traffic hits the member servers from the load balancers IP.

..  seealso::

    - :doc:`../general-concept/index`
    - :doc:`../recommendations`
    - :doc:`../index`


===========================================================
Launching load balancer using the OpenStack terminal client
===========================================================

.. note::

   Before launching your first load balancer, we strongly recommend reading
   our :doc:`concepts <../general-concept/index>` guide to gain a better
   understanding of the parts.

We also recommend setting the correct :doc:`security groups </networking/security-groups/index>`
on the instances that should be members in the load balancing.

Traffic from the load balancer will not come from the default group as the load balancer
is not an instance - this means that explicit rules needs to be setup on the members.

Configuration
-------------

To launch a `load balancer <../index>`_ from the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`, follow
the below steps.

We recommend checking OpenStack Horizon for what options are available or using
the ``-h`` option of the terminal client for more information.

More information is also available `here <https://docs.openstack.org/octavia/latest/user/guides/basic-cookbook.html>`__.

This documentation aims to show how to get going, not display exhaustive information on each available
option. The example below will load balance HTTP (protocol aware) and thus port 80.

.. note::

   If you want to create a load balancer that terminates SSL/TLS, you first need to :doc:`create the requisite
   certificate </secret-store/create-cert-for-loadbalancing>` in our secret store. We recommend reading
   our :doc:`../ssl-termination` guide before proceeding. 

.. note::

   The brackets in below example commands are for demonstrating values that you need to change.

   The suffixes, for example ``_listener_80``, suggested within brackets are for clarity, with
   the ``NAME`` part to symbolise a common name you pick to identify the load balancer.

   You can use any name for each part of the load balancer. That said, each command will reference
   an earlier example names.

- Run this command: ``openstack subnet list``, save the name of the subnet that your members are on. We
  suggest using the same subnet for the load balancer but if you would rather use another subnet (or have
  members in many subnets), then also save the other subnet names.

- Run this command to create the load balancer: ``openstack loadbalancer create --name [NAME_lb] --vip-subnet-id [SUBNET_NAME]`` replacing
  the subnet name with that from previous step, use ``--availability-zone`` to select a available, if not given
  europe-se-1a will be used by default.

- Run this command until it says that the ``operating_status`` is ``ONLINE``: ``openstack loadbalancer show [NAME_OF_LB]``

- Create a :doc:`listener <../general-concept/listeners>`

  - If you want to create a HTTP listener you can use ``openstack loadbalancer listener create --name [NAME_listener_80] --protocol HTTP --protocol-port 80 [NAME_lb]``.

  - If you want to create a HTTP listener with SSL/TLS termination you can use: ``openstack loadbalancer listener create --protocol-port 443 --protocol TERMINATED_HTTPS --name [NAME_listener_80] --default-tls-container=$(openstack secret list | awk '/ [NAME_OF_SECRET] / {print $2}') [NAME_lb]``

- Run this command to setup the :doc:`pool <../general-concept/pools>`: ``openstack loadbalancer pool create --name [NAME_pool_80] --lb-algorithm ROUND_ROBIN --listener [NAME_listener_80] --protocol HTTP``.

- Run this command to setup health checkers: ``openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type HTTP --url-path / [NAME_pool_80]``.

- Run this command: ``openstack server list``, save the IP addresses of the members you want to add.

- Repeat this command to add the members: ``openstack loadbalancer member create --subnet-id [SUBNET_NAME] --address [IP_OF_MEMBER] --protocol-port 80 [NAME_pool_80]``

.. note::

   The load balancer will take some time to start as its a complex process to create it, this particularly
   applies after the second command above.

If you want to assign a :doc:`floating IP <../../floating-ips>` to your load balancer.

- Run this command: ``openstack loadbalancer list``, save the name of the load balancer you want to verify.

- Run this command: ``openstack loadbalancer show [NAME]``. Replace [NAME]with the name from previous step. Save
  the value of the ``vip_port_id`` of the load balancer.

- Run this command: ``openstack floating ip list``, save an unassigned floating IP.

- If you don't have an unassigned floating IP, follow the steps in the :doc:`floating IP addresses <../../floating-ips>`
  article to assign one to the project.

- Run this command: ``openstack floating ip set --port [VIP_PORT_ID] [FLOATING_IP]``, replace the items in angle
  brackets with data from previous steps.

Verification
------------

To verify that the health checking has added the members to the pool, follow this procedure:

- Run this command: ``openstack loadbalancer pool list``, save the name of the pool containing the members
  you want to check.

- Run this command: ``openstack loadbalancer member list [NAME_OF_POOL]`` (replace the name with the name
  of the pool from previous step).

- Members have **Operating status** of ``ONLINE`` when they are online in the pool.

.. tip::

   If the members are not online, make sure you have the proper :doc:`/networking/security-groups/index` configured
   on the them. If you still cant get the members online, verify by using for example ``tcpdump`` or by reading access
   logs, that the traffic hits the member servers from the load balancers IP.

..  seealso::

    - :doc:`../general-concept/index`
    - :doc:`../recommendations`
    - :doc:`../index`


==========================================================
Creating a Layer7 policy using the Cloud management portal
==========================================================

We recommend first reading our :doc:`guide <index>` on layer 7 policies to get
a better understanding of how they work.

To create a layer 7 policy by using the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

- Press **Networking** and then **Load balancers** in the sidebar menu.

- Press the load balancer on which you want to add the rule.

- Press the **Listeners** tab and then the name of the listener to which you
  want to add the policy.

- Press the **L7 policies** tab.

- Press the **+** (plus) sign.

- Name your policy and optionally provide a description.

- Select your action and if applicable, provide a value (e.g. what domain
  to redirect to).

- Select a priority (1 is the lowest).

- Check **admin state up** assuming you want the rule to go live at once.

- Press **Create**

Once you've created a policy, you will also want to create a rule which triggers
it.

- Press the newly created policies name. 

- Press the **L7 Rules** tab.

- Press the **+** (plus) sign.

- Choose the type and if applicable, also input the key value. 

- Choose compare type and input value for comparison.

- Check **Admin state up** assuming you want the rule to go live at once.

- Press **Create**

After some time to provision (when the **Operating status** says **ONLINE**), the
rule is live.

..  seealso::

    - :doc:`../index`


===============
Layer7 Policies
===============

General concept
---------------

When using either HTTP or HTTPS as :doc:`listener protocol <../general-concept/listeners>`, you
can use features of the HTTP protocol directly in the load balancer.

You can configure Layer 7 policies, below is a couple of examples on what you can
configure a policy to perform.

- ``Reject``  will result in HTTP/1.1 403 Forbidden reply.

- ``Redirect to pool``  will send traffic to another listeners pool. This is a good thing if you
  already have a pool setup (from another listener) of the same servers and just want to add
  another listener (for example when adding HTTP protocol to an already configured HTTPS
  listener). Another really useful use case could be if you want to run a separate pool for
  your static content. You can then opt to send for example traffic to ``www.example.com/js``
  or ``/images`` to separate backends that's optimised (maybe via caching) for static content.

- ``Redirect to URL``  this will redirect (using a header: location) the request to another
  URL. Useful when for example requiring HTTPS.

The policies will only triggered once you setup a corresponding Layer 7 Rule on them. The rule
decides when the policy should trigger. You can select different *types* of triggers:

- ``HOST_NAME`` (for example ``example.com``)

- ``PATH`` (for example ``/images``)

- ``FILE_TYPE`` (for example ``.jpeg``)

- ``HEADER`` (for example ``x-forwarded-for``)

- ``COOKIE`` (for example ``phpsessid``)

Each type has different configurable parameters. You can then select a comparison method such as:

- ``REGEX`` (regular expressions)

- ``EQUALS_TO`` (value is exactly compared)

- ``STARTS_WITH`` or ``ENDS_WITH``

- ``CONTAINS`` (value contains)

Finally, you specify a value that should match in the request, for example when comparing hostname
you could specify ``example.com``.

You can also negate the evaluation, for example when looking for a non logged in
user, you might redirect to login when **not** finding a cookie.

If the Layer7 rule matches, the Layer7 policy will trigger.

.. tip::

   We recommend that you read the `Layer 7 Cookbook <https://docs.openstack.org/octavia/latest/user/guides/l7-cookbook.html>`__
   in the OpenStack documentation.

   See the OpenStack `CLI documentation <https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#l7policy>`__
   for more information about the parameters available.

Creating policies
-----------------

To create a Layer 7 policy in Binero cloud, you have three main options as outlined in the links
below. Each option have its advantages and disadvantages:

- :doc:`The cloud management portal <cloud-management-portal>` is what we recommend and will get
  a user with limited prior knowledge from A to B quickly. The tradeoff is that advanced
  features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in OpenStack. Some
  advanced features might only have a GUI implementation here.

- :doc:`The OpenStack terminal client <openstack-terminal-client>` is a command line implementation
  giving terminal oriented users a quick way to access the cloud. The learning curve is steeper than
  the GUI implementation but the workflow will be efficient.

.. toctree::
  :caption: Available services
  :maxdepth: 2

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


================================================
Creating a Layer7 policy using OpenStack Horizon
================================================

We recommend first reading our :doc:`guide <index>` on layer 7 policies to get a
better understanding of how they work.

To create a layer 7 policy by using the
:doc:`OpenStack Horizon portal </getting-started/managing-your-cloud/openstack-horizon>`

- Under **Project**, click **Network** and then **Load balancers** in the sidebar menu.

- Press the name of the load balancer on which you want to add the rule.

- Press the **Listeners** tab and then the name of the listener to which you want
  to add the policy.

- Press the **L7 policies** tab.

- Press the **Create L7 Policy** button in the top right. 

- Name your policy and optionally provide a description.

- Select your action and if applicable, provide a value (e.g. what domain to redirect to).

- Select a position (1 is the lowest).

- Press **Create L7 Policy**

Once you've created a policy, you will also want to create a rule which triggers
it.

- Press the newly created policies name. 

- Press the **L7 Rules** tab.

- Press the **+ Create L7 Rule** button.

- Choose the type and if applicable, also input the key value. 

- Choose compare type and input value for comparison.

- Press **Create L7 rule**

After some time to provision (when the **Operating Status** says
**Online**), the rule is live.

..  seealso::

    - :doc:`../index`


========================================================
Creating a Layer7 policy using OpenStack Terminal Client
========================================================

We recommend first reading our :doc:`guide <index>` on layer 7 policies
to get a better understanding of how they work.

To create a layer 7 policy by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack loadbalancer listener list``. Save the name of
  the listener to which you want to add the rule.

- Run this command: ``openstack loadbalancer l7policy create --action REDIRECT_TO_URL --redirect-url [http(s)://DOMAIN] --name [POLICY_NAME] [LISTENER_NAME]``, replacing
  the values in angle brackets. This sets up the policy.

- Run this command: ``openstack loadbalancer l7rule create --compare-type STARTS_WITH --type PATH --value / [POLICY_NAME]``. This
  sets up the rule.

- Run this command: ``openstack loadbalancer l7rule show [POLICY_NAME] [RULE_ID]``. Replace the items in
  angle brackets (the RULE_ID is from the previous commands output). Once the ``operating_status`` says
  **ONLINE**, the rule should be live.

.. note::

   Above is an example of a rule which would perform a redirect. You can read more information about the
   available rules in detail `here <https://docs.openstack.org/python-octaviaclient/latest/cli/index.html#l7policy>`__.


=======================================
SSL/TLS termination using Load Balancer
=======================================

General concept
---------------

When using SSL/TLS (which is standard today) to provide encryption for visitors to a web
application, managing the SSL/TLS certificates on web servers might be cumbersome.

When using a load balancer, an alternative is to publish the certificates on the load
balancer instead, as a single point of ingress into the system. The traffic, that is
the request the load balancer sends to a server, can be unencrypted as it never leaves
the platforms internal network. Encryption is then managed on the load balancer through
the platform and all instances managing the backend traffic can just run HTTP on port 80.

.. note::

   It's recommended to always encrypt your network traffic even if it's internal to
   your application, but to get you started using unencrypted traffic can help you
   get started faster.

Configuration
-------------

To setup HTTPS in the load balancer, you first need to have a certificate stored in our
security storage. More information is available in our
:doc:`/secret-store/create-cert-for-loadbalancing` article.

Once you have the certificate stored, you are able to follow either
:doc:`launching-a-loadbalancer/openstack-horizon` or
:doc:`launching-a-loadbalancer/openstack-terminal-client` to install
the load balancer.

Remember to select **TERMINATED_HTTPS** as protocol of the
:doc:`listener <general-concept/listeners>` as doing so will (in OpenStack Horizon)
enable the ``SSL Certificates`` tab to appear at the end, under which you are able
to select the certificate you've added.

.. note::

   This feature is only available in :doc:`/getting-started/managing-your-cloud/openstack-horizon`
   or :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

Headers
-------

When you do termination of SSL/TLS in the load balancer, its important to remember that
the web application that receives the request will consider it to be un-encrypted.

It will also consider the request made out to whatever IP address that the instance that
receives the request (from the load balancer) is listening on since the request is *proxied*
(as opposed to *routed*).

To give your web application more information, there are a few headers that your load balancer
can insert:

- ``x-forwarded-port`` will be 443 and can be used to check that the traffic is indeed
  arriving on the encrypted port (if you also allow pure http traffic to the same instances).

- ``x-forwarded-proto`` works the same was as above but will give the protocol.

- ``x-forwarded-for`` will tell you to what IP, the visitor made the request. If you have
  several load balancers fronting the same instances, this will let you know through which
  load balancer the request was made.

The headers are available from the **Listener details** tab when setting up the load balancer
(or by editing the listener).

We recommend adding the headers when setting up a load balancer as the performance impact
is negligible and the information is useful for your application.

Requiring HTTPS
---------------

Web applications that have HTTPS should force the user to use the secure protocol.

If a user is not required to use HTTPS, its possible that they inadvertently send their
credentials over the (unencrypted) HTTP protocol.

Today most browsers will warn but this will be a bad user experience (as the warning will not
mean much to most users - and even if it does, suggest that security of the site is lacking).

To solve the problem you could add a second listener that listens on TCP port 80 (HTTP). On the
listener, its then possible to create :doc:`layer7-policies/index` that could (among other
things) redirect.

Two main ways to add the redirection to HTTPS. The first method would add a new prefix to a
path and would thus take paths into consideration.

This is the recommended approach as it would always send the visitor to the correct page (provided
the path exists). The downside is, its only configurable using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`.

The second method would ensure that all requests made via HTTP redirects to your home page. If your
web application is using HTTPS for linking, the user would afterwards remain in the HTTPS realm.

Create a listener
^^^^^^^^^^^^^^^^^

The first step is to setup a new listener.

.. note::

   The cloud management portal cannot create just a listener but will create an entire pool as
   well.

   We do not recommend using it for this task but if you want to use it, navigate to your load
   balancer in the menu, click the **Listeners** tab and then press the **+** plus sign, after
   which you should be able to follow our :doc:`guide <launching-a-loadbalancer/cloud-management-portal>`.

Documentation for creating a listener by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client` is available
:doc:`here <launching-a-loadbalancer/openstack-terminal-client>`.

To add just a HTTP listener (as opposed to an entire load balancer with pools and health
checking) using OpenStack Horizon.

- Under **Project**, click **Network** and then **Load balancers** in the sidebar menu.

- Press the name of the load balancer to which you want to add the rule.

- Press the listener tab.

- Press **+ Create listener**.

- Name your listener. We recommend calling it ``NAME_listener_http`` to differentiate it
  from other listeners. Optionally provide a description.

- Select HTTP as the load balancer protocol.

- Ensure that you set the port to 80 (if its automatically set to 81, you already have
  a listener that listens on port 80 and should instead use that one).

- Under the **Pool details** tab, select **No** under **Create pool** section.

- Press **Create listener**.

.. important::

   When setting up the policy in the next step, remember that you would need a policy
   for both www.example.com and example.com (assuming you use both).

Create a path aware redirect policy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The recommended way to require HTTPS is to use a redirect prefix. To setup this, use
the OpenStack Terminal Client according to below. If you would rather use OpenStack
Horizon or the cloud management portal, see below for a less accurate way to redirect.

- Run this command: ``openstack loadbalancer listener list``. Save the name of the listener.

- Run this command: ``openstack loadbalancer l7policy create --action REDIRECT_PREFIX --redirect-prefix https://[YOUR_DOMAIN] --name redirect_to_https [LISTENER NAME]`` replacing
  the domain and the listener name from the previous step.

- Run this command: ``openstack loadbalancer l7rule create --compare-type STARTS_WITH --type PATH --value / redirect_to_https``

.. note::

   Don't add a trailing slash on your domain as that will add an extra slash
   in the path.

Create a redirect to the first page
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

If you prefer to stay in the GUI, the following method will allow you to setup similar functionality
using OpenStack Horizon, using this method, for example http://www.example.com/subfolder/index.html
would redirect to https://example.com.

- Under **Project**, click **Network** and then **Load balancers** in the sidebar menu.

- Press the name of the load balancer to which you want to add the rule.

- Press the listener tab.

- Press the name of your HTTP listener.

- Press **L7 Policies** tab and then **+ Create L7 policy**

- Name your policy to for example ``redirect_https`` and optionally give it a description.

- Under **Action** select ``REDIRECT_TO_URL``.

- Under **Redirection URL** type ``https://yourdomain.com`` (replace yourdomain.com with your domain).

- Under **Position**, type ``1``.

- Press the name of your new policy and then the tab **L7 Rules** and then **+ Create L7 Rule**.

- Under **Type**, select ``HOST_NAME``.

- Under **Compare type**, select ``CONTAINS``.

- Enter your domain (same as the one you want to do redirects for) in the value field.

- Press **Create L7 Rule**

..  seealso::

    - :doc:`general-concept/index`


===
MTU
===

MTU or ``Maximum Transmission Unit``, is a measure of how large individual packets (or frames) can be.

The concept of *Jumbo frames* essentially means supporting an MTU on an interface that is larger
than 1500 bytes which is the standard size (Ethernet) of on a network interface in operating systems. 

Fragmentation
-------------

Since the standard MTU size is 1500 bytes, using a larger MTU could have some impact for packets
that have a destination that is outside the scope of the local network (where the MTU is possible
to control).

For these scenarios, **fragmentation** is the normal solution.

Fragmentation means that an intermediate router might split a packet into many packets, should
it need to egress onto a network with a smaller MTU as next hop.

The receiving server will later on have to reassemble the packet. This process happens in the background
and is not something that needs to be actively managed by an administrator, some applications does not work
well with fragmentation so there is an option to disallow it by setting the ``DF`` bit on a frame. 

DF stands for *Don't Fragment* and essentially tells every router involved to avoid fragmentation. The alternative
to fragmentation, when using a larger MTU than 1500, is dropping the packet and send back a  ICMP message with
code *Fragmentation needed*.

Because of this, its important to understand the concept of MTU, especially when using an MTU different
from 1500 bytes.

Here is an example of an ICMP reply to a packet that was to big, as captured using ``tcpdump``: 

::

    17:54:49.979998 IP 1.2.3.4 > 5.6.7.8: ICMP 1.2.3.4 unreachable - need to frag (mtu 1450), length 556

Here is an example of a packet that has the DF bit set: 

::

    6:30:17.621056 IP (tos 0x0, ttl 61, id 39062, offset 0, flags [DF], proto TCP (6), length 4148) 4.5.6.7.443 > 1.2.3.4.34862: Flags [P.], cksum 0x0bdb (incorrect -> 0x3f5f), seq 1:4097, ack 321, win 227, options [nop,nop,TS val 955793242 ecr 3076045005], length 4096

Notice the ``DF`` flag - this indicates that we should not fragment the packet . Also notice the length
indicating that the packet is 4096 bits long and as such, will not pass a standard 1500 MTU interface as
fragmentation is not allowed.

MTU discovery
-------------

The standard for discovering the smallest MTU on a link called `Path MTU Discovery <https://en.wikipedia.org/wiki/Path_MTU_Discovery>`_
or PMTUD.

This enables servers to discover the smallest MTU by setting the DF flag and sending packets until it's forwarded
correctly. It relies on ICMP which not all service providers and network operators honour so its not 100% certain
that it will work.

MTU in Binero cloud
-------------------

Since Binero cloud uses software-defined networking, the MTU size given when using :doc:`subnet/dhcp` on your
:doc:`subnet <subnet/index>` to assign addresses is 1450 bytes.

This is because the VXLAN (network virtualization used) encapsulation needs 50 bytes of the frames
available 1500 bytes.

The underlying interface on the physical servers managing compute and networking in the platform uses the standard
of 1500 bytes and so the instances need to have a smaller MTU to also provide enough room for the host to encapsulate
the frame before sending it onto the network.

.. note::

   It's possible to set any MTU on your interface but using an MTU larger than 1450 will cause
   dropping of your packets. See below for more information.

MTU caveats
-----------

When using a different MTU than standard (as is the case in Binero cloud), it becomes important to understand the
possible effects of that.

If you only have your compute instances and they use 1450 MTU, generally everything will work well. If you for
example use **bridge interfaces** on your instances to connect to local containers (or some other use-case), which in
turn use the default 1500 MTU then this might have adverse effects. 

.. important::

   Use only 1450 MTU on all interfaces that are setup on your instances. 

Larger MTU
^^^^^^^^^^

Larger than 1500 MTU is normally used in scenarios where you need to send data quickly. It causes less data
on the wire when using a large MTU because you need to send less packets to transfer the same amount of data.

For example ISCSI or NFS (storage protocols that transfers large amounts of data) both grain performance by
using larger MTU. Larger MTU than 1450 is not possible to use in the cloud platform.

Smaller MTU
^^^^^^^^^^^

The only real reason to use a smaller MTU is if you use a network protocol that needs room for
more data in its header added to packets.

Below are some examples:

- GRE (IP Protocol 47) (RFC 2784) adds 24 bytes (20 byte IPv4 header, 4 byte GRE header)

- 6in4 encapsulation (IP Protocol 41, RFC 4213) adds 20 bytes

- 4in6 encapsulation (e.g. DS-Lite RFC 6333) adds 40 bytes

- IPsec encryption performed by the DMVPN adds 73 bytes for ESP-AES-256 and ESP-SHA-HMAC

- MPLS adds 4 bytes for each label in the stack

- IEEE 802.1Q tag adds 4 bytes (Q-in-Q would add 8 bytes)

- VXLAN adds 50 bytes

- OTV adds 42 bytes

- LISP adds 36 bytes for IPv4 and 56 bytes for IPv6 encapsulation

- NVGRE adds 42 bytes

- STT adds 54 bytes

Taking into account the extra data from the encapsulation, gives you less space to send data and the
max size you can send decreases for the payload data to fit the extra header.

General recommendations
-----------------------

The mechanisms in TCP to overcome MTU differences is using PMTUD which you cannot trust due to
normalization of blocking ICMP traffic for IPv4.

That said, generally you would want to ensure that you **do not use more than 1450 MTU in total** on the platform.

If you need to use encapsulation, take this into account. A good way to test is using ping with the DF flag set and
by manually specifying the size of the ping packet.

You can do this on Linux:

::

    ping -M do 1.2.3.4 -s 1500

And on Windows

::

    ping 1.2.3.4 -l 1500 f

Above example would ping with 1500 bytes with DF set and would return a fail if that did not work.

Remember that ping uses both ICMP and IP which both add to the size of the packets (20 and 8 bytes) so
even sending 1450 bytes with DF set to an IP in Binero cloud will not work, ``1450 - 28 = 1422`` bytes will.

In conclusion, keeping MTU consistent to 1450 across all interfaces will ensure functionality.


=========================================
Managing networks using OpenStack Horizon
=========================================

Create a network
----------------

To create a :doc:`network <index>` from :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Networks** in the sidebar menu.

- Click **Create Network** in the right upper corner.

- Name your network.

- Check the **Enable admin state** box.

- If you want to create a :doc:`subnet <../subnet/index>`, you could do that from this view as
  well, if so leave the **Create subnet** checkbox checked, if you prefer to follow our documentation and
  do this separately, you can clear the checkbox.

- Under **Availability zone (hints)**, select the availability zone that router or instance that you want
  to connect the network to.

- Under MTU, enter the value 1450. See :doc:`MTU </networking/mtu>` for more information.

- Press **Create**. If you've opted to create a subnet at once, this option will instead be **Next**.

.. note::

   To connect an instance or router to the network, you will also need to create
   a :doc:`subnet <../subnet/index>` which you can then attach to an interface on
   a router or an instance.

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


=================================================
Managing networks using OpenStack terminal client
=================================================

Create a network
----------------

To create a :doc:`network <index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run ``openstack availability zone list --network --long``, this will give you the current availability
  zones for routers and networking. Save this for following step.

- Run ``openstack network create --availability-zone-hint=[AVAILABILITY ZONE] [NETWORK-NAME]``. Replace the
  items within brackets with the values from previous section as well as the name of the router. Remember to
  use the network that corresponds to the availability zone you chose. If you are not sure what availability
  zone to use, we recommend using *europe-se-1a*.

.. note::

   To connect an instance or router to the network, you will also need to create
   a :doc:`subnet </networking/subnet/index>` which you can then attach to an
   interface on a router or an instance.

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


=====
Ports
=====

A port is a virtual representation of a network port.

A network port is a connection between a :doc:`network <network/index>`
and a device, a device can be a instance, router or load balancer.

You secure ports by assigning one or more :doc:`security groups (firewall) <security-groups/index>`
to the port to allow traffic. You can disable port security on a port to
disable security groups entirely.

.. important::

   It's strongly advised to **not** disable port security on a port as you are
   disabling all security groups and allowed address pairs (IP + MAC) filtering.

Ports are generally created on-demand when creating an instance and
selecting a network.

If you for some reason need to create a port manually, you can perform that
as shown below. If you do, you also need to assign the port (explicitly)
to the instance you want to connect to a network.

Remember that a manually created port might not be automatically removed when
you delete an instance.

.. note::

   Certain use-cases might exist where you would want to manually create a port.

   One use-case is if you want to assign many ports on the same network when creating
   an instance. Its only possible to add a network once, but you can assign many ports
   that each connect to the same network. 

We recommend sticking to a single port on an instance as its generally easier to understand
the network flow and plan security that way. You should instead use :doc:`routing <router/routing-between-networks>`.

If using this network design then you should rarely need to add ports manually as an
instance will have a port created when launched.

Below we will show how to create ports.

.. note::

   Should you opt not to use :doc:`subnet/dhcp` to assign IP configuration to a new
   port, remember that the :doc:`mtu` is also set via DHCP and you need to set it
   manually.

Create a port using the cloud management portal
-----------------------------------------------

To create a port by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Ports** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Name your port.

- Select the :doc:`network <network/index>` you want to connect to under **Network**.

- Select **Subnet** under **IP address or subnet**. If you would rather manage your IP-configuration
  on your instance manually (not recommended), you would instead leave **unspecified**.

- Select the :doc:`subnet <subnet/index>` you want to use.

- Press **Create**

Create a port using OpenStack Horizon
-------------------------------------

To create a port by using the
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Networks** in the sidebar menu.

- Click the name of the network to which you want to add the port. 

- Press the **Ports** tab on the top.

- Press **Create port**

- Name your port. 

- Choose **Subnet** under **Specify IP address or subnet**. If you would rather manage your
  IP-configuration on your instance manually (not recommended), you would instead leave **unspecified**.

- Select the :doc:`subnet <subnet/index>` you want to use.

- Press **Create**

Create a port using OpenStack terminal client
---------------------------------------------

- Run this command: ``openstack network list``. Save the name of the network you want
  to connect to.

- Run this command: ``openstack subnet list``. Save the name of the subnet you want to
  use (assuming you want to let the platform configure an IP on the port.

- Run this command: ``openstack port create --network [NETWORK NAME] --fixed-ip subnet=[SUBNET NAME] [PORT NAME]``, replacing
  the items in angle brackets with the information from the previous steps and the name of the port. If you want to know more
  options, use ``-h`` at the end of the command.

.. note::

   Your port is now available for use but remember that it also need to be
   :doc:`assigned to an instance </compute/assign-ip>` before you can use it.


=======================
Reaching your instances
=======================

To manage the instances in your cloud you have four main options:

- :doc:`Using a floating IP per instance <floating-ips>`

- :doc:`Using a client VPN <client-vpn/index>`

- :doc:`Using a site-to-site VPN <site-to-site-vpn/index>`

- Using a bounce server

We outline the different methods below. 

Floating IP
-----------

A floating IP is a public, globally routed, IP address that maps 1:1 with an instance.

It will use DNAT (destination NAT) to allow traffic through the router from the internet.

It's normally used for enabling services to be internet facing (reachable from the internet)
but also works for managing instances (by enabling access to for example SSH to the floating
IP).

The downside with assigning a floating IP for access to the instance is that it doesn't scale
if you have many instances and the cost for floating IP addresses will increase when adding
new instances, another downside is that, while you are able to secure the instance by using
security groups it's not recommended to expose your instance to the internet.

.. warning::

   Using a floating IP with wide open access to the instance is strongly discouraged and will
   likely lead to a compromised system used for malicious activity.

Client VPN
----------

A client VPN is a VPN that connects from a single client (normally a laptop or a workstation
computer) and that sets up a tunnel for a single user to a remote network.

Using a client VPN is an effective way to get access to the entire inside network by using
routing through a tunnel.

When able to reach the inside, it becomes possible to connect directly to the instances, without
the need for a floating IP for anything but the VPN server.

Since you install a client VPN on the client computer, you can also use it from anywhere where
you have your client computer available, given that you have a internet connection. This works
well for connecting from remote locations.

The downside is that you need to have an application installed on your computer with a secure
configuration which you will need to provision to you in a secure manner.

To reach the infrastructure, you first need to logon to the VPN separately. You also need to
maintain the VPN server and the users on it.

Binero cloud provides a :doc:`client VPN service <client-vpn/index>` that would enable you to
get going with a secure tunnel to your infrastructure quickly with minimal effort. 

Site-to-site-VPN
----------------

Using a site-to-site tunnel, you would setup a tunnel between your cloud and your router.

This could be the office router or a (more advanced) router based in your home. Once the tunnel is up, users
that sit connected to the (home or office) router would have their traffic automatically (and transparently)
routed into the tunnel.

This would negate the need to login, the cloud network(s) and the office networks would be seamlessly
connected using the tunnel and the user experience is equally seamless - the cloud becomes local to the
user.

The downside is that a site-to-site VPN is difficult to setup (while only having to be setup once) and only
works from behind the router that connects to the cloud, so if you set it up in your office and have users
working from home, they still need an alternate solution. 

Bounce server
-------------

The concept of a bounce server is that you create an instance with a floating IP to which you have your users
connect using for example SSH (or RDP if running Windows).

This way, only the bounce server needs a floating IP. Once you've connected your users to the server (preferably
securely, for example by using SSH keys and with their own account that has not got elevated permissions), they are
then able to connect to other instances from the bounce server as the bounce server is also connected to the internal
network(s) and is able to reach them when originating traffic from itself (there is no routing happening, users would
work through the bounce server).

An upside to *not having network access directly*, but rather sending traffic through the bounce server (and using its
authentication), is that malicious software that originate from your workstation will not be able to connect to the internal
systems directly (as there is no network path available). 

The downside is that its cumbersome to for example copy a file to the instances (as you will first need to copy it to
the bounce server and then copied again from there to the instance). The user experience is further lessened by having
to use another computer than your personal workstation (the bounce server instance) to 99% of your tasks.

Aside from this, the bounce servers own security becomes paramount. Updating the bounce server regularly (as well
as locking it down using security groups) is highly recommended. 

.. tip::

   While it might sound appealing to mix-match the above alternatives, we strongly recommend implementing a single
   strategy for cloud access and making sure its locked down.

   With many ways to access your cloud infrastructure, the risk of compromise increases drastically which
   decreases your security.

   A well rounded solution is the client VPN. This will provide a good tradeoff between security and usability
   for management of cloud infrastructure. 

..  seealso::

    - :doc:`/networking/router/index`
    - :doc:`/networking/floating-ips`
    - :doc:`/networking/client-vpn/index`
    - :doc:`/networking/site-to-site-vpn/index`


=========================================
Networking regions and availability zones
=========================================

Networking is a core service and is available in all Binero clouds regions and
availability zones.

Below we outline the region and availability zone concepts for networking.

If you are looking to deploy a geographically redundant network, understanding the base
design principles will help you create a resilient and highly available system with no
dependencies between the availability zones.

This is key to retaining connectivity (and system functionality) in case of a hypothetical
availability zone outage scenario.

Networks and subnets
--------------------

:doc:`network/index` (and :doc:`subnet/index`) is available in all availability zones in the
region, but **not** between regions.

This enables you to connect instances in different availability zones directly to each other
on the same network (same layer 2 broadcast domain).

We do recommend the routed approach to inter availability zone communication (more
information in section about routers below). 

.. tip::

   A valid use case for using the same subnet in more then one availability zone would be a non
   routed back network.

   You could use this for backups, database connections, access from bounce server, and so on.

   If its only used for traffic within the platform and not for ingress or egress to the
   platform, it will not introduce dependencies on a specific zone.

   This is a solution but implies multi homing instances (adding more than one port) which would
   negate the gain by having to maintain more firewall rules, separate IP networks and more
   complex to troubleshoot.

   Our recommendation is to use routing to communicate between availability
   zones. More on this below.

Routers
-------

Routers are always local to an availability zone even though its possible to provision multi-zone
routers by using the OpenStack terminal client or OpenStack Horizon.

The reason for this is that Binero cloud does not provide anycast public IP addresses, see section
on :doc:`floating-ips` below for more information on this subject. 

.. note::

   Selecting more than one Availability Zone Hint when provisioning a router will, silently, still
   provision in just the first zone of the region.

When using many availability zones, take care to have at least one router per zone and
always connect your instances or services to the router that is local to them.

While its possible to connect a port on an instance to a router in a different availability zone via
a network (as outline above), the router is not provisioned with high availability between zones (meaning
the router would go offline should the zone its placed in go offline).

All traffic flowing through a router will go through the same availability zone (regardless of the
location of the instances behind the router).

When setting up systems with geographic redundancy by using more then one availability zone, its recommended
to setup a router in each availability zone and :doc:`route traffic between them <router/routing-between-networks>`
as this (in a scenario in which an availability zone becomes unavailable) would just bring down the one availability
zone.

Floating IP addresses
---------------------

Because Binero cloud is does not provide anycast public IP addresses, floating IP addresses are still
local to a single availability zone.

While its possible to connect a floating IP to an instance in another availability zone by using a network (that
stretch across zones), should the availability zone that hosts the floating IP for some reason fail, the floating
IP will fail with it also in the second availability zone. 

When working with public access, it's recommended to connect floating IP addresses that are local to the instances
or services they connect to, through a router that is also hosted in the same availability zone.

To send traffic to one or more availability zones, a `global load balancer </regions-and-availability-zones.html#global-load-balancing>`__
would route traffic to all floating IP addresses.

Load balancer
-------------

The same way as with floating IP addresses and routers, :doc:`load-balancer/index` will be able reach instances
in availability zones that is not local to the load balancer (because the networks stretches across availability
zones).

The actual load balancers, are still only local to a single zone and will fail in the even the zone
should fail. That would leave the load balancer unreachable and the service down.

Summary
-------

When working with networking in many availability zones, you should take care to not introduce dependencies
between the zones.

A dependency would bring down networking across availability zones should the availability zone with being
dependent on become unavailable. This would negate your efforts to setup a geographically redundant network
when it matter the most.

..  seealso::

    - :doc:`/networking/router/index`
    - :doc:`/networking/floating-ips`
    - :doc:`/networking/load-balancer/index`
    - :doc:`/regions-and-availability-zones`


=================
Reverse DNS (PTR)
=================

General concept
---------------

A PTR (or reverse DNS) record is a DNS record that returns a name from a question containing an IP address.

DNS would normally do the opposite, that is return an IP-address from a domain name. Thus, when having the
IP address but not knowing the name, reverse DNS comes into play. 

The most common use-case for reverse DNS is when setting up an email relay. This is because email servers
normally use their domain name when communicating with each other via the so called HELO (or EHLO) message.

For anti spam purposes, its common on the internet to require that the HELO message (which might be for
example ``smtp.binero.com``) comes from an IP-address that in turn has the same PTR record.

By comparing HELO with the PTR and with a forward lookup (asking what IP-address ``smpt.binero.com``
points to), its less likely that the email is spam. 

Aside from this, when doing a traceroute (or other diagnostics involving IP addresses and routing) its good
practice to add a PTR record to understand the routing path by using more human naming.

Using PTR in Binero cloud
-------------------------

PTR is (via the platform) only available on :doc:`floating-ips` (not on :doc:`directly-attached-ips`).

While you would be able to setup a local DNS server on your project to service PTR records for your internal
IP addresses, those would not be publicly available on the internet (since they are on private IP space that
many people use). 

TTL
^^^

When adding a PTR record, you will also need to select a TTL (short for *Time To Live*).

The TTL determines how long DNS servers should cache the DNS record. What this means is that when a lookup of
the PTR record by a resolver, which is the server that the end-user would use to lookup DNS names, that resolver
will cache the record for this amount of time, in seconds.

This has many advantages, load becomes distributed and concurrent DNS resolutions of the same domain is much
faster improving the user experience.

The downside is that when changing a DNS record, it will take this long before you can be sure that the change
is visible worldwide. We recommend an hour (3600 seconds) as a well rounded TTL. 

.. note::

   Setting less than 3600 seconds might appear to work but our backend changes this to 3600 as this is
   the smallest number we allow.

Please see below for information on how to setup a PTR record on a floating IP.

Adding PTR records using the cloud management portal
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a PTR record to a floating IP by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Network** and then ``Floating IPs`` in the sidebar menu.

- On the floating IP that you want to assign the PTR to, press the small cog icon **set PTR**.

- In the **domain** field, enter the full PTR (for example ``smtp.binero.com.``). Note the final dot in the example
  which needs to be there (this is the root zone that is most often left out). 

- In the description field, enter an optional description of the IP (for example *external mail relay*).

- In the TTL field, enter the TTL (in seconds) you prefer.

- Press **Save**

.. note::

   You are not able to unset a DNS record by using the cloud management portal (but you can change it). This can be
   done using either horizon or the terminal client, see below.

Adding PTR records using OpenStack Horizon
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a PTR record to a floating IP by using the
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **DNS** and then **Reverse DNS** in the sidebar menu.

- On the floating IP that you want to assign the PTR to, press the **Set** button.

- In the **domain** field, enter the full PTR (for example ``smtp.binero.com.``). Note the final dot in the
  example which needs to be there (this is the root zone that is most often left out). 

- In the description field, enter an optional description of the IP (for example *external mail relay*).

- In the TTL field, enter the TTL (in seconds) you prefer.

- Press **Save**

- You are also able to unset (as in remove) the record using the small arrow next to
  the **Set** button according to above.

Adding PTR records using OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To assign a PTR record to a floating IP by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack floating ip list``. Save the UUID (id column) of the IP you want to set PTR on.

- Run this command: ``openstack ptr record list``. Find the row that matches the UUID from previous command. Save
  the complete ID from this command (such as ``europe-se-1:2e6a89c0-d8ae-471e-ae28-e858c63c4c6e``).

- Run this command: ``openstack ptr record set --description [DESCRIPTION] --ttl 3600 [FLOATING IP ID] [PTR DOMAIN].``. Take
  care to end the PTR name (the domain), with a dot (``.``) and replace the values within angle brackets with their corresponding
  values. Description is optional.

- You are also able to unset (as in remove) the record using the following
  command: ``openstack ptr record unset [FLOATING IP ID]``

.. note::

   To be able to run above terminal client commands, you might need to install the Designate client to add the
   commands to the OpenStack terminal client.

   You can do this by running ``pip install python-openstackclient python-designateclient``.

..  seealso::

    - :doc:`/dns`
    - :doc:`/networking/floating-ips`


======
Router
======

A router is the recommended approach for setting up networking in Binero cloud. The opposite of a
router are `directly attached IP addresses </networking/directly-attached-ips>`__.

To a router you connect `networks </networking/network/index>`__ on which you define
`subnets </networking/subnet/index>`_ that your instances connect to. Using a router
enables (among other things):

- :doc:`../subnet/index`

- :doc:`routing-between-networks`

- :doc:`../floating-ips`

- :doc:`../load-balancer/index`

The typical workflow for starting out in the platform would be to do the following tasks:

- :doc:`Create a router <launching-a-router/index>`

- Create a :doc:`network <../network/index>` and :doc:`subnet <../subnet/index>`

- :doc:`Add the subnet as a router interface <../subnet/connect-subnet-to-router>`

After completing these steps (a guide is available :doc:`here </getting-started/launching-an-instance>`), you
would have a versatile networking setup that will enable you to scale out your infrastructure.

.. tip::

   We recommend reading through the :doc:`availability zone concepts for networking <../regions-and-availability-zones>`
   before creating your first router. 

.. seealso::

  - :doc:`../network/index`
  - :doc:`../subnet/index`
  - :doc:`../security-groups/index`

.. toctree::
  :caption: Available services
  :maxdepth: 4

  launching-a-router/index
  routing-between-networks
  nat
  static-routing


==================
Launching a router
==================

To create a router in Binero cloud, you have four main options as outlined in
the links below. Each option have its advantages and disadvantages:

- :doc:`The cloud management portal <cloud-management-portal>` is what we recommended
  and will get a user with limited prior knowledge from A to B quickly. The tradeoff is
  that advanced features are not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in
  OpenStack. Some advanced features might only have a GUI implementation here.

- :doc:`The OpenStack terminal client <openstack-terminal-client>` is a command line
  implementation giving terminal oriented users a quick way to access the cloud. The
  learning curve is steeper than the GUI implementation but the workflow will be
  efficient.

- :doc:`The API </networking/network-api>` is the full OpenStack REST API. Its intended
  for users that are either writing infrastructure as code or integrate with third-party
  applications.

.. note::

   To create your router, you will first need to know in what
   :doc:`availability zone  </networking/regions-and-availability-zones>`
   to create it.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client


===================================================
Launching routers using the Cloud management portal
===================================================

To launch a :doc:`router <../index>` from the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

- Press **Networking** and then **Routers** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Under region, *europe-se-1* should be pre-selected.

- Name your router and optionally provide a description.

- Under **Availability zone (hints)**, select the :doc:`availability zone <../../regions-and-availability-zones>`
  to place the router in. If you are unsure, select *europe-se-1a* here.

- Under **Select external network**, choose the network that corresponds to the availability zone
  you selected. 

- Set **Admin state** to up.

- Press **Create** in the lower right corner.

.. note::

   To connect instances to the router, it will also need a :doc:`network <../../network/index>`
   and a :doc:`subnet <../../subnet/index>` configured.

..  seealso::

    - :doc:`../../regions-and-availability-zones`
    - :doc:`../index`


=========================================
Launching routers using OpenStack Horizon
=========================================

To launch a :doc:`router <../index>` from the
:doc:`OpenStack Horizon portal </getting-started/managing-your-cloud/openstack-horizon>`

- Under **Project**, click **Network** and then **Routers** in the sidebar menu.

- Click **Create Router** in the right upper corner.

- Name your router.

- Check the **Enable admin state** box.

- Under **External Network**, select the network that corresponds to the
  :doc:`availability zone </networking/regions-and-availability-zones>` that
  you placed the router in.

  - If you are unsure, select **europe-se-1-1a-net[N]** here, where N is one of
    the available network numbers.

- Under **Availability zone (hints)**, choose the availability zone that corresponds
  to the network you selected in previous step.

- Press **Create Router**

.. note::

   To connect instances to the router, it will also need a :doc:`network </networking/network/index>`
   and a :doc:`subnet </networking/subnet/index>` configured on a network.

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


=====================================================
Launching routers using the OpenStack terminal client
=====================================================

To launch a `router <../index>`_ from the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run ``openstack availability zone list --network --long``, this will give you the current
  availability zones for routers and networking, to create the router we will need to select
  an availability zone for it. Save the name of the availability zone you want to use for
  following step. If you are unsure, use *europe-se-1a*.

- Run ``openstack network list --external``, this will show the available external networks. Save
  this for next step.

- Run ``openstack router create --availability-zone-hint=[AVAILABILITY ZONE] --external-gateway=[NETWORK-NAME] [ROUTER NAME]``. Replace
  the items within brackets with the values from previous sections as well as the name of the router. Remember to use the network that
  corresponds to the availability zone you chose. If you are not sure what availability zone to use, we recommend using *europe-se-1a*.

.. note::

   To connect instances to the router, it will also need a :doc:`network </networking/network/index>`
   and a :doc:`subnet </networking/subnet/index>` configured. 

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


========================
Routing between networks
========================

General concept
---------------

A :doc:`router <index>` in Binero cloud allows you to perform routing between
networks and to provide incoming and outgoing access to the internet.

The router will connect to different :doc:`subnets <../subnet/index>` using an interfaces
that connects to a :doc:`network <../network/index>`.

A standard router typically routes automatically between all its connected subnets (that is,
subnets on which the router has an IP address).

A router in Binero cloud works the same way, traffic received by a router destined to a connected
network on that router will traverse through the router.

If the destination network is unknown to the router, it will use its *default route* that
normally is facing the internet.

.. note::

   If you for some reason want to use an instance as a router (perhaps you prefer some software based
   routing suite) instead of a router, this is a viable use case.

   Just create an instance with the correct interfaces setup on the different subnets. We recommend
   using a router as some features in the platform depends on having one setup.

Instance configuration
----------------------

A compute instance will also use routing to decide on what interface to egress traffic. It will do this
based on destination and its routing table.

Three main scenarios exist:

- Traffic destined for networks on which the instance itself has an IP address (locally connected networks)
  will take precedence and the instance will instead use ARP to lookup the MAC address of the receiving instance
  and send traffic directly to it. 

- Traffic destined to a subnet that is in the instances routing table uses the next hop (or gateway) of that
  route, as stated in the routing table. The next hop would typically be a router. 

- Finally, traffic destined to somewhere that is unknown to the instance (not in its routing table).

  - This traffic is (provided one is setup) sent to the instances default route (the route used when no more
    specific route exists). When creating a subnet in Binero cloud, a default gateway is normally define for
    that subnet. The IP that is the default gateway is normally connected to a router.

.. tip::

   While its possible to connect an instance to many networks each with different routers, this will require you
   to maintain static routes on the instances themselves (according to the middle option above).

   You can perform this  with DHCP but is still cumbersome and will add complexity in maintaining the firewall when
   traffic ingress and egress through different interfaces.

   An easier approach is to use just a single interface facing a single router which becomes responsible for handling
   the upstream routing. In this scenario, only the first and third examples according to above relevant.

Single router
-------------

.. svgbob::

  
                 Router1               
  
                                 
                                 
                                 
        
   Subnet1     Subnet2     Subnet3 
        

The easiest routing setup (which is the one we recommend, if not using more then one
:doc:`availability zone <../regions-and-availability-zones>`) is using a single router
with many subnets connected.

Reasons for wanting to route between different subnets might be: 

- Wanting to have different security zones where you force traffic through the
  router.

- Wanting to split different applications or users in their own networks. 

- Wanting to keep a staging system and production system separated.

If you use a single router, make sure its the default route for your instances (see :doc:`../subnet/index`),
as routing between the networks would then work by default.

The typical caveat (if traffic is not flowing as expected) would be :doc:`../security-groups/index` that (if
not setup properly) blocks traffic.

Multi router
------------

.. svgbob::

                                               link
       network
                   Router1               
          
                                              
                                              
                                              
                
     Subnet1     Subnet2     Subnet3       
                
                                                 
                                                  
          
                   Router2               
    
                                        
                                        
                                        
          
     Subnet4     Subnet5     Subnet6 
          

The typical use case for having many routers is to run a multi availability zone setup for geographic
redundancy (along with many other use cases).

If you are considering designing your network with availability zones, we strongly recommend you to read
our :doc:`../regions-and-availability-zones` documentation which explains the concepts before proceeding.

When sending traffic (routing) between routers, its recommended to first setup the individual routers so
that they work as intended on their own before proceeding to route between them.

When setting up routing between routers, we use a *link network*. This is a standard private subnet that
will not connect to instances, just the different routers in the setup.

We recommend :doc:`choosing an IP range <../subnet/index>` for this subnet that is different from your
production subnets where you run your instances and services.

Follow the below steps to setup routing between two (individually functioning) routers:

- :doc:`Create a new network <../network/index>` (with a corresponding subnet) to use as
  link network.

- :doc:`Connect the new link-network to the routers <../subnet/connect-subnet-to-router>`. 

- :doc:`Setup static routing <static-routing>` for your destination networks to use the new link-network. You
  need to add static routing to all routers.

- Depending on your settings, change or add :doc:`security groups <../security-groups/index>` on instance
  facing :doc:`ports <../ports>` to allow traffic.

Once you've completed the above steps, you will be able to forward traffic between routers from all instances
behind the routers. 

Dynamic routing
---------------

For wanting to use a dynamic routing protocol (most commonly BGP), you would need to use instances as routers.

Binero cloud only supports static routing in the routers. 

..  seealso::

    - :doc:`static-routing`
    - :doc:`../security-groups/index`
    - :doc:`../regions-and-availability-zones`


===
NAT
===

General concept
---------------

NAT (Network Address Translation) provides instances with internet access
when located behind a :doc:`router <index>`.

By default, a router will have a source NAT setup when launching it. This means that
the router has :doc:`floating IP addresses<../floating-ips>` on its external interface
and when instances on the internal network wants to access the internet, the router will
change the source IP on the packet to its own floating IP.

The reason for this is that :doc:`subnets </networking/subnet/index>` are normally assigned
from the pool of IP addresses reserved for internal usage, according to IETF RFC1918.

The router is able to mask requests as coming from itself and by doing so, all instances are
able to share the routers single public IP.

Incoming requests to an instance can't use the routers IP address but rather a
:doc:`floating IP </networking/floating-ips>`.

If an instance has a floating IP configured, it will instead use its floating IP as source
address when connecting to the internet.

This works the same way but instead of sending inbound and outbound requests from different
IP addresses, the floating IP takes precedence. 

Verify NAT address of router
----------------------------

Below are some ways to verify which IP address your router will use for outbound internet
connections (SNAT):

- In the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`, press **Network** and
  then **Routers**. Press the router you want to verify. under **External network** you will see the
  outbound SNAT IP.

- In :doc:`/getting-started/managing-your-cloud/openstack-horizon` portal under **Project**, press
  **Network** and then **Routers**. Press the router you want to verify. under **External gateway** you
  will see ``External fixed IPs``. These are the outbound SNAT IP.

- Using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`, run this
  command: ``openstack router list`` to get a list of your routers

  - Followed by ``openstack router show [ROUTER NAME] -c external_gateway_info -f yaml`` which
    will give you a yaml formatted list of the outbound SNAT IP.

..  seealso::

    - :doc:`/networking/router/index`
    - :doc:`/networking/floating-ips`


==============
Static routing
==============

General concept
---------------

Routing is, in the context of Binero clouds implementation, done based
on destination.

The two key parameters when talking about routing is:

- The destination IP

- The gateway IP used as the next hop

When routing packets, a router will consult a routing table that will have different routes to networks
available, each matched to a next hop (or gateway) which is the next device the packet gets sent to
reach its destination.

When routing an IP packets, the destination IP belongs to a network of which we match against the
routing table, the router will then forward the packet to the next hop (or gateway) to reach that
network.

Should the router be directly connected to the network in question (local to the network), it will
instead use Address Resolution Protocol (ARP) to map the destination IP to a hardware (MAC) address
and forward the packet to its final destination.

Static routing is the process of manually defining what networks exists and what next hop to use when
forwarding packets to reach that destination.

It's only needed if there is *more then a single router in the network* as a single router normally
connects to all networks directly and already knows how to reach all networks.

If a routing entry is missing and the routers are not able to match a destination IP to a network in
their tables, one of two things will happen:

- The routers will use their **default route** (if present). This would normally point upstream to the
  internet so effectively the packet would get lost in an upstream router that does not route private
  networks and would drop the packet.

- If there is no default route, the packet would get dropped.

.. note::

   The next hop of a routing entry will need to be reachable *and on a common network* (that is a network
   to which both the sending and receiving router has).

   Routing cannot rely on routing to reach a gateway, all gateways need to be directly accessible. A link-network
   is commonly use for this (but any subnet will suffice when the routers are both connected to it), see
   our :doc:`routing-between-networks` for more information.

Working with static routes
--------------------------

Below is how to add static routes to a :doc:`router <index>`.

Since you route based on destination (but also need to account for return traffic, that is the traffic that
a reply sends), you would want to add the routes for both sides with the distinction that the networks defined
on each router are the ones **not local** to that router.

The routing entries are *not the same* on any two (or more) routers.

Static routing is currently only supported using :doc:`OpenStack Horizon </getting-started/managing-your-cloud/openstack-horizon>`
or the :doc:`OpenStack terminal client </getting-started/managing-your-cloud/openstack-terminal-client>`.

OpenStack Horizon
^^^^^^^^^^^^^^^^^

- Under **Project**, click **Network** and then **Routers** in the sidebar menu.

- Press the name of the router you want to connect the subnet to.

- Press the **Static Routes** tab on the top.

- Press **Add static route**

- In the **Destination CIDR** field, type the *destination IP range* (that is, the network behind the neighbour router that you
  want to reach) in :doc:`CIDR format <../subnet/subnet-format>`. 

- In the **Next Hop** field, type the gateway address, that is the address that the neighbour router has on the network which
  it shares with the router you are adding the static route to - and to where you want to forward traffic.

- Press **Submit**. 

OpenStack terminal client
^^^^^^^^^^^^^^^^^^^^^^^^^

- Run: ``openstack router list`` to get the list of your available routers. Save the name of the router you want to
  connect the subnet to.

- Run: ``openstack router add route --route destination=[DESTINATION_RANGE],gateway=[GATEWAY IP] [ROUTER NAME]``, replacing
  the items in brackets to *destination IP range* (the network behind the neighbour router that you want to reach)
  in :doc:`CIDR format <../subnet/subnet-format>`, the gateway address (the address that the neighbour router has on
  the network which it shares with the router you are adding the static route to - and to where you want to forward traffic)
  and the router name (from previous step).

..  seealso::

    - :doc:`../subnet/subnet-format`
    - :doc:`routing-between-networks`
    - :doc:`index`


==========================================================
Managing security groups using the Cloud management portal
==========================================================

Creating a security group
^^^^^^^^^^^^^^^^^^^^^^^^^

To create a :doc:`security group <index>` by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Security groups** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Under region, *europe-se-1* should be pre-selected.

- Name your security group and optionally provide a description.

- Press **Create**

.. note::

   The new security group will not have any rules so wont actually do anything. To
   add rules, follow next step.

Adding rules to a security group
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add rules to a :doc:`security group <index>` by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Security groups** in the sidebar menu.

- On the security group you want to add a rule to, press the small **create rule** icon (rows of
  text with a small **+** plus sign).

- Define the rule. More info in our :doc:`designing-rules` article.

- Press **Create rule**.

.. note::

   For ease of use, we recommend using the already defined rules. These are project specific
   so you can also tweak them as you see fit. 

Adding a security group to an instance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add a :doc:`security group <index>` to an instance by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you want to add the security group to.

- In the top tab (under the instance name), security groups menu is available when scrolling
  to the right. Press it.

- Press ``Allocate``

- Select the group you want to add.

- Press **Connect group**

.. note::

   Remember: you need to add your security groups to all instances.

..  seealso::

    - :doc:`../index`


==============================
Designing security group rules
==============================

Recommendations
---------------

Below are some key points to take into consideration when designing security group
rules (firewall rules) in Binero cloud that will lead to a more secure application.

- Its recommended to make the rule as granular as possible. If you for example want to
  give access to a web server, then just add a rule for HTTP (tcp/80) and HTTPS
  (tcp/443), don't also allow access to UDP or any other ports.

- Lock down the source IP when the rule is for management, don't expose services
  that are not needed to be provide as your service. Only leave rules that give
  access to internet facing services from any source.

- Use the custom rules that enables you to add a destination port. Don't use the
  all open security group even if its internal. Its a large gain in security to
  just add access to the services that's needed as vulnerable services are one of
  the biggest reasons for security breaches. The fewer exposed, the better, including
  on internal networks.

- Since you apply your security groups per instance you wont set a destination when
  creating rules. The destination (or source if its an egress rule) depends on which
  instances you add your security groups to. Take care to not add a rule to an instance
  that gives to much access. Rather create more security groups with more granular rules
  so that you reduce your instances exposure. 

- Remember egress or ingress (more info on the :doc:`index` page). Generally speaking, the
  you should add as few ingress rules as possible since security becomes worse when providing
  ingress access, we recommend that you design to have few and that they have a specific purpose.

- If you still have the `default group <index.html#default-settings>`__ activated on your
  instances, you do not also need to provide access for your internal systems to reach each
  other. For an easier setup, we recommend keeping the default group (as it will emulate a
  traditional network where all local machines will reach each other). For a more secure
  solution, we also recommend adding security groups for internal access and removing the
  default group. **We do not recommending mixing the two options as that will be a less
  predictable (and more complex to understand) design**.

- When doing many subnets to allow for different security zones (the need for this is
  negated by using proper security groups that replace the default group as that will give
  you security between your instances on the same network as well), remember that the default
  group will leave your internal networks open to each other even though they are traversing
  a router.

- Create security groups based on use cases. A use case might be a web server and another might
  be a database server. You should design the rules for each use case with the instance type in
  mind. Remember that its possible to add many security groups to any instance. Its much easier
  to have many granular security groups instead of a single complex security group.

- ``0.0.0.0/0`` means *the entire internet* when using it as a destination. We only recommend
  using this to provide access to a service that should indeed be globally reachable.

- Rules are by default stateful. This means when we receive an ingress request and it's accepted,
  the *return traffic* (the traffic that the answering service sends as a reply to the request) is
  also allowed, irrespectively of wether that traffic flow is explicitly defined.

  - This is a standard behaviour in a firewall. If you want to create a non stateful (stateless)
    rule, you can create the group using the :doc:`terminal client <openstack-terminal-client>` and
    the flag ``--stateless``. A stateless rule would only work in a single direction (egress or
    ingress). We generally recommend stateful rules but for some use-cases (for example asymmetrical
    routing where you would not send the return traffic the same path), stateless rules might be
    required.

.. tip::

   The best way to ensure that you have correctly applied your rules is by testing. You can test by using
   the ``nmap`` command that can perform a port scan.

   For instance ``nmap -T4 1.2.3.4`` would scan the server with IP 1.2.3.4 (the -T4 flag limits the timeout
   per port to 10 ms which speeds up the otherwise slow process). The output will be a list of open ports.

   Remember to scan both from the inside (using your internal instances against their internal IP addresses)
   and by using for example your workstation (against the floating IP that expose your services) to get the
   public internet perspective.

   We recommend doing scans after setting up or changing your security groups to make sure the actual result
   corresponds with your expectations. Binero protects it's network against malicious traffic, we don't
   recommend doing excessive port scanning as that can trigger our intrusion detection system to block you.

Available parameters
--------------------

The following section shows what parameters you can use to create security groups. Note that not all
parameters will be available from the GUI, some might require using the :doc:`terminal client <openstack-terminal-client>`

We provide the parameters to the terminal client in the parenthesis below.

- Remote IP (``--remote-ip``) - Remote IP address or IP range (may use :doc:`CIDR notation <../subnet/subnet-format>`,
  default for IPv4 rule: 0.0.0.0/0, default for IPv6 rule: ::/0).

- Remote Group (``--remote-group``) - Remote security group (name or ID). Instead of IP, you could use a remote
  group to decide from where the traffic originates.

- Destination port (``--dst-port``) - Destination port, may be a single port or a starting and ending port range
  137:139. Required for IP protocols TCP and UDP.

- Protocol (``--protocol``) - IP protocol  AH, DCCP, EGP, ESP, GRE, ICMP, IGMP, ipv6-encap, ipv6-frag, ipv6-icmp, ipv6-nonxt,
  ipv6-opts, ipv6-route, PGM, RSVP, SCTP, TCP, UDP, UDP-Lite, VRRP or a integer representations (0-255) or any (-1), default is 
  any (all protocols).

- ICMP typ (``--icmp-type``) - ICMP type for ICMP IP protocols.

- ICMP code (``--icmp-code``) - ICMP code for ICMP IP protocols.

- Direction (``--ingress`` or ``--egress``) - Rule applies to incoming or outgoing network traffic (incoming is
  default).

- IP version (``--ethertype``) - whether to use IPv4 or IPv6; default is based on IP protocol.

..  seealso::

    - :doc:`../index`


===============
Security groups
===============

General concept
---------------

Security groups allows for filtering traffic on :doc:`ports </networking/ports>` in the
platform. This is commonly known as a firewall.

Binero cloud will provide a variety of default security groups that you can combine to allow
(or by not adding or removing, disallow) traffic to flow through the cloud.

One important difference from a traditional network is that security groups are also filtering
traffic between instances on the same :doc:`subnet </networking/subnet/index>`.

The default settings (see below) would allow internal access.

A security group is (as the name implies) a *group* of rules. This makes it easier to setup common
use-cases if you have certain access-scenarios (for example maybe you want to add HTTP, HTTPS and
port 8080 from all sources and SSH from a single IP to the majority of servers, this could then be
a single group with 4 rules in it).

Managing security groups
------------------------

You are able to manage security groups by using either of the below tools.

Common tasks involving security groups is creating them (and adding rules) and applying them
to instances (to filter traffic).

- :doc:`The cloud management portal <cloud-management-portal>` is what we recommended and will get a
  user with limited prior knowledge from A to B quickly. The tradeoff is that advanced features are
  not always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in OpenStack. Some advanced
  features might only have a GUI implementation here.

- :doc:`OpenStack terminal client <openstack-terminal-client>` is a command line giving terminal oriented
  users a quick way to access the cloud. The learning curve is steeper than the GUI implementation but the
  workflow will be efficient.

Interface direction
-------------------

We perform filtering on interfaces (ports) on :doc:`instances </compute/index>`, an important concept is
the *direction* of the traffic.

The direction could either flow **to** the interface on the instance (*ingress* ) or **from** the interface
on the instance (*egress*).

We filter traffic with security groups in both directions. The use-case for this is normally that you would
enforce less security on traffic that originates (egress) your instance than traffic that ingress because it's
common that threats are external.

A common setup is to allow all outbound (*egress*) traffic but filter the inbound (*ingress*) traffic to an
interface. We still recommend that you also filter outbound as that can protect you from malicious traffic
on your local network if you are ever compromised.

.. note::

   Situations might occur where you might want to filter outbound traffic as well, particularly if your
   infrastructure is inadvertently used for outbound attacks.

   This is difficult as returning traffic (the traffic that's sent as an answer to a request to
   one of the services in your infrastructure) is normally sent to a randomised high port.

   Outbound filtering is most often used to block something explicitly. Security groups does not
   support explicit blocking (rather its inferred, if there is not an explicit allow rule, traffic
   gets dropped). 

Default settings
----------------

A newly provisioned instance will have the ``default`` security group added to its port.

This security group will allow all traffic but only *from other instances that also have the default group* (that
is, it also evaluates if the traffic was using this group to **egress** an instance in the cloud).

This means we allow all traffic within the same network (and also within different networks on the same router)
but **not** traffic that ingresses via a :doc:`/networking/floating-ips` or from
:doc:`another availability zone <../router/routing-between-networks>` as that traffic will not have originated
behind the default group.

Removing the default group could potentially remove outbound access to the internet through the router if there
are no other security groups available. Restoring it in case of any issues is fast, by re-adding it. 

.. note::

   An instance that does not have a floating IP connected and sits behind a router is not reachable from the internet
   and is not as vulnerable.

   A floating IP will use :doc:`../router/nat` to map a public IP to the instance real IP (which is not globally routed).

   Its good practice to not have a floating IP on instances that does not need to be directly reachable from the
   internet (because they host internal services - such as databases) but this will require a
   :doc:`a VPN or a bounce server </networking/reaching-your-instances>` to manage those instances.

Allowing access to an instance over floating IP
-----------------------------------------------

A common task relating to security groups is to configure initial access to a new instance with a floating IP.

Since a :doc:`floating IP </networking/floating-ips>` is for public access, traffic ingress is not evaluated as coming
from the default group and you need to setup explicit security group rules to allow incoming traffic.

To access an instance via a floating IP, you would need to add a security group or security group rules for the service
you want to access.

The two most common types of access to manage operating systems are SSH (for Linux and its derivatives) and RDP (for Windows).

Binero cloud includes security groups for SSH (``bin-ssh``) and RDP (``bin-remote-desktop-protocol``) preconfigured so you just
need to add them to your instances.

See above for guides on how to add a security group by using the tool you prefer.

Pre-configured security groups
------------------------------

Security groups are local to each project (or customer) in Binero cloud.

When signing up, there will be some security groups that are already created in your project. They are there to simplify for
you, making it quick to allow traffic to commonly used services (such as SSH).

A more advanced user might want to create security groups based on instance types (to add all rules that's needed in a single
security group), using the pre-configured security groups is a quick way to get going in the platform.

You can add one or more security groups to ports which combines them together if you don't want to maintain large security
groups.

.. important::

   The pre-configured groups will all provide an ingress rule according to the groups name but will also provide wide open egress
   rules from the instance.

   This is to simplify for users that would just have a single group and then not be able to reach the internet.

   If your intent is to do outbound filtering, you are able to either remove rules from these groups or create new groups that are
   similar but without the egress rules.

   Assuming outbound filtering is not implemented (which would be the most common use case), the egress rules makes no difference. 

.. toctree::
  :caption: Available services
  :maxdepth: 1

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client
  designing-rules


================================================
Managing security groups using OpenStack Horizon
================================================

Creating a security group
^^^^^^^^^^^^^^^^^^^^^^^^^

To create a :doc:`security group <index>` by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Security Groups** in the sidebar menu.

- Click **Create Security Group** in the right upper corner.

- Name your security group and optionally provide a description.

- Press **Create Security Group**

.. note::

   The new security group will not have any rules so wont actually do anything. To add
   rules, follow next step.

Adding rules to a security group
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add rules to a :doc:`security group <index>` by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Security Groups** in the sidebar menu.

- On the line of the security group you want to add a rule to, press **Manage rules**

- Press **+ Add rule** in the top right corner.

- Define the rule. More info in our :doc:`designing-rules` article.

- Press **Add**

.. note::

   For ease of use, we recommend using the already defined rules. These are project specific
   so you can also tweak them as you see fit. 

Adding a security group to an instance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add a :doc:`security group <index>` to an instance by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- On the line of the instance you want to add a security group to, press the small arrow
  and select **Edit Security Groups**

- Press the **+** (plus) sign on each security group you want to add to the instance.

- Press **Save**

.. note::

   If you instead choose to press **Edit Port Security Groups** (in step 2 above) you are able to set
   security groups on a per interface (port) basis instead.

   This might be useful if you have many networks connected to your instance (which we don't recommend)
   and want to have different settings on them. 

.. note::

   Remember: you need to add your groups to all instances.

..  seealso::

    - :doc:`../index`


============================================================
Managing security groups using the OpenStack terminal client
============================================================

Creating a security group
^^^^^^^^^^^^^^^^^^^^^^^^^

To create a :doc:`security group <index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack security group create --description [DESCRIPTION] [NAME]``

.. note::

   The new security group will not have any rules so wont actually do anything. To add
   rules, follow next step.

Adding rules to a security group
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add rules to a :doc:`security group <index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Running the following command will give you a detailed overview of what options are
  available for creating a rule: ``openstack security group rule create -h``

- Define the rule. More info in our :doc:`designing-rules` article. By using the help output
  from above, you are able to match your rule to the needed parameters. 

- An example command to create a security rule that allows SSH using the terminal. Replace the
  items in angle brackets with corresponding data. 

::

    openstack security group rule create \
      --protocol tcp \
      --dst-port 22 \
      --ingress \
      --remote-ip [X.X.X.X/Y] \
      --description [DESCRIPTION] \
      [NAME]

.. note::

   For ease of use, we recommend using the already defined rules. These are project specific so
   you can also tweak them as you see fit. 

Adding a security group to an instance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To add a :doc:`security group <index>` to an instance by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server add security group [INSTANCE NAME] [GROUP NAME]``, replace
  the items within angle brackets with corresponding data.

- To show what security groups are currently attached to an instance, run this
  command: ``openstack server show --column security_groups [INSTANCE NAME]``, replace
  the items within angle brackets with corresponding data.

Address groups
^^^^^^^^^^^^^^

Address groups enable you to setup a collection of IP addresses and IP ranges that you can then use
to apply to your security groups.

The benefit of this, is that you could re-use them in many security group rules and would not have to
maintain different lists of (the same) addresses. Address groups are only available via the terminal
client, below is how to work with them:

- To create a group, run this command: ``openstack address group create [NAME]``

- To add addresses to the group, run ``openstack address group set --address [x.x.x.x/y] [NAME]``. Address
  could be both a range or a single address.

- To use the address group in a rule, use the ``--remote-address-group [NAME]`` option.

.. note::

   If you instead choose to press **Edit Port Security Groups** (in step 2 above) you are able to set security
   groups on a per interface (port) basis instead.

   This might be useful if you have many networks connected to your instance (which we don't recommend) and want
   to have different settings on them. 

.. note::

   Remember that you need to add your security groups to all instances.

..  seealso::

    - :doc:`../index`


==================================
Adding routes for site-to-site VPN
==================================

Because the remote subnet(s) is unknown to the instances in the cloud, any traffic originating
from the cloud that should traverse the tunnel needs help finding its way to the instance running
the tunnel.

The instance running the tunnel will know about the remote subnet because this is part of the IPsec
protocol to inject routes into the routing table or handled by flows before ever reaching the routing
table.

The two ways to resolve this using routing, which one to use depends on whether you have one or
many :doc:`subnets <../subnet/index>` in your platform.

- The instances that are on the same subnet as the instance running the VPN would need routes
  in their own routing table (normally distributed by DHCP, see below).

- The instances that are on a different network would use their default route to reach the router and
  the router itself then needs a route to the instance to know where to send the traffic.

To summarize: 

- If you only have a single subnet, you just need to add routes via DHCP.

- If you have many subnets, you need to add routes via both DHCP and on the router. 

See below for more information.

.. note::

   Both actions will require a gateway, that is the IP address to send traffic to (also known as next hop).

   This is the local address of the instance running the VPN. Its visible in the GUI for the instance as
   well as in the output of the creation, named ``local_gw_ip``. You will need this IP address below.

Routing via DHCP
----------------

Please see our section about DHCP settings, which is available :doc:`here <../subnet/dhcp>`. 

Routing in router
-----------------

Please see our section on static routing available :doc:`here <../router/static-routing>`.

The **Destination CIDR** field is in this case the remote subnet (the network on the other side of the
tunnel). **Next Hop** is the local IP of the instance running the VPN (``local_gw_ip`` in the output when
you created the VPN).

.. note::

   A third option, in cases where only a few instances should have access and that is managing the routing
   manually on the instances by using whichever method is available in your operating system.

   We don't recommend this approach as it doesn't provide a clear benefit and makes it more difficult
   to maintain.


======================
Advanced configuration
======================

While the VPN service sets up a template configuration of a site-to-site VPN with
verified defaults, should you for example require more IPsec flows (more subnets)
or changing settings, you will need to use the administration interface for the VPN
itself.

When provisioning the VPN, you will get output. The ``mgmt_url`` output will be the
URL that you us for managing the VPN service. Enter it into a browser. 

.. note::

   Remember that only the IP (range) that you added in **Admin IP ranges** when setting
   up the VPN can login.

   You can update that in the :doc:`security group </networking/security-groups/index>`
   named ``IPSec-<name>-<random string>_management``, if your public IP address has
   changed, you may need to edit the security group.

You would also have gotten an ``admin_password`` output, which is a generated password for
your instance. This, combined with the username `admin`` will give you access to the portal.

Security recommendations 
------------------------ 

While the admin password is uniquely generated and secure it **is** saved as part of the
output in the platform in plain text (a user with access to the platform can read it).

While only users with access to the platform could read it and it can likewise be reset
via access to the console of the instance (which users with access to the platform has),
we still advice you to set your own password.

To do this, follow these instructions:

- Login to the admin UI according to above.

- Press **System** in the main menu.

- Press **User manager** in the sub menu.

- Edit the ``admin`` user by pressing the small pencil icon.

- Enter the new password in both password fields.

- Press **Save**

.. note::

   Should you loose your password, it can be reset via :doc:`console output </compute/console>`
   on the instance.

While the PSK (Pre-Shared Key) is uniquely generated, it **is** saved as part of the output in the platform
in plain text (a user with access to the platform can read it).

The IPsec service is using a firewall (other than traffic from the remote gateway) to secure the service.

You might still want to consider switching this from the standard output to your own string. If so, remember
that the PSK need to be identical on both sides of the tunnel.

If you want to change the PSK:

- Login to the admin UI according to above.

- Press **VPN** in the main menu.

- Press **IPsec** in the sub menu.

- Press the small pen icon on the only row under
  the tunnels (default) tab.

- Scroll down to ``pre shared key`` and input an
  equally long and complex string.

- Press **Save**

.. note::

   Remember to also update the PSK on the remote system or the tunnel will not work.

Changing IPsec configuration
----------------------------

While this guide does not exhaustively cover all the features of the VPN service (more information
available `here <https://www.pfsense.org/get-involved/>`__), the most common task you might face with
the site-to-site appliance might be to change the IPsec configuration. Its located under the main
menu **VPN** and then **IPsec**.

The IPsec configuration consists of two phases, phase 1 and phase 2. In phase 1 (also called quick
mode) the you configure settings for negotiating cryptographic parameters.

You can press the small pencil (edit) next to the only row available (in a default installation) to
edit it. 

If you want to edit phase 2 (also known as main mode), you can press **Show phase 2 entries** and then
press the small pencil next to each line. 

To add another flow (another subnet sent across the tunnel), you need to add an extra phase 2 row. The
simplest way to do so is to clone the current row by pressing the small *copy* button next to the
pencil. Then change the relevant information.

..  seealso::

    - :doc:`index`
    - :doc:`configure-remote`


==========================
Configuring the remote end
==========================

Part of the tunnel runs on Binero cloud but the other part runs on the site that you want to connect to.

It's difficult to write a comprehensive documentation on how to setup the remote part of the tunnel as
the equipment used could vary but normally the options that need editing are the same.

IPsec will only work with identical settings on both ends so take care to setup the remove end point
with **exactly** the same settings (as outlined below). 

Remember that for your tunnel to establish and be able to connect from both ends, you will need to allow
traffic from the remote endpoint to your firewall.

Either open up from all traffic from the remote endpoint IP (and *protocols* as IPsec uses more than a
single protocol) or do a selective firewall opening for the following:

- Protocol ESP

- Protocol AH

- Port 4500/UDP

- Port 500/UDP

The below steps should provide the information needed to get a tunnel up and running, we recommend reading
the official documentation from your vendor on how to configure.

- ``Key exchange version``: IKEv2

- ``Remote gateway`` + ``Peer identifier``: The floating IP address of our local pfSense instance (provided
  by the output value ``ipsec_endpoint`` of the IPsec service).

- ``Authentication method`` PSK

- ``Pre-shared key``: The pre-shared key that was either chosen by you or generated while creating the pfSense
  instance (provided by the output value ``ipsec_psk`` of the IPsec service).

.. tip::

   Binero cloud supports (by default) two options for phase 1 encryption. We recommend the first option as it's
   more secure but we provide support for a second option for compatibility with older hardware.

   If you need to change to an even more insecure cipher, edit the phase 1 configuration in the management
   interface. See :doc:`advanced-configuration` for more information.

- Phase 1 configuration:

  - ``SA lifetime``: 28800 seconds

  - ``Dead peer detection (DPD)``: Enabled

  - ``Algorithms``

  - Alternative 1:

    - ``Cipher``: AES-256-GCM with 128-bit ICV (also known as AES-GCM-16)

    - ``Pseudo-random function (PRF)``: Auto / SHA2_384 (this can often be left as auto, if there is not an option
      for it available, assume its automatically chosen).

    - ``(Integrity) Hash``: SHA2_384 (All vendors might not require this)

    - ``Diffie-Hellman (DH)-Group``: 20 (NIST ecp384)

  - Alternative 2:

    - ``Cipher``: AES-CBC-256

    - ``Pseudo-random function (PRF)``: Not used (not relevant)

    - ``(Integrity) Hash``: SHA2_384

    - ``Diffie Hellman (DH) Group``: 15 (modp3072)

.. tip::

   Binero cloud supports (by default) two options for phase 2 encryption. We recommend the first option as it's  more secure
   but we provide support for a second option for compatibility with older hardware.

   If you need to change to an even more insecure cipher, edit the phase 2 configuration in the management
   interface. See :doc:`advanced-configuration` for more information.

- Phase 2 configuration:

  - ``Remote network``: Your local network

  - ``Protocol``: ESP

  - ``Lifetime``: 3600 seconds

  - ``Algorithms``:

  - Alterative 1:

    - ``Cipher``: AES-256-GCM with 128-bit ICV (also known as AES-GCM-16)

    - ``(Integrity) Hash``: SHA2_384 (All vendors might not require this)

    - ``Perfect Forward Secrecy (PFS) key group``: 15 (modp3072)
  
  - Alterative 2:
  
    - ``Cipher``: AES-CBC-256

    - ``(Integrity) Hash``: SHA2_384

    - ``Perfect Forward Secrecy (PFS) key group``: Not used (not relevant)

..  seealso::

    - :doc:`index`
    - :doc:`setting-up`
    - :doc:`advanced-configuration`


================
Site-to-site VPN
================

A site-to-site VPN is a tunnel that connects two, or more, sites.

A site can consist of one or more :doc:`subnets <../subnet/index>` that all route over the
tunnel but the standard use case is a single subnet on each side that send its traffic to the
other site through the tunnel where the encrypted traffic provides security from third parties
being able to read the traffic in-transit.

In Binero cloud you are able to provision site-to-site VPNs by using the
`IPsec protocol <https://en.wikipedia.org/wiki/IPsec>`__.

This protocol consists of two phases, the first negotiates encryption for itself (phase 1) and
sets up a tunnel that then kicks off the negotiation for the second phase (phase 2) that is for
the agreed encryption of the traffic send over the tunnel.

Routing is inherent in IPsec as traffic flows between subnets. Because of this, tunnels (phase 2 flows)
are always defined as a single source and destination network. There can be many different flows as
they define traffic flow between one or more subnets.

The VPN service will come with its own management interface (web based) that enables you to manage change
security settings, add or remove flows, and so on.

Complete documentation of all the features our VPN solution provides is out of scope for our support pages
but more information is available on https://www.pfsense.org/get-involved/ and the web in general.

Please see the subsections of this section for the most common information relating to the service. Our
support staff can also help you out.

To create a VPN-tunnel between sites, first read through our :doc:`prerequisites` and then follow our
:doc:`setting-up` guide.

Once done, we provide general guidelines for :doc:`setting up the remote end <configure-remote>` of the
tunnel.

..  seealso::

    - :doc:`../client-vpn/index`

.. toctree::
  :caption: Available services
  :maxdepth: 2

  prerequisites
  setting-up
  configure-remote
  adding-routing-for-vpn
  advanced-configuration


=================================
Prerequisites for site-2-site VPN
=================================

You can only create site-to-site VPNs from the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

Please see :doc:`/getting-started/managing-your-cloud/index` for more information.

.. note::

   For the purposes of this guide, the local location is the Binero cloud data center
   and the remote locations is the site to which you want to connect.

Checklist to get started
------------------------

You will need the following information to get started:

- An existing :doc:`subnet <../subnet/index>` already setup in the
  platform which will be the local subnet (in :doc:`CIDR notation </networking/subnet/subnet-format>`)
  that's sent over IPsec to the remote subnet (which would be your office or secondary
  data center, for example).

- The IP address of the remote gateway (your public IP that your internet service provider
  provides to you) for the tunnel.

- The remote subnet (in CIDR notation) that's tunneled (meaning the network on which the
  computers or servers that should be able to connect to resources in the cloud using the
  tunnel are on).

- A management IP that you allowed access to managing the VPN over the internet.

  - If you have another means of accessing the internal network (where the VPN server will
    be running), for example our :doc:`client VPN </networking/client-vpn/index>`, this is
    not needed and once the tunnel is up, you can connect to its remote IP. Normally the
    management IP is the same as the local gateway.

Sizing guidelines
-----------------

Our tests show that CPU time and CPU clock-speed are the two main factors in getting a high
throughput from the VPN.

Using ``iperf3`` we've measured 1+ Gbps through the service by using a standard **hp.2x4**
:doc:`flavor </compute/flavors>` which we believe is a good choice, and is the default option.

If cost is your main concern, scaling down to a gp.1x2 will still run the VPN with no issues
but expect throughput around 100 Mbps (which is enough for many use-cases).

Scaling up further to more than two CPU cores is not recommended as the speed gain is minimal
instead make sure to use the high performance flavors to get greater CPU clock speeds. 

If you want your tunnel 100% consistent, we also recommend our gp.4x8-pinned flavor which will
grant all the capacity of the four cores to the VPN.

.. note::

   Performance is equally based on the other end of the tunnel and your remote bandwidth. Take care
   when measuring the capacity of your bandwidth, if you don't get the throughput you expect, consult the
   manufacturer of your IPsec equipment about what its rated for.

Remote equipment
----------------

To run a site-2-site tunnel to Binero cloud, you will need a remote router or firewall that is
capable of IPsec.

This standard is well supported in hardware (and software) from a wide variety of manufacturers. 

We recommend using the central firewall or router as remote endpoint as that will greatly simplify
routing, its possible to also forward the correct ports to a server (virtual or physical) on the
inside of the firewall as Binero cloud support the ``NAT-T`` extension of the IPSec protocol.

This will make the routing more difficult as, depending on architecture, you might need to add
routes to the different computers and servers directly. 

If investing in a new router, Binero has good experience from the `pfSense solution <https://www.pfsense.org>`__
which is available in both hardware and software versions.

..  seealso::

    - :doc:`index`
    - :doc:`setting-up`


===========================
Setting up site-to-site VPN
===========================

We recommend that you first read through our :doc:`prerequisites` guide. Follow
these steps to setup site-to-site VPN by using the cloud management portal:

- Press **VPN** and then **IPsec** in the sidebar menu.

- Click the **+ (plus)** symbol in the lower right corner.

- Enter the required information:

  - ``Name``: This is a friendly name used to distinguish the resources
    created by this template.

  - ``Admin SSH key pair``: Your SSH key pair used to access the pfSense
    instance by SSH.

  - ``Admin IP ranges``: IP ranges in :doc:`CIDR notation </networking/subnet/subnet-format>`
    that's allowed to access the pfSense instances WebUI and SSH server through the router.

    - This would be your office public IP address followed by ``/32`` (for example ``8.8.8.8/32``)
      in which case traffic from ``8.8.8.8`` would be able to reach management. If you prefer open access to the VPN
      server, you can enter ``0.0.0.0/0`` here (not recommended) or leave the standard ``127.0.0.1/32`` which will
      not allow access other than from inside the cloud (for example via the client VPN). You can change this later
      by updating the security group named ``IPSec-<name>-<random string>_management``.

  - ``Availability zone``: This needs to be the same availability zone that's used for the
    network that you intend to use.

  - ``pfSense instance flavor``: The instance flavor used for the pfSense instance. The default
    ``gp.1x2`` should be enough for most use cases.

  - ``Private network``: The network that's exposed through IPsec.

  - ``IPSec pre-shared key``: If you want to use a specific pre-shared key you may enter that
    here. If left at default, a random key will be generated for you.

  - ``IPSec peer IP``: This should contain the public gateway address for the remote IPsec
    endpoint. It should not be a DNS name but an IP-address.

  - ``IPSec remote network``: This should contain the network address (in CIDR notation) for
    the tunneled remote network.

- Press **Create**. VPN creation takes considerably longer than setting up a standard
  instance. This is due to the (comparably) long first time bootstrap of the VPN instance
  when generating ciphers and so on.

- Post installation tasks:

  - In the output of the stack, save the output for the ``mgmt_url`` parameter, along with the
    ``admin password`` (the username is always ``admin``). You will need these when doing extra
    configuration.

  - We recommend logging into the above URL and changing the password and pre-shared key, see
    :doc:`advanced-configuration` for more information on this.

  - After the service creation is complete you will need to :doc:`add routes to the remote subnets <adding-routing-for-vpn>`
    to get traffic flowing over VPN.

  - Add the newly created security group ``IPSec-<name>-<random string>_access`` to all instances
    that should be accessible from the remote end of the IPSec tunnel.

  - Once you are ready, proceed to configure :doc:`the remote end <configure-remote>` of the tunnel.

.. note::

   Some steps included in the setup that might cause you to hit a predefined quota (for example if you
   cant create more security groups or floating IP addresses), :doc:`contact our support </general/getting-support>`
   to raise the :doc:`quota </general/quotas>` on your account.

   If the installation fails, you are able to get the reason by clicking the service and checking error messages. 

..  seealso::

    - :doc:`index`
    - :doc:`prerequisites`
    - :doc:`configure-remote`
    - :doc:`advanced-configuration`


===============================================
Managing subnets in the cloud management portal
===============================================

Create a subnet
---------------

To create a :doc:`subnet <index>` from the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Networks** in the sidebar menu.

- On the network to which you want to add the subnet, press the **Create subnet** icon (rows of text with a small
  **+** plus sign).

- Name your subnet.

- Under **Network address** you need to input the address in CIDR notation (see the :doc:`/networking/subnet/subnet-format`
  article for more information). For example ``192.168.0.0/24``.

- Under **Gateway-IP** you have the option to either :doc:`set an IP as gateway <connect-subnet-to-router>` or
  not. If you want a gateway, select **Use standard gateway-IP**. If you don't, select **don't use a gateway**. The
  last option **enter gateway manually** will enable you to choose a specific IP as your gateway. If you are unsure, we
  recommend **Use standard gateway**.

- Select IPv4.

- If you want to use IPv4, we recommend using DHCP to assign IP addresses to hosts.

- Under **Add pool**, add the :doc:`DHCP scope <dhcp>`. That is, the starting and end addresses you want to auto-assign
  to hosts using DHCP. Press the **+** (plus) sign above **Add pool** and enter start and end addresses. For example
  ``192.168.0.10`` and ``192.168.0.254`` if your range is ``192.169.0.0/24``.

- Under **DNS name servers** we recommend adding our servers which are ``83.168.225.225`` and ``83.168.226.226``. You can
  press the **+** (plus) sign repeatedly to add more servers.

- Under **Add route**, you would add static routes (see: `../router/routing-between-subnets`). 

- Press **Create subnet**

Assign subnet to router
-----------------------

To :doc:`connect the subnet to a router <connect-subnet-to-router>`

- Press **Networking** and then **Networks** in the sidebar menu.

- Press the network you want to connect to your router.

- Under **Subnet** section, check what **Gateway-IP** you have set on the subnet you want
  to add to the router . This would normally be the first IP in the range. If there is not
  gateway IP, you need to set one first, using the **Edit** (small pencil icon) next to the
  subnets row. Save the Gateway-IP for upcoming steps.

- Press **Networking** and then **Routers** in the sidebar menu.

- Press the **Create interface** on the router you want to connect the subnet to. Its a small
  icon that looks like rows of text with a **+** (plus) sign.

- Select the subnet that you want to connect in the **Subnet** dropdown.

- Input the Gateway-IP from above step under **IP** field.

- Press **Create interface**

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


========================
Connect subnet to router
========================

For most situations, you would want to connect your subnet to your router so that you can
for example reach the internet (or your other networks) through it.

This means giving your router an interface on the network on which the subnet exists
and an IP address on the subnet, thereby enabling traffic routing to and from the subnet.

For instructions on how to connect a subnet to a router, a step-by-step guide is available
based on your choice of management platform:

- :doc:`cloud-management-portal`

- :doc:`openstack-horizon`

- :doc:`openstack-terminal-client`

.. tip::

   If the communication on the subnet is only local to hosts that connect to it (and does
   not need routing) there is no need (and indeed its not advised) to connect the subnet to
   a router.

   An example use-case where this might be true is an isolated backend network on which for
   example database connections or backups run.

   We generally recommend keeping your setup clean and consistent and not use multi-homing,
   that is connecting an instance to more than one subnet as troubleshooting your network
   is easier with a clean and structured network design.

   The reason for multi-homing is historically related to either capacity or security, none
   of which is a problem in a modern cloud platform.

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


====
DHCP
====

For assigning addresses to instances, Binero cloud uses the DHCP protocol. While its possible to do
static IP addresses in the platform, we highly recommend using it because it greatly simplifies adding
new instances (they will become available with zero configuration needed).

IP allocations on ports are permanent so there is no risk that an instance (as is the case traditionally
with DHCP - which was typically used for client computers) would suddenly get another IP address.

A DHCP scope (or pool) is part of a network range that's reserved for the DHCP server to deliver to its
clients. The scope is setup on a :doc:`subnet <index>` when creating it.

DHCP, other than assigning addresses, also assigns the correct :doc:`/networking/mtu` to instances. This is
important for networking functionality in general so if opting to not use DHCP, we recommend reading the
MTU article carefully to understand the MTU concept. 

.. tip::

   When assigning a DHCP scope (or pool), leave some room for potential statically assigned addresses and
   the gateway address.

   A typical recommendation would be to reserve the first 10-20 addresses in a subnet, depending on the subnet
   size. The remaining addresses would ideally go into the pool.

Routing via DHCP
----------------

Since DHCP provisions IP-configuration to instances, if there is a need to add :doc:`static routes <../router/static-routing>` to
instances (so called **Host routes**) for whatever reason, DHCP is a good way for this as any host routes that's added
to the subnet will get pushed to all instances on that subnet, existing and any new instances in the future.

DHCP will only add routes on instances, which is only usable to route to other instances on the same subnet. To route to a
destination outside the same subnet, the default route is normally used but a good use case for routing via DHCP is when a
VPN is setup on the same subnet that does not do :doc:`../router/nat`. See below for how to add routing to a DHCP pool.

Add a route using the cloud management portal
---------------------------------------------

To add a route by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Networking** and then **Networks** in the sidebar menu.

- Press the network for which you want to add the route.

- Press the edit icon (a small pencil) next to the subnet for which you want to add the route.

- Press the **+** (plus) sign above **Add host route**

- Under **Destination CIDR**, enter the destination network in
  :doc:`CIDR notation </networking/subnet/subnet-format>`.

- Under ``Nexthop (IP)`` add the gateway address. This address needs to be in the subnet you are editing.

- Press **Save subnet**.

Add a route using OpenStack Horizon
------------------------------------

To add a route by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Networks** in the sidebar menu.

- Press the name of the network for which you want to add the route.

- Press the **Subnets** tab and then **Edit subnet** button on the row of the subnet for which you
  want to add the route.

- Press the **Subnet details** button.

- Enter the route as ``destination subnet in `CIDR notation </networking/subnet/subnet-format>`,nexthop (gateway)``,
  for example ``192.168.10.0/24,192.168.1.5``. A single route entry on a single row.

- Press **Save**

Add a route using OpenStack terminal client
--------------------------------------------

To add a route by using the using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack subnet list``. Save the name of the subnet you want to add the route to.

- Run this command: ``openstack subnet set --host-route destination=[DESTINATION_SUBNET],gateway=[GATEWAY] [NAME_OF_SUBNET]``, replacing
  the items in angle brackets with the information from the previous steps and the destination network and gateway/nexthop. If you want
  to know more options, use ``-h`` at the end of the command.

..  seealso::

    - :doc:`../index`


=======
Subnets
=======

General concept
---------------

A subnet in Binero cloud is a :doc:`IP subnet <subnet-format>` on a :doc:`network <../network/index>`
that is local to you and isolated on a network and only reachable through a :doc:`router <../router/index>`
with the exception if you are using :doc:`directly attached IP addresses <../directly-attached-ips>`.

When creating a new instance, it's connected to a :doc:`network <../network/index>` and receives a IP address
from a subnet on that network.

When you create a subnet, you add it to a :doc:`network <../network/index>`.

This will allow you to:

- Use addresses from the subnet when creating instances, either by using :doc:`dhcp`
  (which we recommended) or by manually assigning a fixed IP address from the subnet.

- Use the subnet as a :doc:`gateway <connect-subnet-to-router>` for the network, by assigning
  it to a router.

You are also able to :doc:`route between networks <../router/routing-between-networks>`. This is useful
for creating security zones or for segmenting your infrastructure into different networks and subnets (for
example when having separated production and staging system).

Managing subnets
----------------

You are able to manage subnets by using either of the below tools.

- :doc:`The cloud management portal <cloud-management-portal>` is a good start and recommended to get a
  user with limited prior knowledge from A to B quickly. The tradeoff is that advanced features are not
  always available.

- :doc:`OpenStack Horizon <openstack-horizon>` is the web interface included in OpenStack. Some
  advanced features might only have a GUI implementation here.

- :doc:`OpenStack terminal client <openstack-terminal-client>` is a command line implementation
  giving terminal oriented users a quick way to access the cloud. The learning curve is steeper
  than the GUI implementation but the workflow will be efficient.

Choosing an IP range
--------------------

When IP addressing your network you would typically choose some IP range that's reserved for internal use
(presumably something from the `RFC1918 <https://en.wikipedia.org/wiki/Private_network>`__ range).

In Binero cloud you are able to select whichever range you prefer for your networking but we recommend
sticking to the ranges that's intended for internal use (according to above link) as you might otherwise
experience issues with your connectivity.

More information on defining an IP range for use in the cloud in the :doc:`subnet-format` article.

In the subsections to this article we show some ways to work with subnets by using the different portals.

.. tip::

   When selecting what IP-range to use, remember that you might want (at some point) to connect your
   infrastructure to some other network by using a site-to-site tunnel.

   We recommend that you choose a range that is less likely to be overlapping other subnets.

   The best way to do this, in our opinion, is to use a range from somewhere in the middle of
   172.16.0.0/12 (for example 172.29.45.0/24). 

.. tip::

   Take care to choose a subnet size (or mask) that has enough IP-space for your need.

   We recommend envisioning the largest amount of IP addresses you think you'll ever need and then
   at least doubling the amount of addresses.

   If you, for example, think you might have 20 servers then a single /24 (C-class) network with 255 addresses
   might be enough.

   If you think you might need to have 100 servers at some point (keeping in mind some servers will have
   more than one address) then aim for a subnet with a /23 mas that would give you 510 usable addresses.

.. toctree::
  :caption: Available services
  :maxdepth: 2

  cloud-management-portal
  openstack-horizon
  openstack-terminal-client
  subnet-format
  connect-subnet-to-router
  dhcp


========================================
Managing subnets using OpenStack Horizon
========================================

Create a subnet
---------------

To create a :doc:`subnet <index>` from the
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Network** and then **Networks** in the sidebar menu.

- Press the name of the network to which you want to add the subnet.

- Press the small dropdown next to **Edit network** in the top right corner. Select **Create subnet**

- Name your subnet.

- Under **Network Address** you need to input the address in CIDR notation, more
  information about this in the :doc:`subnet-format`. For example ``192.168.0.0/24``.

- Under **IP Version**, keep **IPv4**.

- Under **Gateway-IP** you have the option to either :doc:`set an IP as gateway <connect-subnet-to-router>`
  or not. 

- If you want a gateway, choose a gateway IP and enter it in the form (we recommend the
  first IP in the range, if its available) or just leave it blank to select the first
  available IP. 

  - If you don't, leave the field blank and select **Disable gateway** below.

- Press **Next**

- Under **Allocation pool**, add the :doc:`DHCP scope <dhcp>`. That is, the starting and end
  addresses you want to auto-assign to hosts using DHCP. Add each pool (we recommend just
  one pool per subnet) by adding the first address, (comma) last address (with no spaces), for
  example (if you would use the 192.168.0.0/24 range) ``192.168.0.20,192.168.0.254``.

- Under **DNS name servers** we recommend adding our servers which are ``83.168.225.225`` and
  ``83.168.226.226``, one per line, in the form. You can use any DNS servers you want though.

- Under **Host routes**, you would add static routes (more information in
  the :doc:`../router/routing-between-networks` article. 

- Press **Create**

Assign subnet to router
-----------------------

To :doc:`connect-subnet-to-router`

- Under **Project**, click **Network** and then **Routers** in the sidebar menu.

- Press the name of the router you want to connect the subnet to.

- Press the **Interfaces** tab on the top.

- Press **Add interface**

- Select the subnet in the **Subnet** dropdown.

- Leave the **IP address (optional)** field blank. This assumes the gateway address
  on the subnet is not in use somewhere (perhaps by some instance that is running
  as gateway). Otherwise, you need to choose the IP address.

- Press **Submit**

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


================================================
Managing subnets using OpenStack terminal client
================================================

Create a subnet
---------------

To create a :doc:`subnet <index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run: ``openstack network list --internal``, this will give you a list of
  available networks to add a subnet to. Note the name of the network.

- Decide what subnet range to use. More information in the :doc:`subnet-format`
  article.

- Decide on using DHCP (recommended) and what pool to use. More information in
  the :doc:`dhcp` article.

- Decide if you want to use a gateway or not. More information in
  the :doc:`connect-subnet-to-router` article. 

- Run: ``openstack subnet create --subnet-range=[RANGE] --gateway=auto --network=[NETWORK NAME] --dns-nameserver=83.168.225.225 --dns-nameserver=83.168.226.226 --allocation-pool start=[POOL START IP],end=[POOL END IP] [SUBNET NAME]``, replacing the items in brackets to the values identified in previous steps.

Assign subnet to router
-----------------------

To :doc:`connect to a router <connect-subnet-to-router>`

- Run: ``openstack router list`` to get the list of your available
  routers. Save the name of the router you want to connect the subnet to.

- Run: ``openstack subnet list`` to get the list of your available
  subnets. Save the name of the subnet you want to connect to the router.

- Run: ``openstack router add subnet [ROUTER NAME] [SUBNET NAME]``

.. note::

   In the above example, the router will for example use the default gateway IP. To have
   more granular control, try to add ``-h`` to the commands, you will then get the help
   function that will show you more options to work with. 

..  seealso::

    - :doc:`/networking/regions-and-availability-zones`
    - :doc:`/networking/router/index`


=============
Subnet Format
=============

A subnet is generally formatted in what we known as `CIDR notation <https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>`__.

A CIDR notation might for example look like this: ``192.168.0.0/24``

This is a way to describe both the network ip-number (which is the part before the ``/`` symbol) and the subnet size (which is
the part after the ``/`` symbol).

The network chosen in the above example would mean that the amount of IP-addresses available are between 192.168.0.1 and
192.168.0.255 (the first and last host address are typically reserved for network and broadcast). 

Below we outline how to select your subnet.

.. tip::

   If the below is something you do not want to understand but just want to know what subnet to choose, we recommend
   using /24 as mask size.

   Your first subnet then, might be for example 172.29.45.0/24, your next might be 172.29.46.0/24, and so on).

Explanation
-----------

After the ``/`` sign, the numeric value we refer to as subnet mask.

You could select anything between (or at least you would need to already understand subnet math if you
wanted anything larger or smaller) 18 to 27 as value for the mask.

If you chose 18 (``192.168.0.0/18``, according to above example), you will get 16382 IP addresses in the subnet. If you select 27
(``192.168.0.0/27``, according to above example), you would get 30 addresses.

This is because when the mask increments by one, the network size halves (which is why when we get to 27, counting down
from 18, we only have 30 addresses left to use from the original 16382 at ``/18``, bearing in mind reserved addresses). 

A standard size is ``/24``, this mask size yields 255 usable IP addresses and would give the subnet mask setting (in the operating
system, this is really the same as the prefix length, or mask, but in another format) ``255.255.255.0``.

In Binero cloud, the platform will calculate the subnet mask for you and is normally assigned using :doc:`dhcp`, so there is no need
to understand what subnet mask a certain CIDR prefix will have.

You just need to choose one that yields the right amount of usable IP addresses to suit your use-case. 

.. tip::

   When selecting the size, don't choose something that you might outgrow. Its a complicated task to re-number a network. Choose a subnet
   that will be large enough (and our recommendation is, double it, just to be sure).

.. tip::

   If you are still unsure after reading above, go with /24 as the subnet.

Advanced explanation
--------------------

Early IP address allocations given to providers in on of 3 address classes; **A**, **B** and **C**. An A-class
network (equal to a CIDR notation of ``/8``) has 16.777.214 usable addresses.

A B-class network has 65534 usable addresses. A C-class network has 255 usable addresses. This meant that
addresses where most often not optimally assigned.

An organisation that needed to have for example 75.000 IP addresses would instead get 16777214 (as a B-class network
would have been to small).

An organisation that needed 300 IP addresses would instead get 65534. This was not considered an issue in the early
days as the total IPv4 pool of 4,294,967,296 addresses seemed large at the time.

When the internet grew, it became clear that the total pool would get depleted fast.

To solve this, we introduced Classless Inter Domain Routing or `CIDR <https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing>`__, enabling
the use of a subnet *mask* that would make it possible to decide the size of a given network more granularly than in 3 stages.

The subnet mask showed how many addresses were available in the subnet based on a number that stated how big a
part of the **IP address** was the network bits compared to how big a part was the host bits.

An IPv4 address is a 32 bit number divided into 4 octets to make it easier to read. In the example ``192.168.0.0/24``,
the *mask* (``/24`` in this case) states that the 24 first bits (which is ``192.168.0``) is the network address.

This address is just used for describing the network and is unusable when addressing hosts in the network. Explained as
``32 / 4 = 8``, so 8 bits per octet, 24 bits then making the first 3 octets the network address (so ``192.168.0`` being
the first 3 octets).

The remaining (up to 32) bits are the hosts addresses so for example in the example ``192.168.0.5``, the last octet
(which is 5) is assignable to a host. 

When configuring an IP address in an operating system, some systems will want the IP-address in CIDR notation
(so ``192.168.0.5/24`` for example), some want the subnet mask specified separately, so ``192.168.0.5`` and ``255.255.255.0``

The function is the same, the subnet mask is just another way to tell that the first 24 bits (as shown by the fact
that 255 is the max size, so all 1s when writing in binary, of an 8 bit number is 255) are the network address and
the last 8 bits are the host address (the mask then is not the IP address, it just describes *what part* of the IP
address is the network bits what part is the host bits.

The previous network sizes (A, B and C) corresponds to /8, /16 and /24 in CIDR notation that uses even octets, if we
want to use an smaller subnet it's a bit more complicated.

For example, where one might want to have 3578 IP addresses , consulting a CIDR chart or using ``ipcalc`` software would
show that the closest network size that has more addresses then 3578 would be a /20 network that has 4094 addresses.

The principle is the same, the CIDR notation becomes ``192.168.0.0/20`` instead. The subnet mask would be ``255.255.240.0``
(because only the first 20 bits are the network address, giving some extra bits available for hosts).

In conclusion, if you have more advanced requirements with regards to wanting to use a specific subnet or needing a
specific subnet size, Binero cloud will be able to cater to your need.

.. tip::

   Included in most Linux distributions is the terminal based software ``ipcalc``. This will help you calculate subnets
   as is visible in the below example.

::

	$ ipcalc 192.168.0.0/20
	Address:   192.168.0.0          11000000.10101000.0000 0000.00000000
	Netmask:   255.255.240.0 = 20   11111111.11111111.1111 0000.00000000
	Wildcard:  0.0.15.255           00000000.00000000.0000 1111.11111111
	=>
	Network:   192.168.0.0/20       11000000.10101000.0000 0000.00000000
	HostMin:   192.168.0.1          11000000.10101000.0000 0000.00000001
	HostMax:   192.168.15.254       11000000.10101000.0000 1111.11111110
	Broadcast: 192.168.15.255       11000000.10101000.0000 1111.11111111
	Hosts/Net: 4094                  Class C, Private Internet


=======
Storage
=======

Storage is a core function in the Binero cloud platform.

You are able to provision both :doc:`block <persistent-block-storage/index>` and
:doc:`object <object-storage/index>` storage directly in the platform and
:doc:`file </service-catalog/file-share>` (using NFS) via our :doc:`../service-catalog/index`. 

We provide three main :doc:`storage types <storage-types>`.

- SSD (flash), IOPS and throughput optimized highly available storage

- HDD (mechanical), throughput and space optimized highly available storage

- NVMe (PCI-E attached high performance flash), performance optimized local storage.

You are able to use our :doc:`backup solution <../backup/index>` to backup the data of
volumes onto our object platform in another data center. 

See the subsections to this article to get a good understanding of the storage
possibilities in the platform.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  storage-types
  persistent-block-storage/index
  snapshots/index
  retype-a-volume
  object-storage/index
  nvme-storage
  regions-and-availability-zones


=============
Storage types
=============

By storage type, we refer to the media used to store data. Binero cloud has three main storage
types, namely SSD, HDD and NVMe.

You are able to :doc:`retype <retype-a-volume>` a volume later on if you are using either SSD or
HDD (to the other type).

SSD
---

SSD is the most versatile storage solution. It uses NAND cells to store data This technique vastly
shortens latency since seek time is a big issue with traditional drives where a mechanical read/write
head needs to physically move into position before being able to read or write data.

This means SSD media have great random read/write performance with lower latency than mechanical drives.

The downside is that SSD storage is more expensive than mechanical (HDD) drives and so storing large
amount of infrequently accessed data on SSD drives is not cost efficient. 

SSD backed storage is the default option, that is if you don't select what media to use, you will
end up on SSD media.

All SSD storage in Binero cloud is three-way replicated. That means that for each gigabyte of data that
you store in the platform, three times the data gets stored on disk. The reason behind this is data
integrity, we want to minimise the risk for data loss for our customers.

SSD is a good tradeoff between cost and performance for most use cases involving data that needs to
be widely accessible.

HDD
---

HDD is the most cost effective storage solution when only taking pure storage space into account.

This storage tier consists of traditional mechanical spinning drives which use magnetism on a metallic
surface to store data.

HDD drives suffer from seek times, that is they have a read/write mechanical head that needs to be physically
moved on the disk surface for operations.

When reading or writing data sequentially, meaning working with a large data set from start to finish
for example a large backup, HDD media still have good performance throughput but still less than SSD.

HDD backed storage is available when creating an :doc:`instance </compute/index>` or by choosing the
proper Storage policy when provisioning an object storage container or bucket.

When using HDD, storage is three-way replicated. When using HDD for :doc:`object storage <object-storage/index>`
it's also three-way replicated, except for the storage policy **gp.archive** (which uses erasure coding).

HDD is a good option when wanting to store large amounts of data, preferably infrequently used as with
use-cases for file servers, archive or backup solutions.

NVMe
----

NVMe is the fastest, highest performing storage in the platform. It's based on enterprise grade NAND-flash
cells such as our SSD type. The difference is the access, NVMe disks connect directly on the PCI-Express bus
on the hypervisor running your instance.

That means that the path from CPU to storage is much faster and you get access times more closer to memory. This
lowers the *latency* (the time from CPU requesting data until its delivered from storage) to microseconds where
other storage media gets measured in milliseconds.

Faster access to storage is particularly important when reading (or writing) random data. The downside is
that NVMe storage, since its physically located in a hypervisor node, is not redundantly setup.

NVMe based storage is only available in specific instance flavors and has some disadvantages that you need to
be aware of, read more in our :doc:`nvme-storage` article.

NVMe is a strong option for situations where you need fast access to disk, beyond what SSD can deliver.

..  seealso::

    - :doc:`nvme-storage`
    - :doc:`persistent-block-storage/index`
    - :doc:`object-storage/index`


============
NVMe storage
============

General concept
----------------

NVMe based storage is the fastest available persistent storage in the platform. This is due to:

- NVMe uses flash (SSD) cells connected close to the CPU of the computer (on
  the PCI Express bus directly - similar, but not identical, to RAM).

- NVMe storage in Binero cloud is local to the hypervisor, meaning networking is not used to reach
  the storage. 

- NVMe storage is **NOT** replicated, meaning there is no added latency when writing data.

These attributes combine to dramatically lower the latency from request to response (for both reads
and writes) to the storage layer.

NVMe storage measures in microseconds as opposed to other storage that is generally measured in
milliseconds. As latency is arguably the most important factor in storage performance, reducing it
will certainly have an impact on storage intensive applications where CPU will not have to idle while
waiting for IO as much.

How to use
----------

When provisioning an :doc:`instance </compute/index>` that is including NVMe in its name (for more
information, see our list of :doc:`flavors </compute/flavors>`), the platform will schedule
the instance onto hypervisors with local NVMe disks attached. 

With NVMe, you are only able to select between 50 GB or 250 GB disk sizes, the reason for this is because
NVMe disks by design are local and does not provide the same flexibility as our block storage service.

You are not able to grow a disk or :doc:`retype it <retype-a-volume>` either for the same reason. Because
of this, we recommend choosing an NVMe disk that fits your need. 

.. note::

   Should you need more NVMe storage than 250GB, :doc:`contact us </general/getting-support>` and we
   can discuss the available options.

Differences to standard storage
-------------------------------

Since NVMe-based storage is setup differently than our standard :doc:`SSD and HDD <storage-types>` storage
types, there are some caveats that a consumer of NVMe storage should be aware of: 

- NVMe storage is **NOT** redundantly setup. We deliver NVMe storage from the same hypervisor that runs the
  instance to provide the performance that NVMe disks does, they are not using RAID nor are they replicated. That
  said, they are high enterprise grade disks.

- NVMe storage cannot be live migrated. Live migration is a technique that our administrators use when doing system
  maintenance to evacuate parts of the platform to allow for maintenance with zero downtime. Since NVMe uses a local
  resource, its not possible to live migrate. Because of this, when doing maintenance, instances based on NVMe is 
  powered off for the duration of the maintenance. Please see our :doc:`/general/outages` article that details how to
  get noticed in case of planned maintenance. 

- NVMe-based storage cannot be :doc:`extended <persistent-block-storage/extend-volume>` through the platform, if
  you think you will ever need more than 50 GB we recommend going with 250 GB. If you need more than 250 GB,
  :doc:`contact us </general/getting-support>` to discuss a solution.

- NVMe-based storage cannot be :doc:`retyped <retype-a-volume>`. You can add one or more extra HDD or SSD volumes to
  the same instance and copy the data. Since the NVMe volume is the boot disk, we recommend doing a migration to
  another instance if you need to retype.

- Since we need to write images to the local NVMe disk on the hypervisor, it takes longer to provision an NVMe
  based instance.

- NVMe based storage is currently only available in the *europe-se-1a* :doc:`availability zone <regions-and-availability-zones>`.

- Because NVMe is generally used only when performance is critical to the application, we only have our High Performance
  :doc:`/compute/flavors` available with NVMe.

.. important::

   For above reasons, we strongly recommend you to regularly backup your data if you are using NVMe based storage.


========================
Object storage endpoints
========================

When using the object storage service in Binero cloud you connect to a HTTP based API endpoint to
any of the available APIs (S3 or Swift API).

This is how you interact with the object storage service by sending HTTP API requests to for example
get an object or put an object into the object storage.

We provide the object storage over four different HTTP based endpoints per :doc:`availability zone <../regions-and-availability-zones>`,
two per API (:doc:`S3 API <s3>` and :doc:`Swift API <swift>`), one is standalone (without replication)
and one is with replication, using replication will enable replication for your bucket between the availability
zones in the same region.

If you want to use :doc:`replication <replication>`, you would use the replicated endpoints. If you just want
to store objects in one availability zone or are unsure on what to choose, you would use the standalone endpoints.

europe-se-1a
^^^^^^^^^^^^

- S3 standalone: ``https://object-eu-se-1a.binero.cloud``

- S3 replicated: ``https://object-eu-se-1a-rep.binero.cloud``

- Swift standalone: ``https://object-eu-se-1a.binero.cloud/swift/v1/AUTH_[PROJECT_ID]``

- Swift replicated: ``https://object-eu-se-1a-rep.binero.cloud/swift/v1/AUTH_[PROJECT_ID]``

europe-se-1b
^^^^^^^^^^^^

- S3 standalone: ``https://object-eu-se-1b.binero.cloud``

- S3 replicated: ``https://object-eu-se-1b-rep.binero.cloud``

- Swift standalone: ``https://object-eu-se-1b.binero.cloud/swift/v1/AUTH_[PROJECT_ID]``

- Swift replicated: ``https://object-eu-se-1b-rep.binero.cloud/swift/v1/AUTH_[PROJECT_ID]``

..  seealso::

    - :doc:`index`
    - :doc:`s3`
    - :doc:`swift`


===================================
Getting started with object storage
===================================

Preparation
-----------

Before getting started, you will need to choose if you want to :doc:`replicate <replication>` your
storage or not.

If you are unsure about replication, we recommend not replicating. Once you know this, you are able to select an
:doc:`endpoint <endpoints>`, which would also depend on what :doc:`availability zone <../regions-and-availability-zones>`
you want to run your requests from.

If you are unsure what availability zone to use, select **europe-se-1a** as endpoint. The combination
of replication (or not) and availability zone will result in an endpoint URL according to our
:doc:`endpoint <endpoints>` article, you will use this endpoint to reach the proper storage backend.

Initial setup
-------------

To get started with storing objects (either S3 or Swift), you create a container or bucket to store the objects in.

You can create container or bucket by using either S3 or Swift, the concept is the same and both APIs can manage
all containers or buckets.

You can use the two GUI portals (the cloud management portal or OpenStack Horizon) to create the container or bucket
(they will use the Swift API), *they are only able to create un-replicated containers or buckets in availability zone
europe-se-1a* (since you cannot specify which endpoint to access).

Because of this, we recommend using the protocols own command line to create the container or bucket:

- :doc:`s3`

- :doc:`swift` 

Above said, its possible to use the following methods to setup a new container or bucket without replication in the
availability zone europe-se-1a if this meets your requirements. Below methods use the Swift API.

.. note::

   By using any of the below methods, you will create a container or bucket without replication in the
   europe-se-1a availability zone.

.. tip::

   We enforce a default quota of 1000 buckets or container in the object storage, contact our support if
   you need more.

Cloud management portal
-----------------------

To create a container by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Storage** and then **Object storage** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Name your container.

- Select your :doc:`storage policy <storage-policy>`.

.. caution::

   Setting your container as public means anybody with a correct URL can access your
   data without any authentication!

- If you check **public**, users will not need any credentials reach the container (good for when
  using the object storage as a CDN for example).

- Press **Create**

You are now able to upload objects by using the portal, by pressing the container icon and then pressing **Upload file**
or **Create folder**. You are also able to use the container via external access, see each protocol sub-article
for more information.

OpenStack Horizon
-----------------

To create a container by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Object Store** and then **Containers** in the sidebar menu.

- Click **+ Container**

- Name your container.

- Select your :doc:`storage policy <storage-policy>`.

.. caution::

   Setting your container as public means anybody with a correct URL can access your
   data without any authentication!

- If you select **public** under **Container Access**, users will not any credentials
  to reach the container (good for when using the object storage as a CDN for example). 

- Press **Submit**

You are now able to upload objects by using the portal, by pressing the container name and then pressing
the upload icon (looks like a small upwards facing arrow) or **+ Folder** to create a folder.

You are also able to use the container via external access, see each protocol sub-article for more
information.

OpenStack Terminal Client
-------------------------

To create a container by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Decide which :doc:`storage policy <storage-policy>` you want to use.

- Run this command: ``openstack container create --storage-policy [STORAGE_POLICY_NAME] [CONTAINER_NAME]``, replacing
  the values in angle brackets.

.. note::

   The above methods will create an container without replication in availability zone europe-se-1a.


==============
Object storage
==============

Object storage is a good way to store unstructured data. Unlike a file system where you would have a structured
tree of files and directories or block storage upon which you can use a file system, object storage stores arbitrary
objects.

Each object consists of the data stored and a varying amount of metadata, that is, information about the stored object.

The :doc:`backup service </backup/index>` in Binero cloud uses our object storage to save the backup data.

In Binero cloud you are able to consume the object storage service by using both the :doc:`S3 API <s3>` and the
:doc:`Swift API <swift>`, both from the same underlying storage platform and can reach the same data but use
different APIs.

.. note::

   In the documentation we can refer to either a container or bucket both being the same thing, a container holding
   objects, where bucket is S3 terminology and container is Swift terminology.

.. tip::

   We enforce a default quota of 1000 buckets or containers in the object storage, contact our
   support if you need more.

You have the ability to choose from different :doc:`storage policies <storage-policy>` of object storage to setup a
storage solution that delivers at the lowest possible cost per GB while still meeting your requirements. You cannot
change storage policy on an existing bucket/container.

We also enable :doc:`replication <replication>` if you want to have your data available in more than one
availability zone for example as part of a multi data center high availability solution.

We recommend reading through our :doc:`getting-started` guide to get starting using our
object storage service.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  getting-started
  endpoints
  s3
  swift
  storage-policy
  replication
  versioning
  object-locking
  object-encryption
  known-limitations


=================
S3 object storage
=================

AWS S3 is an object storage service provided by Amazon that exposes a HTTP based API known as the
S3 API. Binero cloud provides it's own object storage service that supports this HTTP based S3 API.

You are able to consume our object storage by using the S3 API from any of our
:doc:`../regions-and-availability-zones`, either from one availability zone
at a time or both using :doc:`replication <replication>`.

In S3 terminology a bucket holds objects and is the same as a
container in Swift terminology.

.. note::

   The complete list of S3 features is available `here <https://docs.ceph.com/en/latest/radosgw/s3/>`_

.. note::

   See :doc:`known limitations <known-limitations>` for more information on compatibility and
   interoperability between the :doc:`S3 <s3>` and :doc:`Swift <swift>` APIs.

Setting up credentials
----------------------

To access the S3 service you need to create a EC2 credential, this credential consists of an access
key and a secret key for your :ref:`API user <api-users-label>` and can impersonate that user by using
the credential.

.. important::

   Sharing the EC2 credential will allow that entity to impersonate your :ref:`API user <api-users-label>`
   and gives access to the cloud platform and not only the S3 service.

You can only create an :ref:`EC2 credential <ec2-credential-label>` using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client` by running the
``openstack ec2 credentials create`` command which creates a new EC2 credential for your API user.

You can list existing EC2 credentials for your API user by using ``openstack credential list --type ec2``.

When you create a new EC2 credential save the access and secret key that is the credential itself.

You can read more about :ref:`ec2-credential-label` in our :doc:`/getting-started/users` documentation.

S3 client
---------

S3 is an HTTP based API protocol, meaning to administer it you would use HTTP API requests.

To simplify the management of S3 in the cloud, we recommend installing the official client from AWS, its
available `here <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html>`__.

Once its installed, you should be able to run ``aws --version`` and get an output.

Configure your client with your credentials (see the section above) by creating the file
(on a Linux or MacOS based computer) ``~/.aws/credentials`` as such:

:: 

	[default]
	aws_access_key_id=[ACCESS_KEY]
	aws_secret_access_key=[SECRET_KEY]

When you've completed the configuration, you are now able to test reaching the cloud by for example running
this command: ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api list-buckets`` which will list the
buckets in the account from the non replicated endpoint of availability zone europe-se-1a.

If you have no buckets setup, it will return an empty ``Buckets`` array.

Creating a bucket
-----------------

To create a bucket via S3, you would either use the API (not covered in this documentation) or the S3
client. To use the latter to create a bucket:

.. note::

   You cannot change storage policy on the bucket after creation.

- Decide which :doc:`storage policy <storage-policy>` you want to use. Save the name.

- Decide if you need to use :doc:`replication <replication>` or not.

- Decide in what :doc:`availability zone <../regions-and-availability-zones>` to store
  your data, save the name.

- Based on replication (or not) and availability zone, choose the right
  :doc:`endpoint <endpoints>`. Save the endpoint URL.

- Based on replication (or not) the ``LocationConstraint`` will be either ``europe-se-1`` or
  ``europe-se-1-rep``, save the one that is right for your use-case.

- Run this command:

  ::

    aws --endpoint=[ENDPOINT_URL] s3api create-bucket --bucket [BUCKET_NAME] --create-bucket-configuration LocationConstraint=[LOCAL_CONSTRAINT]:[STORAGE_POLICY_NAME]``

Replacing the items in angle brackets with the proper data from previous steps. The storage policy is optional
and will use the default if not specified.

- Verify by running this command: ``aws --endpoint=[ENDPOINT_URL] s3api list-buckets``

You are now able to use your bucket to save data in using your credentials from your application.

Deleting a bucket
-----------------

Delete a bucket by using the ``aws`` terminal client.

- Run this command: ``aws --endpoint=[ENDPOINT_URL] s3api list-buckets``, save the name of the bucket
  you want to delete.

- Run this command: ``aws --endpoint=[ENDPOINT_URL] s3api delete-bucket --bucket [BUCKET_NAME]``, replace
  [BUCKET_NAME] with the name of the bucket.

.. note::

   The delete will fail unless the bucket is empty.


=================
Known limitations
=================

This documentation tracks any compatibility issues that the :doc:`object storage <index>` service in Binero cloud
exhibits compared to other implementations and interoperability between the :doc:`S3 <s3>` and :doc:`Swift <swift>` APIs.

Browser Uploads and Chunked Uploads in S3 API
---------------------------------------------

The authentication backend used for Binero cloud does not support all details from the AWSv4 specification and
features such as Browser Uploads and Chunked Uploads in the :doc:`S3 API <s3>` is not unsupported.

This is normally not an issue but we've had customers utilizing the official AWS C# SDK encountering the issue
with Chunked Uploads because ``UseChunkEncoding`` in the ``PutObjectRequest`` methods defaults to true, this needs
to be false for it to work.

Using the Swift API to download a S3 API multipart upload
---------------------------------------------------------

You can only upload objects of a max size up to 5 GB for a single upload by design and to upload a larger file than that
you must use the multipart upload features in for example the :doc:`S3 API <s3>`.

This gets handled by the client or library that you use and is not something that you need to thing about, this splits
your file into parts and then uploads them concurrently, the parts is then assembled in the platform.

If you upload a large file by using the :doc:`S3 API <s3>` and then try to download that using the :doc:`Swift API <swift>`
you will get an error such as ``Error downloading [object-name]: md5sum != etag`` indicating the operation failed, but the
object is successfully downloaded. We recommend that you select one API to use for your application and not combining
the usage unless absolutely neccesary.


=================
Object encryption
=================

Object encryption concept
-------------------------

.. note::

   Object encryption is a :doc:`S3 API <s3>` feature and will not work the same when using
   the :doc:`Swift API <swift>`.

.. note::

   If you retrieve the object by using the :doc:`Swift API <swift>` you will still receive the
   encrypted data as the decryption only happens when using the :doc:`S3 API <s3>`.

.. note::

   When using any of our portals to work with the object storage it uses the :doc:`Swift API <swift>`
   and will thus not decrypt the object upon retrieval.

.. important::

   Do not combine replication with object encryption as this could result in data loss when updating
   objects from both ends.

The object storage service in Binero cloud supports encryption your data at rest (on disk) using different
methods. When retrieving or writing objects they are transparently encrypted or decrypted by the platform.

When talking to the object storage service the traffic and thus objects gets secured in-transit by using HTTPS
with TLS to transport the requests.

The examples in below methods uses the ``aws`` CLI, to get started see our :doc:`S3 documentation <s3>`.

Using server-side encryption with SSE-C
---------------------------------------

The object storage service in Binero cloud supports the customer-provided keys (SSE-C) specification
in the :doc:`S3 API <s3>`.

When using this method you are responsible for sending an encryption key for the object in each API requests to
retrieve or write to it. This needs to be an encryption key that works with AES-256.

The data gets stored at rest (on disk) with your encryption key and the encryption key is not saved by Binero. This
way you don't have to handle the encryption or decryption of objects and only manage the encryption key sent.

To get started with using SSE-C, see below:

- Create a bucket to test with ``aws s3 mb s3://demo``

- Create a random encryption key to use ``openssl rand 32 -out ssec.key``

- Upload a object to the bucket and encrypt it with ``aws s3 cp text.txt s3://demo/text.txt --sse-c AES256 --sse-c-key fileb://ssec.key``

- Download the object that we just uploaded with ``aws s3 cp s3://demo/test.txt test-download.txt --sse-c AES256 --sse-c-key fileb://ssec.key``

Using server-side encryption with SSE-KMS
-----------------------------------------

.. important::

   If you delete secret in the :doc:`secret store </secret-store/index>` service there is no way to recover
   the encrypted objects. Make sure to backup the secret and the data.

The object storage service in Binero cloud also supports the SSE-KMS specification in the :doc:`S3 API <s3>`.

When using this method you create an encryption key and store it in our :doc:`secret store </secret-store/index>` service
and send the secret ID with each API request. This needs to be an encryption key that works with AES-256.

The data gets stored at rest (on disk) with your encryption key that is using the :doc:`secret store </secret-store/index>`
service.

To get started with using SSE-KMS, see below:

.. note::

   The ``SECRET_HREF`` mentioned below is for example ``https://api-eu-se-1.binero.cloud:9311/v1/secrets/ea7454d8-d0af-4008-bba4-71245b942bb7`` but
   for your secret and in that case ``SECRET_HREF_ID`` is the UUID ``ea7454d8-d0af-4008-bba4-71245b942bb7`` from that URL

- Create a bucket to test with ``aws s3 mb s3://demo``

- Create a random encryption key that you can store using ``openssl rand -base64 32`` and save the key.

- :doc:`Create a secret </secret-store/create-secret>` in our secret store and save the returned ``Secret href`` value.

- To allow the platform access to your secret (so that it can handle encryption and decryption with the key) you need to add a
  :doc:`ACL </secret-store/acl>` using the :doc:`openstack CLI </getting-started/managing-your-cloud/openstack-terminal-client>`
  with command ``openstack acl user add --user 23646ed0e7d240ceb56eef6ec909c2ff [SECRET_HREF]``

- Upload a object to the bucket with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3 cp test.txt s3://demo/test.txt --sse=aws:kms --sse-kms-key-id [SECRET_HREF_ID]``

..  seealso::

  - :doc:`s3`
  - :doc:`swift`


==============
Object locking
==============

.. note::

   :doc:`Versioning <versioning>` of objects are implicitly enabled when using object locking.

.. note::

   Locking an object enforces on the version of the object, deleting an object is still possible but will
   cause a DeleteMarker thus preserving your data even on deletion. See :doc:`/storage/object-storage/versioning`.

Object locking concept
----------------------

Object storage is often used in use-cases where objects gets stored once and read many times. The objects saved
are seldom, if ever, updated but should remain available and protected against accidental or malicious deletions.

To guarantee that an object is not accidentally or intentionally deleted, enable locking. When setting up
locking, it becomes impossible, even for a cloud operator, to delete the object.

Combined with the high availability and data durability of the object storage platform, this adds an extra
layer of data security in the platform.

.. note::

   Object locking can be setup for a certain time. Remember that depending on the mode, you cannot delete the objects
   until the time passes, meaning you will need to pay for the storage of the object until the locking has expired.

Using object locking
--------------------

To create a bucket with support for locking.

.. note::

   Below guide will use the s3 object storage and the `aws` client. We suggest you read
   :doc:`our guide on the same </storage/object-storage/s3>` before proceeding. 

- Run this command to create the bucket: ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api create-bucket --bucket [BUCKET_NAME] --create-bucket-configuration LocationConstraint=europe-se-1:gp.recurring --object-lock-enabled-for-bucket``

- Select either **COMPLIANCE** or **GOVERNANCE** as retention mode. COMPLIANCE will enforce the settings for all users
  (also administrators), GOVERNANCE will allow users with privileges to remove objects or change the retention days.

- Run this command to setup locking on the bucket: ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api put-object-lock-configuration --bucket [BUCKET_NAME] --object-lock-configuration '{ "ObjectLockEnabled": "Enabled", "Rule": { "DefaultRetention": { "Mode": "COMPLIANCE", "Days": 1 }}}'``

Objects are now protected according to the policy and days chosen. See the :doc:`versioning <versioning>` documentation on
how it's enforced with versions.

..  seealso::

  - :doc:`/storage/object-storage/s3`
  - :doc:`/storage/object-storage/versioning`


=========================
Replicated object storage
=========================

For use-cases where you would either want you storage available in case of a disaster, or more commonly, design
your application to be more resilient, making sure your object storage is available from more than one
availability zone.

Please see our :doc:`../regions-and-availability-zones` for information on where we provide object storage.

Binero cloud supports object storage replication. This works for both S3 and Swift APIs. 

The replication method used is *eventually consistent*. This means that objects will replicate in the background once
uploaded to the storage by using the proper *endpoint*.

This replication process follows an eventually consistent model and ensures that your data replicates within 15 minutes
but can take longer depending on the size of the object uploaded.

Because of this, a new object will not be immediately available in both availability zones, but rather on the site where
you uploaded it, and within a short time once replicated, on the other.

.. note::

   When using replication, always connect to the API endpoint that is local to you - replication will manage the rest.

.. important::

   Do not combine replication with object encryption as this could result in data loss when updating objects from both ends. 

Endpoints
---------

You enable using replication by connecting to a replicated endpoint. Please see :doc:`endpoints` for information on what
our endpoints for replicated storage are.

Make sure to choose the replicated endpoint local to the site that you are running your requests from (or **europe-se-1a** if
they are not from one of our sites). If you put objects in buckets or containers that's created on either of these APIs, the
objects will replicate.

.. important::

   The portals (for example the cloud management portal) in the platform only connect to the **standalone** object storage
   in availability zone **europe-se-1a**. As such, if you want to create a replicated bucket or container, you will need to
   use a CLI tool.

Storage policy
--------------

The storage policies that are available when using replicated object storage is the policies that is
most suitable as a primary storage.

The following storage policies is available when using replication:

- ``hp.intensive``

- ``gp.recurring``

..  seealso::

    - :doc:`index`
    - :doc:`s3`
    - :doc:`swift`


==============
Storage policy
==============

Binero cloud provides different storage policies for you to place your container or bucket on.

A storage policy is using an underlying :doc:`storage type <../storage-types>` and
other features to accommodate different use-cases. You cannot change storage policy
on an existing bucket/container.

The intent is to allow users to select on a more granular level on what they need for
their use-case such as:

- Size of the object(s)

- Risk of data loss

- Access frequency on object(s)

- Write frequency on object(s)

- Latency

- Price point

.. note::

   The default storage policy on a container or bucket is ``gp.recurring`` unless other is
   specified upon creation.

.. note::

   The storage policies available when using replication might differ,
   see :doc:`replication <replication>`

The storage policies available are:

- ``gp.archive``

- ``gp.intermittent``

- ``gp.recurring``

- ``hp.intensive``

.. tip::

   Plan the storage policy you use based on your use-case as this will have an impact on
   your performance and also on your cost and retrieval fees.

The different storage policies have different intended use-cases as presented in detail below.

Their price points for usage and retrieval is available in our `price list <https://binero.com/public-cloud-platform/publikt-moln/pris/>`__
where the pricing is different for each storage policy.

The pricing for usage is the combined size in gigabytes (GB) of objects you store in
all your containers or buckets.

The pricing for retrieval per gigabyte (GB) is also applied on some storage policies, this cost
is the bytes sent to you when for example downloading objects or listing objects in a container or
bucket, this is to keep a low price point for data set that require massive amount of usage but
infrequent access.

gp.archive
----------

This storage policy is general purpose (gp) and its intended use-case is archiving, that
is a long term storage for cold data, for example backups (our :doc:`backup service</backup/index>` use
this policy). 

To keep the price down, this solution uses erasure coding which is a way to store data redundantly
but using minimal space.

This method is somewhat less secure from a data integrity standpoint than replication (which all the other
storage policies use) but can in turn provide a lower price point per GB. The :doc:`storage media <../storage-types>` is HDD.

gp.intermittent
---------------

This storage policy is general purpose and its intended use-case is storing data that is seldom
requested but that requires a higher security than erasure coding. 

This solution uses three-way replication (meaning each object gets stored three times on different disks
in different servers). The :doc:`storage media <../storage-types>` is HDD.

gp.recurring
------------

This storage policy is general purpose and its intended use-case is storing large objects that are
frequently requested (for example video or audio material for a website). 

This solution uses three-way replication (meaning each object gets stored three times on different disks
in different servers). The :doc:`storage media <../storage-types>` is HDD.

hp.intensive
------------

This storage policy is high performance and its intended use-case is storing objects that are frequently
requested and when user experience (for example latency and speed) is important. 

This solution uses three-way replication (meaning each object gets stored three times on different disks
in different servers). The :doc:`storage media <../storage-types>` is SSD.

..  seealso::

    - :doc:`index`
    - :doc:`s3`
    - :doc:`swift`


====================
Swift object storage
====================

Swift is the object storage service in the OpenStack ecosystem exposing a HTTP based API
knows as Swift API.

Binero cloud provides it's own object storage service that supports this HTTP based
Swift API.

The Swift API is fully supported within the platform and you are able to run it from any
of our :doc:`../regions-and-availability-zones`, either from one availability zone at a
time or both using :doc:`replication <replication>`.

In Swift terminology a container holds objects and is the same as a
bucket in S3 terminology.

.. tip::

   The Swift implementation does not have as broad support as the :doc:`S3 API <s3>` among
   applications and libraries. If you don't need the Swift API specifically we recommend
   going with the :doc:`S3 API <s3>`.

.. note::

   See :doc:`known limitations <known-limitations>` for more information on compatibility
   and interoperability between the :doc:`S3 <s3>` and :doc:`Swift <swift>` APIs.

Setting up credentials
----------------------

You are able to use the Swift API by using any API-user and setting up application specific
credentials performed in the same manner.

See our :doc:`/getting-started/users` article for more information.

Swift client
------------

While you can use the :doc:`/getting-started/managing-your-cloud/cloud-management-portal` or
:doc:`/getting-started/managing-your-cloud/openstack-horizon` to create and manage containers 
see :doc:`getting-started`), if you want to manage swift storage in another
:doc:`availability zone </storage/regions-and-availability-zones>` than *europe-se-1a* (which is
the only one these tools can access), you will need the swift client. 

If you read our guide :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`, chances
are that you've already installed the client, you can test this by running ``swift --version``.

If not, its available via your python package manager (or your operating system package manager) as
``python-swiftclient``.

To use the client, provided you are also using the OpenStack terminal client, you should be able to
re-use the same environmental variables as it does.

That means if you are able to run for example ``openstack server list`` in a terminal, you should also
be able to run ``swift list`` (which will show your buckets in availability zone europe-se-1a) in the same
terminal.

Please see the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client` article for more
information.

Client endpoints
^^^^^^^^^^^^^^^^

When you want to use the client with one of the non-default endpoints - that is when you want to use
replicated storage or when you want to use another availability zone, you would use the ``--os-storage-url``
flag with the new endpoint: 

- Run this command: ``openstack project list``, save the ID of the project. 

- Run this command: ``swift --os-storage-url https://object-eu-se-1b.binero.cloud/swift/v1/AUTH_[PROJECT_ID] list``, replacing
  the [PROJECT_ID]with the ID from previous step and using the URL from the :doc:`endpoints` article to reach
  the storage.

.. note::

   Without the ``--os-storage-url`` all requests will go to the non-replicated storage in
   availability zone **europe-se-1a**.

Creating a container
--------------------

To create a container via Swift, you would either use the API (not covered in this documentation) or the swift
client.

To use the latter to create a bucket see below. Add the ``--os-storage-url`` as per above
if you need to change API endpoint.

.. note::

   You cannot change storage policy on the bucket after creation.

- Decide which :doc:`storage policy <storage-policy>` you want to use. Save the name.

- Decide if you need to use :doc:`replication <replication>`.

- Decide in what :doc:`availability zone <../regions-and-availability-zones>` to store your
  data, save the name.

- Based on replication (or not) and availability zone, choose the right :doc:`endpoint <endpoints>`. Save
  the endpoint URL.

- Create your container using ``swift post -H "X-Storage-Policy:[STORAGE_POLICY_NAME]" [CONTAINER_NAME]``, add
  the correct information in the command, you can skip specifying a :doc:`storage policy <storage-policy>` in
  which case it will use the default.

- Verify by running the following command ``swift stat [CONTAINER_NAME]``

.. note::

   Please read the section above about client :doc:`endpoints <endpoints>` and add the ``--os-storage-url`` flag
   should you need to use something other than non-replicated storage in availability zone *europe-se-1a*.

Deleting a container
--------------------

.. caution::

   Everything in the bucket will get deleted when you run the below!

To delete a container by using the ``swift`` terminal client.

- Run this command: ``swift list``, save the name of the bucket you want to delete.

- Run this command: ``swift delete [BUCKET_NAME]``, replace [BUCKET_NAME] with the
  name of the bucket. 


==========
Versioning
==========

.. note::

   Versioning is a :doc:`S3 API <s3>` feature and is also called Bucket Versioning, the 
   data protection it gives when enabled is still enforced when using the :doc:`Swift API <swift>`.

Versioning concept
------------------

Versioning is a means to keep many variants of an object in the same bucket. You can use it to store many versions of an
object to preserve, retrieve and restore every version of objects stored in your bucket.

This means that you can get you data back from accidental user actions such as a deleted object or application failures.

If you delete an object instead of permanently deleting the object it sets a DeleteMarker on the object. If you overwrite an object
instead of overwriting it a new version of the object gets stored and the original preserved as it's own version.

If you list the objects in a bucket it can still says the bucket is empty even though there are objects still stored in the
bucket since there might be older versioning of an object that is not displayed due to DeleteMarkers.

A bucket cannot be permanently deleted before deleting all objects, with versioning that also means deleting all versions of
all objects.

Versioning is not enabled by default on buckets and thus needs to activated.
          
Using versioning
----------------

This guide uses the :doc:`S3 API <s3>` by using the ``aws`` CLI client. See our :doc:`S3 documentation <s3>` to get started.

.. warning::

   When enabling versioning on a bucket you cannot turned off again, but you can suspended it on the bucket.

.. note::

   Enabling versioning on a bucket only affects new objects after you have enabled it.

- Create a new empty bucket ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api create-bucket --bucket versioning-demo``

- To check versioning configuration on a bucket use ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api get-bucket-versioning --bucket versioning-demo``, this
  returns nothing for this new empty bucket.

- If we upload a ``test.txt`` object to the bucket now, it won't get any versioning and the delete of that object will be permanent.

  - Upload a ``text.txt`` object from your current directory ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3 cp test.txt s3://versioning-demo/test.txt``

  - Delete the ``test.txt`` object ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3 rm s3://versioning-demo/test.txt``

  - Check that no versioning of the bucket exists ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api list-object-versions --bucket versioning-demo``, this returns nothing.

Now let's enable versioning on the bucket and do the same thing.

- Enable bucket versioning with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api put-bucket-versioning --bucket versioning-demo --versioning-configuration Status=Enabled``

- Upload then delete the ``text.txt`` object the same way as above.

- Now checking again with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api list-object-versions --bucket versioning-demo`` you can see that the
  ``test.txt`` object is still stored in your bucket with a DeleteMarker but still not listed in ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3 ls s3://versioning-demo``

- You can restore the ``test.txt`` by deleting the DeleteMarker, from the above output take the VersionId of the DeleteMarker (not the VersionId of the object version but of the DeleteMarker)
  using ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api delete-object --bucket versioning-demo --key test.txt --version-id [DeleteMarker VersionId]``

- If you now see the ``test.txt`` object again when listing objects with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3 ls s3://versioning-demo``

- To permanently delete the ``test.txt`` object delete all versions, take the VersionId of the object version and delete with
  ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api delete-object --bucket example-bucket --key test.txt --version-id [object VersionId]``

- You can verify it's permanently gone with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api list-object-versions --bucket versioning-demo``, this now returns nothing again.

Please note that an object can have many versions if it's overwritten, so to delete it you would need to list all object
versions and delete them.

Suspending versioning
---------------------

.. note::

   Suspending versioning does not affect objects that already has versioning, it will only suspend versioning on new objects.

If you set versioning to ``Enabled`` on a bucket you can never set it back to ``Disabled`` again but you can set it to ``Suspended``.

To set versioning to Suspended run ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api put-bucket-versioning --bucket versioning-demo --versioning-configuration Status=Suspended``

Swift API and versioning
------------------------

You cannot handle bucket versioning using the :doc:`Swift API <swift>` but the functionality for versioning and DeleteMarkers still apply.

If you upload an object with ``swift upload versioning-demo test.txt`` and then for example delete it with ``swift delete versioning-demo test.txt`` a
DeleteMarker is created and you can view it with ``aws --endpoint=https://object-eu-se-1a.binero.cloud s3api list-object-versions --bucket versioning-demo``.

..  seealso::
  - :doc:`/storage/object-storage/s3`
  - :doc:`/storage/object-storage/object-locking`


============================
Creating a persistent volume
============================

If you just want a single disk attached to your instance, this will get created with the instance
and does not need to be separately created. If you want more disks, then you need to create and
attach more volumes to the instance. 

New volumes can either be empty or based on:

- A previous :doc:`image </images/index>`, public or private.

- A current volume (only in the cloud management portal).

- :doc:`A snapshot <../snapshots/create-volume-from-snapshot>` (only in OpenStack Horizon or via
  OpenStack terminal client).

If your intention was to create an instance from a previous volume, follow the steps in our
guide :doc:`creating-an-instance-from-a-volume`.

.. note::

   New volumes created and attached to an instance might not get deleted with the
   instance.

.. note::

   Volumes cost according to our price list even when they are not attached to
   an instance.

Cloud management portal
-----------------------

To create a volume by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Storage** and then **Volumes** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Region should be *europe-se-1*.

- :doc:`availability zone <../regions-and-availability-zones>` should be the same as the one your
  instance is running in. Choosing default means *europe-se-1a*.

- Name your volume and optionally give it a description.

- Choose what to use as base for your volume (see above).

- Choose the size of the volume in GB.

- Choose which :doc:`volume type <../storage-types>` you want for your volume.	

- Press **Create**

To attach your volume:

- Press **Compute** and then **Instances**

- Press the instance you want to connect the volume to.

- Press the **Volumes** tab. It might require scrolling the tabs to the right.

- Press **Connect volume**

- Select your volume in the list and press **Attach**

OpenStack Horizon
-----------------

To create a volume by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Volumes** and then **Volumes** in the sidebar menu.

- Click **+ Create volume** in the right upper corner.

- Name your volume and optionally give it a description.

- Choose what to use as source for your volume (see above).

- Choose which :doc:`volume type <../storage-types>` you want for your volume.

- Choose the size of the volume in GB.

- Choose :doc:`availability zone <../regions-and-availability-zones>`, it should be the same
  as the one the instance you want to connect the volume to is running in. Choosing default
  means *europe-se-1a*.

- Press **Create volume**

To attach your volume:

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- Click the name of the instance to which you want to attach the volume.

- Press the small dropdown to the far right on the row of the instance you want to add the
  volume to and choose **Attach volume**

- Select the volume you created and press **Attach volume**

OpenStack Terminal Client
-------------------------

To create a volume by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume type list``. Save the name of the volume type you
  want to use. 

- Run this command: ``openstack availability zone list --volume``. Save the name of the
  availability zone you want to use.

- Run this command: ``openstack volume create --type [VOLUME_TYPE_NAME] --size [SIZE_IN_GB] --availability-zone [AVAILABILITY_ZONE] [VOLUME_NAME]``, replacing
  the values in angle brackets with values from previous steps and your selections.

To attach your volume:

- Run this command: ``openstack volume list``, save the ID (not name) of the volume you
  want to attach.

- Run this command: ``openstack server list``, save the ID (not name) of the instance to
  which you want to attach the volume.

- Run this command: ``openstack server add volume [INSTANCE_ID] [VOLUME_ID]``


============================================
Creating an instance from an existing volume
============================================

If you have a bootable volume available you can launch a new instance
using the volume.

Cloud management portal
-----------------------

To launch a new instance from an existing volume by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Follow our guide :doc:`/compute/launching-an-instance/cloud-management-portal`, but:

  - When selecting boot source, press the **Volumes** tab.

  - Select your volume.

  - Proceed according to the guide.

OpenStack Horizon
-----------------

To launch a new instance from an existing volume by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Follow our guide :doc:`/compute/launching-an-instance/openstack-horizon`, but:

  - On the second tab **Source**, in the **Select boot source** dropdown, select **Volume**

  - Press the small arrow next to the volume you want to base the instance on (available below).

  - Proceed according to the guide.

OpenStack Terminal Client
-------------------------

To launch a new instance from an existing volume by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Follow our guide :doc:`/compute/launching-an-instance/openstack-horizon`, but:

  - Run this command: ``openstack volume list --status available``, save the ID of the volume to use.

  - Replace the server create command in the guide with the below, the **Volume ID** you get from previous
    command. You can also skip the steps about which image to use as when using an existing volume, an
    image is not used.

::

     openstack server create \
     --flavor [FLAVOR NAME] \
     --availability-zone europe-se-1a \
     --volume [VOLUME_ID] \
     --network [NETWORK NAME] \
     --security-group default \
     --key-name [KEY NAME] \
     [NAME OF SERVER]

..  seealso::

    - :doc:`create-volume`


===============
Detach a volume
===============

If you want to save a volume (when deleting the instance) for later or just remove it
from an instance, you must first detach it.

A volume used as boot disk, that is, the disk that stores the instances operating system
from an image **cannot detach**. If you want to save such a volume, the workflow would be:

- :doc:`Snapshot <../snapshots/index>` the volume.

- :doc:`Create a new volume from the snapshot <../snapshots/create-volume-from-snapshot>`.

- (optional) Delete the original instance with its volume.

Because of this, volumes that are non-detachable (for being boot drives) will not give that
option in the cloud management portal - and will produce an error when trying to detach in
OpenStack Horizon or via the OpenStack terminal client.  

.. important::

   While its possible, we don't recommend detaching a volume on a running instance as it could
   impact the data integrity if its not properly unmounted in the operating system.

Cloud management portal
-----------------------

To detach a volume by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you want to resize.

- Press the **Volumes** tab.

- Press the small chain-link icon with a line over to the right next to the volume you want
  to remove. If there isn't such an icon, deleting the volume is not possible.

OpenStack Horizon
-----------------

To detach a volume by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- Press the name of the instance from which you want to remove a volume.

- At the bottom, under the **Volumes attached** section, press the volume you want
  to detach.

- At the top right, press the dropdown and select **Manage Attachments**

- Press **Detach volume** on the row of the instance. 

- If you get an error, most likely the volume is a boot volume and cannot be
  detached (see above).

OpenStack Terminal Client
-------------------------

To detach a volume by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server list``, save the name of the instance having the
  the volume you want to extend.

- Run this command: ``openstack server volume list [NAME_OF_INSTANCE]``, replacing the
  name with that from previous step.

- Run this command: ``openstack server remove volume [INSTANCE_ID] [VOLUME_ID]``, replacing
  the items in angle brackets with the **Server ID** and **Volume ID** from the output of the
  previous command.

- If you get an error, most likely the volume is a boot volume and cannot be
  detached (see above).

..  seealso::

    - :doc:`create-volume`


==========================
Extend a persistent volume
==========================

You are able to extend volumes to create more space available on your compute instances.

This is good because you do not have to pay for space that you are not sure if you are
going to need or not, from when setting up the instance and can start small.

.. caution::

   While you can extent a volume, you cannot revert an extension or decrease the size of
   a volume. If you need a smaller volume, create a new volume and migrate the data.

.. important::

   Manipulating the partition table of disks is always a dangerous process. We **strongly**
   recommend first :doc:`backing up </backup/index>` your instances disks before proceeding.

Cloud management portal
-----------------------

To extend a volume by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances**.

- Press the instance having the volume you want to extend.

- Press the **Volumes** tab. It might require scrolling the tabs to the right.

- Press the name of the volume that you want to extend.

- Press the **Extend volume** button (to the top right, looks like a dotted square). 

- Enter the new size in GB and press **Extend**.

OpenStack Horizon
-----------------

To extend a volume by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, press **Compute** and then **Instances** in the sidebar menu.

- Press the instance having the volume you want to extend.

- Press the name of the volume under the **Volumes Attached** section.

- Press the dropdown next to **Edit volume** on the top right and select **Extend volume**

- Enter the new size in GB and press **Extend Volume**

OpenStack Terminal Client
-------------------------

To extend a volume by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server list``, save the name of the instance having the
  volume you want to extend.

- Run this command: ``openstack server volume list [NAME_OF_INSTANCE]``, replacing the name
  with that from previous step.

- Find the volume you want to extend and copy the volume ID.

.. note::

   Use 3.42 microversion or newer for the Cinder API as that introduced support for
   extending an in-use volume.

- Run this command: ``openstack --os-volume-api-version 3.42 volume set [VOLUME_ID] --size [SIZE]``, replacing
  the values in angle brackets with the volume ID from the previous step and the size in GB.

- To verify, run this command: ``openstack volume show [VOLUME_ID] -c size``, replacing value
  with the volume ID. 

Operation inside operating system
---------------------------------

Above we walked through the process for growing a volume in the platform but there is
also a need to grow the disk and file system inside the instance.

Depending on what operating system you are running this process will vary and below are
just suggestions that we have found to be useful.

For a complete reference, see your operating systems documentation.

.. important::

   Manipulating the partition table of disks is always a dangerous process. We **strongly** recommend
   first :doc:`backing up </backup/index>` your instances disks before proceeding.

Linux
^^^^^

- Run this command: ``lsblk``, verify that the new disk size is visible on the device. If its not, reboot
  your machine and try again. 

- Run this command: ``growpart -u auto /dev/sda 1``, use the correct device (``/dev/sda`` in the example) from
  previous commands output. This might not work on all images, if not then we recommend reading the documentation
  of your Linux distribution.

- Run this command: ``resize2fs /dev/sda1``.

- To verify, run this command: ``df -h /dev/sda1``.

Windows
^^^^^^^

- Type Computer Management in the search box on the taskbar, select and hold (or right-click) Computer Management, and
  then select Run as administrator > Yes. After Computer Management opens, go to Storage > Disk Management.

- Select and hold (or right-click) the volume that you want to extend, and then select Extend Volume.

..  seealso::

    - :doc:`create-volume`


==========================
Persistent storage (block)
==========================

Persistent block storage provides a block based storage media to a :doc:`compute instance </compute/index>`.

This comes in the form of **Volumes**, which can use either of our :doc:`../storage-types` as
backing media to save data permanently. 

Binero cloud will not provision an instance without a volume attached (unless using NVMe) and the smallest
volume size allowed is the size of the :doc:`image </images/index>` or disk specified in the :doc:`flavor </compute/flavors>`.

The image is the operating system image copied onto a volume, you can also create a raw volume
and add a filesystem on the volume and attach it to an instance for other use-cases.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  create-volume
  extend-volume
  volume-operations
  detach-volume
  creating-an-instance-from-a-volume
  multi-attached-persistent-volumes


=================
Volume operations
=================

On a volume, you can: 

- :doc:`Extend the size <extend-volume>`

- :doc:`Create and work with snapshots <../snapshots/index>`

- :doc:`Change the type (for SSD and HDD volumes) <../retype-a-volume>`

You are also able to :doc:`backup </backup/index>` your volumes and re-use
them as :doc:`images </images/index>`. 

A volume created at the same time as an instance can have the **Delete on termination**
set which means the volume is **permanently deleted** when deleting the instance.

If you want to keep a volume you can always perform any of the below actions
to save or clone a volume.

- Backup the volume.

- Copy the volume to an image.

- Snapshot the volume and create a new volume from the snapshot

.. important::

   Standard behaviour is that a volume gets deleted with its instance. Don't delete an instance
   that you want to keep data from without performing one of the above actions first. 


=================================
Multi-attached persistent volumes
=================================

Standard concept
----------------

A volume normally has a one-to-one connection to an instance. This is akin to a physical hard drive
cabled directly to a computer.

In Binero cloud, the volume type determines the ability to connect a volume to one or
more instances. Our ``ssd`` and ``hdd`` volume types supports attachment to one instance.

Since volumes in a cloud based system are logical constructs it's technically possible to connect
a :doc:`volume </storage/persistent-block-storage/index>` to more than one :doc:`instance </compute/index>`,
but there are some disadvantages in doing so, read more about that below.

Multi-attach considerations
---------------------------

Binero cloud also offers two more volume type for multi-attach purposes.

- `ssd-multiattach`

- `hdd-multiattach`

These are not available by default but made available upon activation on a per customer basis (by :doc:`contacting our support </general/getting-support>`)
and will allow a volume to attach to more than one instance.

.. important::

   It's important to consideration data integrity in combination with
   multi-attach. We **strongly** recommend reading the article in full
   before proceeding.

When attaching a volume to more than one instances, **each will assume it has exclusive access to the volume**.

Operating systems include not default support that checks for other systems accessing a volume. Storage operations
will read and write to the volume with an assumption that it has exclusive access to the storage media.

Writes (which change the data) depend on exclusive access. This is because a drive needs to do things in a serialized
manner on the block level. If another process (unbeknownst to the first) writes, blocks can become garbled which would
corrupt the filesystem on the volume.

Block compared to file storage
------------------------------

A commonly used solution for sharing a disk is NFS on Linux and SMB or CIFS on Windows. These services provides *file storage*, as
opposed to *block storage* which is what volumes and multi-attached volumes provide, in a shared manner.

File storage happens on a higher level than block storage as file systems store data on blocks.

NFS accepts write requests that reach it over the NFS protocol. This protocol will manages locking of *files* controlling the
access to a file during operations.

Further, since the server running the NFS service *alone will write data to the block device* this mitigates the risk
for the file system being corrupt.

The instance running the NFS service is file-system-aware and will manage writes within the confines of whats acceptable
for the filesystem, mitigating the risk of the data becoming corrupt.

Another example of network attached storage is ISCSI. This protocol, unlike NFS, SMB and CIFS, presents to the operating system
as a pure block device on which the operating system can, for example, write its file system. SCSI commands gets sent across the
network instead of an internal cable in a server. ISCSI is not in and of itself, aware of files or even a filesystem. It will
write (manipulate blocks) according the SCSI instructions received.

The storage system running ISCSI is not file-system-aware and will manage writes in a file-system-agnostic manner
(in the ISCSI case, by SCSI commands), thereby not guaranteeing the write integrity of what is above the SCSI layer.

Cluster file systems
--------------------

In a cluster file system, the file system itself guarantees that data is not corrupted by locking and fencing methods
between all members of the cluster thus coordinating the operations performed on blocks.

Popular cluster filesystems include:

- OCFS2

- GFS2

- VMFS

Popular non clustered file systems include:

- EXT4 or EXT3

- XFS

- NTFS

These will **NOT** work with similar writes from more than a single instance. 

.. caution::

   Its entirely possible to setup both multi-attach and ISCSI with for example EXT4 and mount the device on servers, it will
   mount just fine.

   The issues will come from writing the same block and for the file system to be aware that there has
   been changes to a block.

   After a while it's most likely your file system will become corrupt if you use a non clustered file system.

Using multi-attach in Binero cloud
----------------------------------

In Binero cloud, the two volume types for multi-attach is available upon activation for a customer by request to
our support department.

**The reason for this is that we want to make sure our customers are aware of the risk for data loss when using
multi-attach improperly.**. 

Once made available, they are usable as *volume type* when
:doc:`provisioning volumes </storage/persistent-block-storage/index>`.

If you want to change a current volume to be multi-attachable, see :doc:`retype a volume</storage/retype-a-volume>`. 


======================================
Storage regions and availability zones
======================================

Storage is a core service and is available in all Binero clouds regions and availability
zones.

Below we outline the region and availability zone concepts for storage.

If you are looking to roll out a geographically redundant platform, understanding the base
design principles will help you create a resilient and highly available system and securing
your data in case of a hypothetical availability zone outage scenario.

Volumes
-------

:doc:`Volumes <persistent-block-storage/index>` are local to an availability zone. Binero cloud
does *not* stretch our storage platform(s) but use individual storage in each availability zone.

This is to guarantee that the storage platform is available even in case of an outage on storage
in other availability zones.

Binero cloud currently does not allow for replication of block storage between zones. We recommend
replicating your data on the application layer or by using our :doc:`object storage <object-storage/index>` 
service.

Moving a volume between availability zones
------------------------------------------

For security reasons, there is no network connectivity between the storage platforms of our
availability zones. Because of this, there are two main options to move volumes:

- Create a :doc:`backup </backup/index>` and restore in the different zone. 

- Create an :doc:`image </images/index>` from a volume and setup an instance in a different zone based on the image. 

Object storage
--------------

:doc:`Object storage <object-storage/index>` is available in all our availability zones.

Replication is available using a separate :doc:`endpoint <object-storage/endpoints>` that will replicate between
our europe-se-1a and europe-se-1b availability zones.

When using replicated object storage, there are no extra considerations (with regards to the actual replication)
for the user as the platform manages the entire replication.

The user needs to take care to connect to the endpoint that is local to the application, we recommend using the
europe-se-1a availability zone when connecting from the internet.

NVMe storage
------------

NVMe storage is not replicated and is currently only available in the europe-se-1a availability zone. 

Snapshots
---------

All snapshots are local to the volume that the snapshot depends on. This means that snapshots are store in the
availability zone where the volume exists.

When creating a volume from a snapshot, it will be save in the same availability zone as the snapshot.

While you are able to provision an instance from a snapshot in a different availability zone, this provision
task will fail. 


===============
Retype a volume
===============

If you created a volume with the wrong :doc:`storage type <storage-types>` you are able
to retype it.

Perhaps you've noticed that a disk was to heavily utilised causing IO-wait on your instance
because it's using HDD storage and want to change it to SSD. The other scenario might be a
volume has grown and is to expensive to host on SSD. 

.. note::

   NVMe volumes are different, they are a local resource on the hypervisors and cannot be
   retyped. If you need to move to NVMe, you would need to setup a new instance based with
   NVMe, attach your volume, move the data to the local disk, then detach and delete the
   volume. 

The following two requirements **must be true** to re-type a volume attached to an instance:

- The instance must be running.

- There must not be :doc:`snapshots <snapshots/index>` on the volume.

Any deviations will cause a silent fail. A volume that is not attached also cant have any snapshots
but will otherwise be possible to just retype at convenience.

A retype can take hours to complete depending on the size of the volume, the instance is available
during this time but can have reduced IO during the swap-over.

We recommend doing this procedure during the night time (when IO is likely less).

.. important::

   As retyping involves copying *all* the data of the instance, we **strongly** recommend
   first :doc:`backing </backup/index>` it up before proceeding.

Cloud management portal
-----------------------

Its currently not possible to retype volumes by using the cloud management portal.

OpenStack Horizon
-----------------

To retype a volume by using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- Press the name of the instance from which you want to remove a volume.

- At the bottom, under the **Volumes attached** section, press the volume you want to retype.

- Select **Change volume type** in the dropdown in the top right corner.

- Select the new type.

- In migration policy dropdown, select **On-demand**.

- Press **Change volume type**

- A progress bar will show during the retype operation (which can take hours depending on size).

OpenStack Terminal Client
-------------------------

To retype a volume by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume list``, save the ID of the volume you want to retype.

- Run this command: ``openstack volume type list``, save the name of the new volume type you want
  to use (for example ``ssd`` or ``hdd``).

- Run this command: ``openstack volume set --type [TYPE_NAME] [VOLUME_ID] --retype-policy on-demand``, replacing
  the items in angle brackets with the volume type name and the ID of the volume.

- Run this command to verify: ``openstack volume show [VOLUME_ID] -c status``, when the status changes
  from retyping, the retype is done.

..  seealso::

    - :doc:`persistent-block-storage/create-volume`


===========================
Create backup from snapshot
===========================

A snapshot is a good base for creating a backup as its guaranteed to be
static data. That is, once the snapshot is available (which is a fast
procedure), no more writes will occur to the data in the snapshot, this
assumes the integrity of the data stored, see :doc:`/storage/snapshots/create-snapshot`.

More information on backups (which we recommend you to read) is available in
our :doc:`/backup/index` article.

.. note::

   Backups are just for volumes. You are not able to backup an image.

Cloud management portal
-----------------------

Its not possible to create a backup from a snapshot manually using the cloud
management portal.

OpenStack Horizon
-----------------

To create a backup from a snapshot by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Volumes** and then **Snapshots** in the sidebar menu.

- Locate the snapshot in the list and click **Create backup** from its dropdown menu to the right.

- Input a name for your backup and an optional description.

- Leave the container field empty. 

- The snapshot you chose should be pre-selected, if not select it.

- Click **Create Volume Backup**

OpenStack Terminal Client
-------------------------

To create a backup from a snapshot by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume snapshot list``, save the ID of the snapshot you want to backup.

- Run this command: ``openstack volume snapshot show [SNAPSHOT_ID] -c volume_id``, replace the SNAPSHOT_ID
  with the id from previous command. Save the volume ID of the snapshot.

- Run this command: ``openstack volume backup create --snapshot [SNAPSHOT_ID] --name [BACKUP_NAME] [VOLUME_ID]``, replacing
  the items in angle brackets with the values from previous two commands as well as a name for the backup-job.

- Run this command to verify: ``openstack volume backup list``, the status field on the backup will
  change from creating to available when done.

..  seealso::
    - :doc:`index`
    - :doc:`../persistent-block-storage/index`


===============
Create snapshot
===============

A snapshot is a point-in-time snapshot of the state of an instance or
a volume.

You can create a snapshot of a single volume or create a snapshot of an
instance, when creating a snapshot of an instance, snapshots get created
for each volume attached to the instance. Metadata about the instance gets
saved as an image.

This part will focus on creating a snapshot of an instance. This will
allow you to create a new instance from the snapshot.

.. note::

   Creating a new instance from the image created from a snapshot will
   reuse some parameters from the original instance but not for example
   instance ports (and thus not any IP addressing).

.. important::

   For data integrity reasons, we recommend shutting of the instance before taking
   a snapshot.

   Incomplete writes might otherwise impact data integrity and in result in a
   inconsistent snapshot with corrupt data resulting in at worst data loss.

We recommend only having snapshots for limited periods of time and recommend that you
regularly :doc:`delete snapshots <delete-snapshot>`.

.. caution::

   Snapshots is **NOT** backups and are does not provide any more data safety or
   guarantee than the underlying storage used for the volume does.

Cloud management portal
-----------------------

To create a snapshot using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Instances** in the sidebar menu.

- Press the instance that you want to resize.

- Press the **Snapshots** tab. 

- Press **Create snapshot** button.

- Name your snapshot. 

- Press **Create**

Your snapshot is available in the **Snapshot** tab of the instance and if you press
**Storage** and then **Snapshots** in the main menu. In the latter case, there will
be one snapshot per volume of the instance you created the snapshots from (only one
if the instance only had a single volume).

OpenStack Horizon
-----------------

To create a snapshot using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Instances** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the instance you
  want to snapshot, press **Create snapshot** (which is either the default option if the
  instance is running, of in the dropdown if it's not running).

- Name your snapshot. 

- Press **Create Snapshot**

Your snapshot is available in the **Images** menu (under main menu **Compute**) as well
as if you press **Volumes** and then **Snapshots** in the main menu. In the latter case,
there will be one snapshot per volume of the instance you created the snapshots from (only
one if the instance only had a single volume).

OpenStack Terminal Client
-------------------------

To create a snapshot using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack server list``, note the name of the instance you want to
  snapshot.

- Run this command: ``openstack server image create --name [SNAPSHOT_NAME] [INSTANCE_NAME]``, replacing
  the items in angle brackets with a descriptive name (this is optional, if omitted the instance name
  will be used) and and the name of the instance you want to snapshot.

- Run this command to verify: ``openstack image list --private``

..  seealso::

    - :doc:`index`
    - :doc:`../persistent-block-storage/index`


===============================
Create a volume from a snapshot
===============================

If you want to access the data of a snapshot, without provisioning an entirely new
identical instance :doc:`from an image </images/launch-instance-from-image>`, you
would first need to create a new volume based with the data in the snapshot.

When doing this, the new volume will be separate from the original volume and will
forthwith provide its data in its own right.

The version of the data would be from the time when creating the snapshot.

The new volume can attach (see subsection of our guide :doc:`../persistent-block-storage/create-volume`
for more info on attaching a volume) to any instance or used as base to create an entirely
new instance, if it's a volume with bootable media on it.

.. note::

   If your intent is to create a new instance based on a volume, using OpenStack Horizon there is
   an option in the dropdown on the row of each snapshot to use it as base for a new instance, this
   would be the easiest and fastest approach.

Cloud management portal
-----------------------

The cloud management portal cannot create a volume based on a snapshot but can create it
based on a current volume. See :doc:`../persistent-block-storage/create-volume` for more
information.

OpenStack Horizon
-----------------

To create a volume from a snapshot using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Volumes** and then **Snapshots** in the sidebar menu.

- In the dropdown menu to the far right of the line corresponding to the snapshot you want to
  use as base, press **Create volume** (which is either the default option or in the dropdown list).

- Name your new volume and optionally give it a description.

- If you want to increase the size, enter the new size in the **Size** field.

- Press **Create volume**

OpenStack Terminal Client
-------------------------

To create a volume from a snapshot using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume snapshot list``, save the ID of the snapshot you want
  to backup.

- Run this command: ``openstack volume create --snapshot [SNAPSHOT_ID] [VOLUME_NAME]``, replacing
  the items in angle brackets with the ID from previous step and a descriptive name for the new volume. 

- Run this command to verify: ``openstack volume list``, the new volume will have a **Status** of
  **available** when done.

..  seealso::

    - :doc:`index`
    - :doc:`../persistent-block-storage/index`


===============
Delete snapshot
===============

If you took a snapshot of an instance there will be two parts to delete, the *volume snapshot(s)*
and the *created image*.

Deleting a snapshot is not the same as removing the data as you only remove the ability to revert
to that snapshot as it's permanently deleted.

.. note::

   If your intent was to revert to a snapshot to bring the instance or volume back to the state of
   taking the snapshot, see the :doc:`/images/launch-instance-from-image` guide.

We recommend only having snapshots for limited periods of time and that you regularly delete
old snapshots.

In the below example, we will delete both the image and the volume snapshots. If you only took a
snapshot of a volume, then the image will not be available and you should skip the first two steps.

Cloud management portal
-----------------------

To delete a snapshot by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Images** in the sidebar menu.

- Press the delete button (trashcan icon) of the image/snapshot you want to delete.

- Press **Storage** and then **Snapshots** in the sidebar menu.

- Press the delete button (trashcan icon) of the image/snapshot you want to delete.

.. note::

   If you are unsure of which volume snapshot to delete, its possible to press it and then press
   its related volumes and in turn, the volumes related instances.

   The image is metadata and has no ties the actual instance it's modeled from.

OpenStack Horizon
-----------------

To delete a snapshot using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Images** in the sidebar menu.

- Locate the image in the list and press **Delete image** from its dropdown menu to the right. 

- Under **Project**, click **Volumes** and then **Snapshots** in the sidebar menu.

- Locate the snapshot in the list and press **Delete snapshot** from its dropdown menu to the right. 

.. note::

   If you are unsure of which volume snapshot to delete, its possible to press the ID of the Volume
   to see which instance its attached to.

   The image is metadata and has no ties the actual instance it's modeled from.

OpenStack Terminal Client
-------------------------

To delete a snapshot by using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack image list --private``, save the ID of the image/snapshot that
  you want to delete.

- Run this command: ``openstack image delete [ID_OF_IMAGE]``, specifying the ID of the image.

- Run this command: ``openstack volume snapshot list``, save the ID of the snapshot that you
  want to delete.

- Run this command: ``openstack volume snapshot delete [SNAPSHOT_ID]``, specifying the ID of
  the snapshot to delete.

.. note::

   If you are unsure of which volume snapshot to delete, its possible to run
   ``openstack volume snapshot show [SNAPSHOT_ID]`` to get information on what
   volume it's created from.

   Based on the volume ID, you could then run ``openstack volume show [VOLUME_ID] -c attachements``
   to get information on the attachments of the volume (i.e. which instance its attached to).

   Finally, using the server ID, you can get the instance of the volume as
   such ``openstack server show [SERVER_ID] -c name``.

   The image is metadata and has no ties the actual instance it's modeled from.

..  seealso::

    - :doc:`index`
    - :doc:`../persistent-block-storage/index`


=========
Snapshots
=========

A snapshot is a point-in-time snapshot of a filesystem on a volume.

Snapshots are pretty much instant because they don't move any data but
only creates a checkpoint from where it tracks changes. A revert to a
snapshot is also fast.

Snapshots are hierarchal in the sense that each snapshot will reference the
previous snapshots.

The most common use-case for a snapshot is for testing out something in a
production system. For example upgrades or other invasive actions and if
something goes wrong you can revert to the snapshot.

.. tip::

   We recommend that you are restrictive in the usage of snapshots and
   instead use best practices to test your changes before doing any invasive
   actions.

.. important::

   A snapshot is **NOT** a :doc:`backup </backup/index>`. While it might be tempting
   to take recurring snapshots for safety, all data for snapshots is in the same storage
   platform as your primary data and does not protect you in case of an unexpected outage.

If you want to access the data of a snapshot, the recommended approach is to
:doc:`create-volume-from-snapshot` and attach it to an instance (or base a new
instance of the volume).

.. toctree::
  :caption: Available services
  :maxdepth: 1

  create-snapshot
  delete-snapshot
  create-volume-from-snapshot
  create-backup-from-snapshot


======
Backup
======

Using the backup feature in Binero cloud, you are able to backup and restore volumes.

The system stores the backups in the object storage of your account in
:doc:`availability zone </storage/regions-and-availability-zones>` **europe-se-1b**
using the gp.archive :doc:`storage policy </storage/object-storage/storage-policy>`. 

.. important::

   When backing up volumes from availability zone *europe-se-1b*, note that the backups currently
   will *also end up in the same availability zone* (and, in part, on the same storage).

   If you have data that is not also stored in zone europe-se-1a, we recommend using a different
   backup to secure your data for a potential storage outage in zone europe-se-1b.

Data integrity
--------------

The backup service in Binero cloud backs up :doc:`volumes </storage/persistent-block-storage/index>`.

When backing up a volume the system copies the data bit-by-bit from the source to the destination, it
takes time when copying large amounts of data.

The system first takes snapshot of the data if it's a volume. If you're writing data to the volume while
taking the snapshot or if you are not using a volume and are writing data while a backup is running the
data might be corrupt before the data is even copied.

It's recommend to not write data while taking a backup and if you can power off your instance during
a backup that makes it more safe, but that's not always an option so you need to consider the impact
if your data in a backup is corrupt.

For a file server the risk of corrupted files should be minimal and in case of file damage or data loss
it's limited to that file.

When backing up a database the entire database might be reliant on a few files and should one of them
become corrupt, the database will not start again. In this scenario, the usual solution is to first run
a dump of the database data onto a file on the filesystem. This file would then be safe (as its not written
to after the dump) and in case of a restore, you can import the data back into the database from the dump.

Shutting of your instance will make the backup entirely safe from above issues. 

.. tip::

   We strongly recommend doing a restore test of your backups. Since :doc:`restoring <restore-volume>` to a
   new volume and creating a new instance that could run parallel to your production workload, this is a good
   way to ensure data consistency in your backup.

Setting up backup
-----------------

You can use the Binero cloud backup feature in two main ways, see below.

Manual backup
^^^^^^^^^^^^^

A manual backup is useful before doing a migration, upgrade or when retire an old system but want to
keep a copy of the data.

More information in our :doc:`manual-backup` article.

Automated backup
^^^^^^^^^^^^^^^^

The platform is able to automatically create backups for you. This can be setup via the
:doc:`platform automation tool </platform-automation/index>`, we strongly recommend using
our :doc:`service catalog </service-catalog/schedule-backup>` to enable automated backup.

The platform will not do incremental backups when using the built-in workflow to run backups.

More information in our :doc:`automatic-backup` article.

.. note::

   Before creating a backup, note that you are strongly recommended to also dump any database to disk. If
   the backup is ran on a schedule, also dump your database on a schedule inside your instance before the
   system takes the backup.

.. toctree::
  :caption: Available services
  :maxdepth: 2

  manual-backup
  automatic-backup
  restore-volume


========================
Setting up manual backup
========================

When doing a manual backup, you would copy a single volume (in its entirety) and send
the data to the backup system.

We recommend first reading through our :doc:`general information <index>` on backups to
get more information about the backup system.

You are able to use one of the following methods to take a backup.

.. note::

   It will take some time to take a backup as it involves moving all data of the server to
   the backup storage.

Cloud management portal
-----------------------

To create a backup of a volume by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Storage** and then **Backups** in the sidebar menu.

- Press the **+** icon in the lower right corner.

- Select a volume from the dropdown menu. If you are unsure about which volume (ID) you
  can check the instance to see which volumes is currently attached to it: 

  - Press **Compute** and then **Instances**. 

  - Press the instance you want to backup.

  - Press the **Volumes** tab. 

  - Note the name of the attached volumes (there might be many).

- Give your backup a name. 

- Press **Force** if the volume is currently attached to an instance or else the backup
  will give an error.

- Optionally, give your backup a description. 

- Press **Create backup**

.. note::

   An option for doing incremental backups exists but the platform will not keep track of the full backups.

OpenStack Horizon
-----------------

To create a backup of a volume by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Volumes** and then **Volumes** in the sidebar menu.

- Click **Create backup** in the dropdown menu to the far right on the row of the volume that you
  want to backup. You are able to see which instance the volume is currently attached to in the
  **Attached to** column.

- Give your backup a name and optionally a description.

- Optionally select a snapshot if you want to backup based on a snapshot and not the entire disk.

- Press the **Create volume backup** button.

OpenStack Terminal Client
-------------------------

To create a volume by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume list``, save the ID of the volume you want to backup. Its visible
  which instance the volumes is currently attached to in the **Attached to** column. 

- Run this command: ``openstack volume backup create --force --name [BACKUP_NAME] [VOLUME_ID]``, replacing
  with the name of the backup and the ID from the previous step.

- If you want to backup a snapshot, you could instead first list the snapshots by running ``openstack volume snapshot list``
  and then backup using ``openstack volume backup create --force --name [BACKUP_NAME] --snapshot [SNAPSHOT_ID] [VOLUME_ID]``

.. note:: 

   The ``--force`` is only required if the volume is attached to an instance.

..  seealso::

    - :doc:`restore-volume`


===========================
Setting up automatic backup
===========================

To setup an :doc:`automated backup job <index>` by using the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

- Press **Service catalog** and then **Service templates** in the sidebar menu.

- Press the small arrow (create service) on the **Schedule backup job** service.

..  seealso::

    - :doc:`restore-volume`


==========================
Restore volume from backup
==========================

When restoring from backup, you would restore an entire volume.

Restoring single files is not possible and requires creating a
new :doc:`instance </compute/index>` created you can then collect
the files from the instance. 

To restore, you would first create a volume, restore to it and then
:doc:`create and instance from the volume </storage/persistent-block-storage/creating-an-instance-from-a-volume>`.

Cloud management portal
-----------------------

To restore a backup to a volume by using the
:doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Storage** and then **Backups** in the sidebar menu.

- Note the size of the backup you want to restore. 

- Follow the steps in our :doc:`/storage/persistent-block-storage/create-volume` guide to
  create an empty volume of the same size as the backup. Note the name.

- Press **Storage** and then **Backups** in the sidebar menu.

- Press the small rounded arrow **Restore backup** on the backup that you want to restore.

- Select the volume you just created in the dropdown **Volume**

- Press **Restore backup**

.. note::

   While there is an option to create a new volume when restoring a backup, this does not work due to
   not taking availability zones into consideration, you need to first create a new empty volume that
   you can restore to.

OpenStack Horizon
-----------------

To restore a backup of a volume by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Volumes** and then **Backups** in the sidebar menu.

- Note the size of the backup you want to restore.

- Follow the steps in our :doc:`/storage/persistent-block-storage/create-volume` guide to create an
  empty volume of the same size as the backup. Note the name.

- Under **Project**, click **Volumes** and then **Backups** in the sidebar menu.

- Press **Restore backup** to the far right on the line of the backup that you want
  to restore.

- Select the volume you just created in the dropdown **Select Volume**

- Press **Restore backup to volume**

OpenStack Terminal Client
-------------------------

To restore a volume by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack volume backup list``, note the size of the backup that you want to
  restore and the backup ID.

- Follow the steps in our :doc:`/storage/persistent-block-storage/create-volume` guide to create a
  new empty volume of the same size as the backup. Note the new volume ID, of find the volume with
  command ``openstack volume list``.

- Run this command: ``openstack volume backup restore --force [BACKUP_ID] [VOLUME_ID]``

.. note::

   As of writing this, the restore via CLI is not working as intended on all versions of the
   client. If you get issues from above command, try a newer version of the client or restore
   via one of the other options. 

..  seealso::

    - :doc:`index`
    - :doc:`manual-backup`
    - :doc:`automatic-backup`


============
Secret Store
============

General concept
---------------

Using our secret store, you are able to give the platform access to secrets such as certificates
and keys in a secure manner.

We support different secret formats and also incorporate ACLs (Access Control Lists) allowing you
to give certain :doc:`API-user </getting-started/users>` access to secrets while withholding access
for other users.

Here are some examples of secrets that you can store in our secret store:

- Symmetric Keys - Used to perform reversible encryption of data at rest, typically using the AES
  algorithm set. This is a required type to enable features such as encrypted Swift containers
  and Cinder volumes, encrypted Cloud Backups and so on.

- Asymmetric Keys - Asymmetric key pairs (sometimes referred to as public / private keys) has many
  use-cases, in most scenarios it's used for securing communication between parties. The most common
  case is with SSL/TLS certificates.

- Raw Secrets - Barbican stores secrets as a base64 encoded block of data (that is then stored
  encrypted).

Some of the services in the platform that consume secrets are:

- Object Storage - To encrypt objects in the object store.

- Load Balancer - To manage Octavia HTTPS enabled LB certificate-key pairs.

- Storage - To encrypt volumes and snapshots.

- Compute - To use encrypt volumes and snapshots to create instances.

Secrets
-------

Secrets represent keys, credentials, and other sensitive data stored by the secret
store service. The secret service supports operations such as:

- :doc:`Creating secrets <create-secret>`.

- Listing secrets.

- Controlling access to secrets by setting up :doc:`ACLs (Access Control Lists) <acl>`.

- Deletion of secret.

- Decryption of secrets.

.. note::

   Secrets are only manageable via the :doc:`/getting-started/managing-your-cloud/cloud-management-portal` and
   via :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`. No integration in OpenStack
   Horizon is currently available.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  create-secret
  acl
  create-cert-for-loadbalancing


================
Creating secrets
================

Creating a secret in the cloud management portal
------------------------------------------------

To create a `secret <../index>`_ from the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Secret management** and then **Secrets** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Select a user from the dropdown and enter the password for that user along with other details. Credentials of
  the User to make it the owner of the secret. 

- Enter a descriptive name of the secret.

- Select what kind of payload (file or text) and input it. The Payload Data field stores the secret data to be
  encrypted and stored.

- Press **Create**. The secret is now stored.

Creating a secret using the OpenStack terminal client
-----------------------------------------------------

This shows you how to create a secret :doc:`secret <../index>` by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

Uploading a certificate
^^^^^^^^^^^^^^^^^^^^^^^

This is an example of how to upload a certificate in binary format that's needed for TLS termination when
using our :doc:`load balancer </networking/load-balancer/index>` service.

::

    openstack secret store --name='[SECRET_NAME]' -t 'application/octet-stream' -e 'base64' --payload="$(base64 < lb-cert.p12)"

Encryption key for object storage
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is an example of how to upload a encryption key that's needed with the SSE-KMS specification
in our :doc:`object storage </storage/object-storage/index>` service.

You can use ``openssl rand -base64 32`` to generate a new encryption key that is base64 encoded.

::

    openstack secret store --name '[SECRET_NAME]' --payload-content-type='application/octet-stream' --payload-content-encoding='base64' --algorithm 'aes' --bit-length 256 --mode 'ctr' --secret-type 'symmetric' --payload [base64_encoded_payload]


===============
ACLs on secrets
===============

Access Control List (ACL) feature in the secret store provides user level access control for
secrets and containers.

By default the secret store provides access to its resources (secrets and containers) on a per
project level and authorisation based on the roles a user has in that project.

ACL Contains three main information which helps in deciding the level of access which different
users have on the secret:

- Project Access Flag - By default, project access is True. If changed to False, it makes the secret
  Private and only the Owner and Users added would be able to perform operations such as Decryption
  and Deletion.

- Users - This include the user IDs of the users who have access to Decryption of a secret in case of
  a secret marked as Private, that is with Project Access set to False.

- Operation Type: This denotes the read/write operation applicable on a secret. Currently, Barbican
  only supports the read operation.

Following are the operations associated with ACL on a Secret:

- Get ACL: This helps in retrieving the ACL settings on a secret. It contains information about Project
  Access Flag, Users Array, Operation Type, ACL Set Date and update date.

- Set ACL: Used to control the ACL settings on a secret. The user can add other users who will have decrypt
  and delete access for private secret, user can make the secret as public or private using the project access
  checkbox, specify the Operation Type (Barbican only supports read).

- Reset ACL: Used to reset the ACL Settings to default, Mark the secret as Public in the project and makes it
  accessible for decryption and deletion to all user in the project.

.. note::

   Any user in the project can perform ACL Operation on a secret, irrespective of whether the secret is public
   or private.


==================================================================
Creating a certificate for SSL/TLS termination using Load Balancer
==================================================================

When using the :doc:`/networking/load-balancer/index` service in Binero cloud
you can perform SSL/TLS termination for connections directly, you need to have
a certificate saved in our :doc:`secret store <index>`.

The certificate needs to be in the *PKCS#12 format* to work with the
load balancer. 

Converting to PKCS#12
---------------------

Follow the below steps to create a PKCS#12 certificate for use with
the :doc:`/networking/load-balancer/index` service:

- Gather your certificate *and chain* in a single file.

  - For a LetsEncrypt certificates there is the file ``/etc/letsencrypt/live/yourdomain.com/fullchain.pem``
    that is already prepared for this.

  - If you don't use LetsEncrypt you need to make sure you have the full chain
    of your certificate available and copy it into the same file, for example
    as such: ``cat /etc/ssl/cert.crt /etc/ssl/chain.crt > new_cert.crt``

- Make sure you have your key available.

- Run the following command, replacing the items in angle brackets to the correct
  file names according to previous steps: ``openssl pkcs12 -export -in [CERT_FILE_NAME.PEM] -inkey [YOUR_KEY.KEY] -out lb-cert.p12 -passout pass:``

- The new cert (that also includes the key) is then stored in the file ``lb-cert.p12``

Uploading the certificate to the secret store
---------------------------------------------

You are able to follow the :doc:`guides for uploading the certificate <create-secret>`
to store the certificate, either using the cloud management portal or the terminal client. 

..  seealso::

    - :doc:`/networking/load-balancer/index`


===============
Service catalog
===============

The service catalog is a repository of installable open source software and convenience
functions for automating more complex tasks in the platform.

Its only available in the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`.

The service catalog is available in the cloud management portal from the main menu under
**Service Catalog** and then either **Services** which will show you your provisioned services
and allow you to add new or **Service templates** which will show you available services.

Create service
--------------

To provision a new service by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Service Catalog** and then **Services** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Select a service in the dropdown. 

- Press **Next**

- For detailed information on each service, see the articles for each service
  in the documentation.

Features
--------

Automated features simplify creating complex setups in the platform by using the service catalog to
automate them.

The services will be setup the same way as if they where manually setup but the process is faster
and requires less knowledge. The following services is available:

- :doc:`Private network <private-network>` will create a router and a network setup
  behind it.

- :doc:`Schedule snapshot <schedule-snapshot>` will schedule taking a snapshot from a
  particular instance.

- :doc:`Schedule backup <schedule-backup>` will schedule taking a backup from a particular
  instance.

Applications
------------

Automated application installations are available to provide applications pre-installed on
an instance.

Aside from having the software installed, the software and services will also, when applicable,
configured for you with verified and secure defaults. The following applications is available:

- :doc:`PostgreSQL <postgresql>` will install a PostgreSQL instance with the option of
  adding replication.

- :doc:`MySQL or MariaDB <mysql>` will install a MySQL or MariaDB instance with the option
  of using replication.

- :doc:`Grafana <grafana>` will install a Grafana Dashboard.

- :doc:`GitLab <gitlab>` will install a GitLab repository with its dashboard.

- :doc:`File share <file-share>` will install a Linux VM with both NFS and SMB support for use
  as a file store backend in the platform.

- :doc:`LAMP <lamp>` will install a Linux Apache MySQL PHP stack on a single instance.

- :doc:`Load balanced LAMP <lb-lamp>` will install a load balanced, using the Binero cloud load balancer service,
  Linux Apache MySQL PHP stack using separate instances for each component.

- :doc:`OpenVPN <openvpn>` sets up VPN, see :doc:`/networking/client-vpn/index`.

- :doc:`Redis <redis>` will install a `Redis <https://redis.io>`__ in-memory data
  structure store.

.. toctree::
  :caption: Available services
  :maxdepth: 2

  file-share
  gitlab
  grafana
  lamp
  lb-lamp
  mysql
  openvpn
  postgresql
  private-network
  redis
  schedule-backup
  schedule-snapshot


==========
File share
==========

If you need to store files on a filesystem that can be access across a
network, using our file share service is a good option.

The file share service will provision a Linux based instance on which it
will setup either (or both of) NFS and Samba (SMB / CIFS). 

To setup the service, first follow the general instructions on our
:doc:`index` page. Then follow these instructions: 

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>` you
  want to provision in. We recommend the *europe-se-1a* availability zone.

- If you want backup, check the **Backup** checkbox and select an amount of days
  you want your history stored.

- Select disk-type. See the :doc:`/storage/storage-types` article for more information.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking with the default.

- Select your :doc:`SSH-key </compute/ssh-keys>`.

- Under **Local network**, select the :doc:`network </networking/network/index>`
  on which you want to run the service.

- Check **NFS** if you want to install support for NFS.

- Check **Windows Samba** if you want to install support for SMB/CIFS.

- Select the storage volume size. This is the volume on which you will store your data. The
  volume can extended later, see the :doc:`/storage/persistent-block-storage/extend-volume`
  article.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


======
GitLab
======

`GitLab <https://about.gitlab.com>`__ is a DevSecOps platform, providing a
wide array of features.

Running your own installation in your private cloud will increase your security
and give you control of your data.

To setup the service, first follow the general instructions on our :doc:`index`
page. Then follow these instructions: 

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>`
  you want to provision in. We recommend the *europe-se-1a* availability zone.

- If you want backup, check the "backup" checkbox and select an amount of days
  you want your history stored.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking
  with the default.

- Select your :doc:`SSH-key </compute/ssh-keys>`.

- Under **Local network**, select the :doc:`network </networking/network/index>`
  on which you want to run the service.

- If you want your service publicly available on the internet, you can assign a floating
  IP (you can perform this later if you prefer) by checking the **Public access** checkbox.

  - If you don't use a floating IP, you can only access it locally in your network or
    by using some kind of ingress solution such as a VPN.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


=======
Grafana
=======

`Grafana <https://grafana.com>`__ is an observability platform that enables
visualising and graphing data from one or many data sources in a powerful way.

You also have the option to install a local (to the instance)
`InfluxDB time series database <https://www.influxdata.com>`__ to
use as source for your Grafana instance. 

To setup the service, first follow the general instructions on our
:doc:`index` page.

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>`
  you want to provision in. We recommend the *europe-se-1a* availability zone.

- If you want backup, check the **Backup** checkbox and select an amount of days
  you want your history stored.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking
  with the default.

- If you want to also install an InfluxDB server, check the **InfluxDB** checkbox.

  - If you don't, you can still use Grafana to visualise data from some already
    existent data source. 

- Select your :doc:`SSH-key </compute/ssh-keys>`.

- Under **Local network**, select the :doc:`network </networking/network/index>`
  on which you want to run the service.

- If you want your service publicly available on the internet, you can assign a floating
  IP (you can assign one later if you prefer) by checking the **Public access** checkbox.

  - If you don't use a floating IP , you can only access it locally in your network or
    by using some kind of ingress solutions such as a VPN.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


===================================
Linux Apache MySQL PHP stack (LAMP)
===================================

Using the popular and powerful open source tools `Linux <https://www.linux.org>`__,
`Apache <https://httpd.apache.org>`__, MySQL (`MariaDB <https://mariadb.org>`__)
and `PHP <https://www.php.net>`__, you can provision a complete production suite.

Popular tools such as WordPress or frameworks like CakePHP and Laravel run on
LAMP installations. 

To setup the service, first follow the general instructions on our :doc:`index`
page.

- Give your service a name and optionally a description.

- Under SSH Admin IP, add your public IP (this will allow your workstation through the
  firewall) in :doc:`CIDR notation </networking/subnet/subnet-format>`.

  - If you need to change this later, this sets up a :doc:`security group </networking/security-groups/index>`
    on the instance.

  - You can change or update the security group as you see fit but if you want to login
    over the floating IP that will be setup, you will need to input your IP here. 

- Select which :doc:`availability zone </compute/regions-and-availability-zones>` you
  want to provision in. We recommend the *europe-se-1a* availability zone.

- If you want backup, check the **Backup** checkbox and select an amount of days you
  want your history stored.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking with
  the default.

- Select your :doc:`SSH-key </compute/ssh-keys>`. 

- Under ``local_net`` select the :doc:`network </networking/network/index>`
  on which you want to run the service.

- If you want your service publicly available on the internet, you can assign a floating
  IP (you can assign one later if you prefer) by checking the **Public access** checkbox.

  - If you don't use a floating IP, you can only access it locally on your network or
    over some kind of ingress solution such as a VPN.

- Under **Volume size**, select the size (in GB) of the volume of the server. Take into
  consideration that you will store files and a database on the server. We recommend
  at least 100 GB.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


===========================================
Load Balanced Linux Apache MySQL PHP (LAMP)
===========================================

Using the popular and powerful open source tools `Linux <https://www.linux.org>`__,
`Apache <https://httpd.apache.org>`__, MySQL (`MariaDB <https://mariadb.org>`__)
and `PHP <https://www.php.net>`__, you can provision a complete production suite.

Popular tools such as WordPress or frameworks like CakePHP and Laravel run on LAMP
installations.

If you want to be able to scale your LAMP installation beyond a single server, using
this service to setup a load balanced system will be a good way to go about it.

The following will be setup for you: 

- A :doc:`/networking/load-balancer/index`

- A database server instance running MariaDB.

- A NFS service instance for shared file storage.

- Web servers (you can choose how many) that gets added as backends behind the load balancer
  and accept the web requests, running Apache and PHP and mounts the shared file storage.

To setup the service, first follow the general instructions on our :doc:`index` page.

- Give your service a name and optionally a description.

- If you want backup, check the **Backup** checkbox and select an amount of days you
  want your history stored.

- Under **db_flavor**, select your *database instances* :doc:`flavor </compute/flavors>`.
  We recommend sticking with the default.

- Select disk-type for the *database instance*. See the :doc:`/storage/storage-types`
  article for more information. We recommend SSD.

- Select your :doc:`SSH-key </compute/ssh-keys>`.

- Under **local network**, select the :doc:`network </networking/network/index>`
  on that you want to use.

- Under **nfs_flavor**, select your *NFS instances* :doc:`flavor </compute/flavors>`.
  We recommend sticking with the default.

- Under **Subnet**, select the :doc:`network </networking/subnet/index>`
  on which you want to run the service.

- Under **Web count**, select the amount of load balanced web servers (instances
  running apache) you want. 

- Under **web_flavor**, select your **web/apache instances** :doc:`flavor </compute/flavors>`.
  We recommend sticking with the default.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


===============
MySQL / MariaDB
===============

`MySQL <https://www.mysql.com>`__ is a popular open source relational
database. `MariaDB <https://mariadb.org>`__ is an alternative database
that forked from MySQL that is similar but has diverged a
bit through the years.

Binero cloud enables you to provision either MySQL or MariaDB (or both)
as a standalone installation or with replication.

If you select with replication, an auxiliary instance will be setup that
will copy all writes. You can use this server for database reads to reduce
the load on the primary instance and in case of an outage of the primary
server, use it as the new primary server. 

To setup the service, first follow the general instructions on our
:doc:`index` page.

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>` you
  want to provision in. We recommend the *europe-se-1a* availability zone.

- If you want backup, check the **Backup** checkbox and select an amount of days you
  want your history stored.

- Check **Database dump** checkbox if you want periodic dumping of data to the file system
  of the instance. This is good in conjunction with backup as it will guarantee the database
  integrity in case of a need to restore, we recommended that you enable this.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking with the default.

- Select disk-type. See the :doc:`/storage/storage-types` article for more information.

- Select your :doc:`SSH-key </compute/ssh-keys>`. 

- Under **Local network**, select the :doc:`network </networking/network/index>`
  on which you want to run the service.

- Check the **Replication** checkbox if you want to have a auxiliary instance
  setup (see above).

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


=======
OpenVPN
=======

If you need access via a client VPN to your project, we recommend using the
OpenVPN service from our catalog.

This template will install `pfSense <https://www.pfsense.org>`__ and set it
up as a fully fledged VPN suite which you can use to access your account.

More information on the service and how its setup is available in
our :doc:`/networking/client-vpn/index` section.

..  seealso::

  - :doc:`index`


==========
PostgreSQL
==========

`PostgreSQL <https://www.postgresql.org>`__ is a popular open source relational database. 

Binero cloud enables you to provision PostgreSQL as a standalone installation or with
replication.

If you select with replication, a auxiliary instance will be setup that will copy all
writes. This server is good for database reads (to reduce load on the primary server) and
in case of an issue with the primary server, the auxiliary instance can become the new
primary.

- Give your service a name and optionally a description.

- Select your :doc:`SSH-key </compute/ssh-keys>`.

- Under **Secondary node availability zone** dropdown, you are able to select which availability
  zone to place the auxiliary node in (if you choose to replicate). You can choose to use the same
  zone as the primary node or you can select another zone if you want to build a geographically
  distributed application. 

- Under **Alternate network**, you select the :doc:`network </networking/network/index>`
  that the auxiliary server should connected to.

- Select your instance :doc:`flavor </compute/flavors>`. We recommend sticking with the default.

- Select the volume size for the instance. You can extend this volume later, see the
  :doc:`/storage/persistent-block-storage/extend-volume` article.

- Under **Primary node availability zone**, select the availability zone to run your primary instance in. 

- Under **Primary network**, you select the :doc:`network </networking/network/index>` that
  the primary server should connected to.

- Under **Secondary nodes**, you select the amount of auxiliary servers you want. If you select zero, no
  replication will be setup.

- Press **Create**. You will get further details on how to connect to the service. 

..  seealso::

  - :doc:`index`


===============
Private Network
===============

If you want to provision a new :doc:`network </networking/network/index>`, this convenience
function will help you.

More information on how to get started with networking is in our
:doc:`/getting-started/launching-an-instance` guide. 

To setup the service, first follow the general instructions on our
:doc:`index` page. Then follow these instructions: 

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>` you want to
  provision in. We recommend *europe-se-1a* if you are unsure.

- Select a :doc:`subnet range </networking/subnet/subnet-format>` to use.

- Leave remaining items and press **Create**

..  seealso::

  - :doc:`index`


=====
Redis
=====

`Redis <https://redis.com>`__ is a versatile in-memory data structure store.

To setup the service, first follow the general instructions on our :doc:`index` page.

- Give your service a name and optionally a description.

- Select which :doc:`availability zone </compute/regions-and-availability-zones>` you
  want to provision in. We recommend using the *europe-se-1a* availability zone.

- If you want backup, check the **Backup** checkbox and select an amount of days you
  want your history stored.

- Select your instance :doc:`flavor </compute/flavors>`.

- Under ``private_net_name``, select the :doc:`network </networking/network/index>` you
  want to use.

- Under ``private_subnet_name``, select the :doc:`subnet </networking/subnet/index>` on
  which you want to run the service.

- Press **Create**. You will get further details on how to connect to
  the service. 

..  seealso::

  - :doc:`index`


===============
Schedule Backup
===============

Using our service catalog, you can setup a recurring backup job for one ore more of
your :doc:`block storage volumes </storage/persistent-block-storage/index>`.

The backup will be ran using our :doc:`/platform-automation/index` tool but using this to
setup recurring backups can be a complex task. The service catalog greatly simplified this.

To setup the service, first follow the general instructions on our :doc:`index` page.

Then follow these instructions: 

- Give your service a name and optionally a description.

- In the **Cron in GMT** field you decide when and how often the job should run. You can use
  hourly, daily or weekly as base. Select the one that is closest to what you want to do. If
  you for example want to run a backup job once per night, select **daily** and then set the
  **Hour(s)** field to the hour you want to run the job.

- In **Age of backup deletion in days** you select how many historical backups you want to
  keep. If you keep the default value of 10, when creating backups, previous backups that
  are older than 10 days gets deleted. 

- In the **Volume** field, select the volume that you want to backup.

- Press **Launch** when done. 

..  seealso::

  - :doc:`index`


=================
Schedule Snapshot
=================

Using our service catalog, you can setup a recurring :doc:`snapshot </storage/snapshots/index>` job
for one ore more of your :doc:`block storage volumes </storage/persistent-block-storage/index>`.

The snapshot creation is using our :doc:`/platform-automation/index` tool but using this to setup
recurring snapshots can be a complex task. The service catalog greatly simplifies this.

To setup the service, first follow the general instructions on our :doc:`index` page.

Then follow these instructions: 

- Give your service a name and optionally a description.

- In the **Cron in GMT** field you decide when and how often the job should run. You can use hourly, daily
  or weekly as base. Select the one that is closest to what you want to do. If you for example want to run
  a snapshot job once per night, select **daily** and then set the **Hour(s)** field to the hour you want to run
  the job.

- In the **Age of snapshot deletion in hours** you select how many historical copies you want to keep. If you keep
  the default value of 72, when taking snapshots, previous snapshots that are older than 72 hours would get deleted. 

- In the **volume** field, select the volume that you want to backup.

- Press **Launch** when done. 

..  seealso::

  - :doc:`index`


===================
Platform automation
===================

By using Binero clouds platform automation suite, you are able to automate and schedule
different features in the platform.

For a good getting started resource, we recommend `the official documentation <https://docs.openstack.org/mistral/latest/user/overview.html>`_.


======
Images
======

The image service in Binero cloud allows you to store and retrieve virtual machine images that
you can populate a :doc:`volume </storage/persistent-block-storage/index>` or boot an
:doc:`instance </compute/index>` from.

The pricing for uploaded images is per gigabyte hour for the combined used space for all your
private images. See our pricing list for more details.

Public images
-------------

Binero cloud provides images of popular virtual machine images and operating systems from
official sources that are publicly available in the platform so that you can get started quickly.

All images uses :doc:`cloud-init <cloud-init>`. Images is always used as a source for volumes
or instances and the images themselves are never written to.

Private images
--------------

You can upload and store images that only you have access to on your account. You can create a
image from an existing :doc:`instance </compute/index>`, this can be useful if you already have
a certain configuration that you want to re-use.

You can bring any image that you want to use, for example from other cloud providers where you
already have a image that you want to use in Binero cloud as well.

Uploading an image
^^^^^^^^^^^^^^^^^^

.. note::

   Customers can only upload private images and cannot set image visibility to public
   or community as that would open up a security vulnerability for malicious images.

To upload an image by using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Press **Compute** and then **Images** in the sidebar menu

- Click the plus sign (+) in the lower right corner

- Specify a image name

- Select the disk format that the image has

- Select a operating system and version that matches what your image contains

- Set the architecture to x86_64 as this is the only supported architecture

- Set the visibility to private

- You can upload a image either using an URL that's downloaded or
  from a file on your computer, select what you prefer.

- You can leave any remaining fields empty unless you want to fill them
  in as well.

  - If you want to prevent your image from being accidentally deleted you
    can set the image as protected which means you need to un-protect it
    before you can delete the image.

- Click **Create Image**

Creating image from instance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

You can create a private image from an existing instance that is available in the cloud
with the snapshot instance feature.

See the :doc:`/storage/snapshots/create-snapshot` documentation that details creating
an instance snapshot that you can then upload as an image.

Create instance from image
--------------------------

To create a new compute instance from an image, see the :doc:`launch-instance-from-image`
documentation.

.. toctree::
  :caption: Available services
  :maxdepth: 1

  launch-instance-from-image
  create-rescue-image
  cloud-init


==========================
Launch instance from image
==========================

When :doc:`snapshotting </storage/snapshots/index>` and entire instance, the end result is
an :doc:`image <index>`. Restoring from a snapshot (assuming you restore the entire instance) is
then the same as creating an instance from a private image.

When restoring an entire instance from a snapshot, which might be the case if for example an
upgrade was unsuccessful post snapshot and you want to revert to the previous state, you would
create a new instance from the image.

This is a good method because it allows you to safely ensure that your new instance is up and
running before deleting the old one (and not doing major changes to the only copy of the image).

.. note:: While there is a feature in the cloud management portal to do an in place restore to
          the current instance, we *do not recommend it*, but rather that you follow below guide
          to create a new instance from the image. 

See the below guides from creating an instance from a private image.

Cloud management portal
-----------------------

To create a snapshot using the :doc:`/getting-started/managing-your-cloud/cloud-management-portal`

- Click **Compute** and then **Images** in the sidebar menu

- On the image that you want to use as base, press the small arrow **Create from image**

- Enter a name

- Select a :doc:`flavor </compute/flavors>` in the **Configuration** field

- Choose a :doc:`availability zone </compute/regions-and-availability-zones>`

- If applicable choose a :doc:`SSH key </compute/ssh-keys>`

- Choose a :doc:`subnet </networking/subnet/index>`

- Click **Create**

OpenStack Horizon
-----------------

To create a snapshot using :doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **Compute** and then **Images** in the sidebar menu

- Press **Launch** to the right on the row of the image

- Follow the instructions in our :doc:`/compute/launching-an-instance/openstack-horizon` guide

OpenStack Terminal Client
-------------------------

To create a snapshot using the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- List the images and save the name of the image you want to create a new
  instance from

::

    openstack image list --private

- Follow the instructions in our :doc:`/compute/launching-an-instance/openstack-terminal-client` guide
  replacing the ``--image`` value from the public image in the example to your private image

..  seealso::

    - :doc:`index`


==========
Cloud-init
==========

`Cloud-init <https://cloudinit.readthedocs.io/en/latest/>`_ is the industry standard
multi-distribution method for cross-platform cloud instance initialization.

It's supported across all major public cloud providers, provisioning systems for private
cloud infrastructure, and bare-metal installations.

During boot, cloud-init identifies the cloud it's running on and initialises the system
accordingly. Cloud instances will automatically provision during first boot with networking,
storage, SSH keys, packages and other system aspects already configured.

Cloud-init is powerful and users can send their own cloud-user data to cloud-init to
customized configuration. See the `examples here <https://cloudinit.readthedocs.io/en/latest/reference/examples.html>`_.

..  seealso::

    - :doc:`index`


===================
Create rescue image
===================

To :doc:`boot your instance from a rescue image </compute/boot-from-rescue>` you will
need to first have a rescue image available.

A rescue image can be an operating system installation media, Live-CD that boots an
entire operating system from an ISO file or any other rescue media

Because you need to set custom properties on the rescue image you can only
the :doc:`/getting-started/managing-your-cloud/openstack-terminal-client` or
:doc:`/getting-started/managing-your-cloud/openstack-horizon`.

OpenStack Terminal Client
-------------------------

This will guide you through creating a rescue image by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Download the rescue media ISO to your computer

- Upload it as a new image

.. note::

   Replace name and file with the correct information in below command.

::

    openstack image create [NAME] --property hw_rescue_device=cdrom --property hw_rescue_bus=scsi --disk-format iso --file [FILENAME] --private --progress

Your can now use your rescue image, make a note of the image name.

.. note::

   You're charged per hour for the uploaded image, make sure to delete
   the image when you're done to save on the hourly cost for the image.

..  seealso::

    - :doc:`index`


===
DNS
===

General concept
---------------

Using our DNS feature, you can use the platform to control your DNS (domain) zones.

To start using the feature, first add one or more zones, add the records (A, MX and
so on) and finally change the NS records at your DNS registrar for your domain to our
DNS servers.

- ``ns.binero.eu``

- ``ns.binero.se``

- ``ns.binero.com``

The DNS registrar is the company you pay to register and renew your domain name. **Binero
does not have DNS registrar services**.

.. note::

   The DNS name servers provided by Binero (that is, the servers that answer the DNS
   queries) are not hosted with Binero but with `Netnod <https://www.netnod.se/dns/dns-anycast>`__.

   Netnod has been providing anycast DNS services for more than 20 years and also hosts one
   of the worlds DNS root name servers.

Standard anycast
----------------

By default the DNS service provides standard anycast for your DNS zone. This provides you
with anycast DNS in the nordic region and Stockholm.

Premium anycast
---------------

If you want a global DNS service we also provide premium anycast that gives you global anycast
DNS with locations all around the world, if you want to use our premium anycast, contact our
support and we will help you get started.

.. tip::

   By using our premium anycast DNS service you can lower the response time of DNS queries
   to your DNS zones from all around the world. Contact our support to get started.

Create a zone
-------------

To create a zone, see the guide below to the system you want to use.

Cloud management portal
^^^^^^^^^^^^^^^^^^^^^^^

To setup an DNS zone by using the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

- Press **DNS** and then **DNS zones** in the sidebar menu.

- Press the **+** (plus) icon in the lower right corner.

- Input your domain name in the **Domain** field. 

- Enter your email address in the email field. This is for notifications.

- Optionally enter a description.

- In the IP-address field, add an IP-address that you want your `www.domain.com` to point to (among
  other things). If you are unsure, leave this field blank. 

To edit a DNS zone by using the
:doc:`cloud management portal </getting-started/managing-your-cloud/cloud-management-portal>`

- Press **DNS** and then **DNS zones** in the sidebar menu.

- Press the icon / row on the zone you want to edit. 

- To add a record, press the small **+** (plus) symbol at the bottom and edit the row that will appear on the end. 

- To edit a record, press the record name.

- When done, press **Save all changes**

.. note::

   To edit records for the domains origin (for example an MX record), use the domain itself
   followed by a ``.`` (dot) as subdomain.

OpenStack Horizon
^^^^^^^^^^^^^^^^^

To setup an DNS zone by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **DNS** and then **Zones** in the sidebar menu.

- Click **Create zone** in the right upper corner.

- Input your domain name in the **Name** field. 

- Optionally enter a description.

- Enter your email address in the email field. This is for notifications.

- Press **Submit**

To edit a DNS zone by using
:doc:`/getting-started/managing-your-cloud/openstack-horizon`

- Under **Project**, click **DNS** and then **Zones** in the sidebar menu.

- Press the name of the zone you want to edit. 

- Press the **Record sets** tab.

- To add a record, press the **Create record set** button to the top right. 

- To edit a record, press **Update** next to the right on the row of the record.

OpenStack Terminal Client
^^^^^^^^^^^^^^^^^^^^^^^^^

To setup an DNS zone by using
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack zone create --email [EMAIL_ADDRESS] example.com.``, replacing the
  email address with your email and ``example.com.`` with your domain. Note the dot (``.``) at
  the end of the domain name. 

To add a DNS zone record by using the
:doc:`/getting-started/managing-your-cloud/openstack-terminal-client`

- Run this command: ``openstack zone list``, save the UUID from the zone that you want to add/list
  records on. 

- Run this command: ``openstack recordset list [ID]``, replacing the ID with the ID from previous step.

- Run this command: ``openstack recordset create --record [TARGET IP] --type [POINTER TYPE] [DOMAIN ID] [SUBDOMAIN]``, replacing
  the items in angle brackets with correct values. Pointer Type is for example ``A`` or ``MX``, target IP might be a CNAME (if relevant). 


=========
OpenStack
=========

Binero cloud runs on `OpenStack <https://www.openstack.org>`_ which is an umbrella ecosystem of
open source software that you can use together to build public and private cloud infrastructure.

Release
-------

Binero cloud is running the **Caracal** coordinated release of OpenStack. We strive to keep updated
and our goal is to never fall more than two releases behind the newest release and at the same time
not introducing software not yet considered production ready.

Official documentation
----------------------

The official OpenStack documentation is a good place to get detailed information about the services
and their APIs.

- https://docs.openstack.org/2024.1/index.html (documentation)
- https://docs.openstack.org/2024.1/api/ (API documentation)
- https://releases.openstack.org/caracal/ (release notes)
- https://releases.openstack.org/caracal/highlights.html (highlights)


=============
OpenStack API
=============

The OpenStack API is available for managing or performing actions on the resources
in your cloud account.

This is a powerful feature for users that either want to create infrastructure as
code projects or want to integrate with third-party applications to manage your
infrastructure.

The OpenStack API consists of many individual APIs based on what service it provides.

They have in common that an :doc:`API user </getting-started/users>` needs to be available
for authentication to access the APIs on any of the below endpoints.

We keep updated information about the release of the platform along with links to correct APIs
available in our :doc:`OpenStack <openstack>` documentation.

- Function - what services in the platform the API can manage with a link to its section in
  our documentation.

- Service name - the name of the OpenStack feature. With this you can find the relevant section
  in the official API docks.

- API endpoint - the information on where to connect to consume the APIs.

Version
-------

Some of the OpenStack APIs will support different versions for backwards compatibility.

Generally the recommended version is in the URL (where applicable) below but in certain
cases you need to use a certain microversions (for example v2.1 compared to v2.67 for
Nova) for some functionality.

Keystone
--------

Keystone is the identity service in the OpenStack ecosystem that you use to authenticate
using an :doc:`API user </getting-started/users>`.

A good place to start is available in the official
documentation `here <https://docs.openstack.org/keystone/latest/api_curl_examples.html>`_.

You need to authenticate for all API calls, when authenticating for your token a catalog
with endpoints is also returned in the response, for verbosity it's also included below.

.. _openstack-api-endpoints:

Endpoints
---------

.. list-table::
   :widths: 25 25 50
   :header-rows: 1

   * - Function
     - Service name
     - URI

   * - `Compute </compute>`_
     - ``nova``
     - https://api-eu-se-1.binero.cloud:8774/v2.1

   * - `Networking </networking>`_
     - ``neutron``
     - https://api-eu-se-1.binero.cloud:9696

   * - `Block storage </storage/persistent-block-storage>`_
     - ``cinderv3``
     - https://api-eu-se-1.binero.cloud:8776/v3/%(tenant_id)s

   * - `Identity </getting-started/users>`_
     - ``keystone``
     - https://auth.binero.cloud:5000

   * - `DNS </dns>`_
     - ``designate``
     - https://api-eu-se-1.binero.cloud:9001

   * - `Object storage </storage/swift-object-storage>`_
     - ``swift``
     - https://object-eu-se-1a.binero.cloud/swift/v1/AUTH_%(tenant_id)s

   * - `Workflows </platform-automation>`_
     - ``mistral``
     - https://api-eu-se-1.binero.cloud:8989/v2

   * - `Images </images>`_
     - ``glance``
     - https://api-eu-se-1.binero.cloud:9292

   * - Metric
     - ``gnocchi``
     - https://api-eu-se-1.binero.cloud:8041

   * - Alarming
     - ``aodh``
     - https://api-eu-se-1.binero.cloud:8042

   * - `Load Balancer </networking/load-balancer>`_
     - ``octavia``
     - https://api-eu-se-1.binero.cloud:9876

   * - `Orchestration </orchestration>`_
     - ``heat``
     - https://api-eu-se-1.binero.cloud:8004/v1/%(tenant_id)s

   * - `Secret store / Key manager </secret-store>`_
     - ``barbican``
     - https://api-eu-se-1.binero.cloud:9311

..  seealso::

  - :doc:`/getting-started/users`


=============
Orchestration
=============

Binero cloud fully supports orchestration with Heat. By using Heat, you can provision resources
from most parts of the platform via YAML based templates.

Aside from producing a tested and speedy roll out with a predictable result, automating in this
manner gives you the ability to setup identical platforms, either to scale out or to move between
providers that support the same technology.

Using Heat, you use the Infrastructure as Code concept in which you can check in and version
manage the templates for your system and manage changes by updating the template of a stack.

While a comprehensive documentation of Heat is outside the scope of this article, see the
`official documentation <https://docs.openstack.org/heat/latest/>`__ for more information
about Heat.

Binero provides access to a git repository with tested Heat templates.

See `our templates on GitHub <https://github.com/binerogroup/heat-templates>`__ for inspiration
on how to work with Heat!

.. toctree::
  :caption: Services available
  :maxdepth: 1



======
Guides
======

On this page we provide some guides to get started with some
use-cases in the platform. 

.. toctree::
  :caption: Getting started
  :maxdepth: 1

  designate-as-authenticator-for-lets-encrypt
  kubernetes-on-openstack-using-rke
  manage-openstack-resources-using-terraform


===========================================
Designate as authenticator for Lets Encrypt
===========================================

This guide will help you configuring ``certbot`` to work with
plugins for OpenStack DNS (Designate) to verify Lets Encrypt
certificates by using DNS for any domain that is running in the
Binero cloud :doc:`DNS </dns>`.

The guide will assume you are using a recent version of Debian or
Ubuntu but would likely be applicable for other Linux distributions.

- Install ``certbot`` and python package manager pip by
  running ``sudo apt install certbot python3-designateclient python3-pip``. 

- Using pip install the ``dns-openstack`` authenticator plugin by running:

::

    sudo pip3 install certbot-dns-openstack

- Create an :doc:`application credential </getting-started/users>` with the
  roles ``reader`` and ``member``. Use the following in the access rules:

::

  - service: dns
    method: GET
    path: /v2/zones
  - service: dns
    method: GET
    path: /v2/zones/{zone_id}
  - service: dns
    method: GET
    path: /v2/zones/{zone_id}/recordsets
  - service: dns
    method: POST
    path: /v2/zones/{zone_id}/recordsets
  - service: dns
    method: GET
    path: /v2/zones/{zone_id}/recordsets/{recordset_id}
  - service: dns
    method: PUT
    path: /v2/zones/{zone_id}/recordsets/{recordset_id}
  - service: dns
    method: DELETE
    path: /v2/zones/{zone_id}/recordsets/{recordset_id}

- Download the application credential as YAML and save it on your server
  to the following path: ``/etc/openstack/clouds.yaml``

- Set the permission on the file by running ``sudo chown root:root /etc/openstack/clouds.yaml; chmod 600 /etc/openstack/clouds.yaml``.

- You are now able to issue certificates as you would normally do with the
  exception of adding ``-a dns-openstack`` to the command line, for example
  as such: ``sudo certbot -a dns-openstack certonly -d example.domain``.

- To change the authentication of any existing certificates you would
  edit ``/etc/letsencrypt/renewal/example.domain.conf`` and set the
  authentication option to ``dns-openstack``.


=================================
Kubernetes on OpenStack using RKE
=================================

RKE is a CNCF-certified Kubernetes distribution that runs entirely in Docker
containers. RKE solves the problem of installation complexity, a common problem
in the Kubernetes community.

Provided you are able to run a supported version of Docker, you can deploy and
run Kubernetes with RKE.

RKE supports management on different cloud providers by using provider plugins. The
OpenStack plugin interacts with the many resources supported by OpenStack and deploys
Kubernetes on OpenStack instances, the plugin needs credentials to work.

Installing RKE
--------------

We recommend following the `the official installation-guide <https://rancher.com/docs/rke/latest/en/installation/#download-the-rke-binary>`__
to install RKE on your workstation.

Preparing the nodes
-------------------

Create the virtual instances that should run as Kubernetes nodes. It's a prerequisite
that you have installed Docker on all instances.

For a full list of prerequisites and node preparations, follow the steps in
the `official requirements guide <https://rke.docs.rancher.com/os#operating-system>`__.

In this example we create two instances where rancher-node-1 is control and etcd node and
rancher-node-2 is the worker node.

Run the following commands to install docker on both instances:

- ``curl https://releases.rancher.com/install-docker/20.10.sh > install.sh``

- ``sh install.sh``

- ``sudo usermod -aG docker $USER``

After re-logging into the instances, that is closing the SSH session and opening it
again (to enable the environment), you should now be able to run ``docker ps`` which
should work you've installed Docker.

Configure the OpenStack plugin
------------------------------

Follow these instructions to prepare the plugin on your workstation:

* Create a catalog called ``demo`` and an empty file called ``cluster.yml`` in the same catalog.

* Run the command ``rke config -name cluster.yml`` and follow the instructions. When you get to the step
  **override hostname** enter the names of the instances you created in the previous steps.

* Edit the ``cluster.yml`` file. Under the section **cloud provider** enter the correct parameters (more info
  `here <https://rke.docs.rancher.com/config-options/cloud-providers/openstack>`__. IDs and names are available
  in the platform using any of the management tools. The finished file looks like this:

::
	
  # Configure the OpenStack Provider
   cloud_provider:
     name: "openstack"
     openstackCloudProvider:
       global:
         username: 'demo-user'
         password: 'demopass'
         auth-url: https://auth.binero.cloud:5000/v3
         tenant-name: 'demo-project'
         domain-name: 'Default'
         region: 'europe-se-1'
       load_balancer:
         use-octavia: true
         subnet-id: demo-subnet-id
         # Floating network: europe-se-1-1a-net0
         floating-network-id: 35164b56-13a1-4b06-b0e7-94c9a67fef7e
       block_storage:
         ignore-volume-az: false
       route:
         router-id: demo-router-id

- When done, run the command ``rke up`` which will install the cluster. 

- Two new files will be in the demo directory after we run this command, cluster.rkestate
  and kube_config_cluster.yml.

- Use the file ``kube_config_cluster.yml`` with kubectl to check cluster health and perform actions
  for example: ``kubectl --cubeconfig=kube_config_cluster.yml get pods -A``.

Create a Persistent Volume Claim with Cinder Service
----------------------------------------------------

Follow these instructions to create a persistent volume claim via the OpenStack Cinder API. 

- Export kube_config_cluster.yml to a KUBECONFIG environment
  variable with ``export KUBECONFIG=kube_config_cluster.yml``. 

- Create a StorageClass YAML file ``storageclass.yaml`` based on an SSD volume in availability
  zone europe-se-1a with the following content: 

::

   kind: PersistentVolumeClaim
   apiVersion: v1
   metadata:
     name: cinder-claim
     annotations:
       volume.beta.kubernetes.io/storage-class: "ssd-demo"
   spec:
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 1Gi

- Run the command: ``kubectl create -f pvc.yaml`` to create.

- You can get information about your PersistentVolumeClaim by running the command ``kubectl get pvc``.


==========================================
Manage OpenStack resources using Terraform
==========================================

Terraform is a popular tool that automates infrastructure in platforms.

It has an OpenStack module and as such is compatible with Binero cloud. In this guide, we
will walk through some concepts that enable you to use it with the platform. 

Prepare Terraform
-----------------

The official installation guide is available here: `here <https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli>`__. 

To get Terraform speaking with Binero cloud, you will need to configure the
OpenStack provider.

`Here <https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs>`__ is
a good place to get information on how to do that. 

Start by creating an :doc:`API user </getting-started/users>` and download the ``openrc`` file (by
pressing the small arrow next to it).

You need the credentials in this file to configure the Terraform provider. An example configuration
file (called for example ``demo.tf``) could look like this: 

:: 

    # Define required providers
    terraform {
      required_version = ">= 0.14.0"
      required_providers {
        openstack = {
          source  = "terraform-provider-openstack/openstack"
          version = "~> 1.51.1"
        }
      }
    }

    # Configure the OpenStack Provider
    provider "openstack" {
      user_name     = "api-user"
      tenant_name   = "api-user"
      password      = "pwd"
      auth_url      = "https://auth.binero.cloud:5000/v3"
      region        = "europe-se-1"
      endpoint_type = "public"
    }

Working with Binero.Cloud through Terraform
-------------------------------------------

Covering Terraform in-depth is out-of-scope for this guide. Generally speaking, Terraform will
support data sources (information about whats available in the cloud) and resources (objects managed
in the cloud).

Below is an example of an instance setup in the cloud: 

::

    data "openstack_compute_flavor_v2" "demoflavor" {
      name = "gp.1x2"
    }
    
    data "openstack_images_image_v2" "centos8" {
      name = "centos-8-x86_64"
    }
    
    resource "openstack_compute_instance_v2" "demo" {
      name            = "demo"
      image_id        = data.openstack_images_image_v2.centos8.id
      flavor_id       = data.openstack_compute_flavor_v2.demoflavor.id
      key_pair        = "user-key"
      security_groups = ["default"]
      metadata = {
        this = "that"
      }
      network {
        name = "europe-se-1-1a-net0"
      }
    }

Creating the above example in the same demo.tf file from before and
running ``terraform apply`` will create the instance. 

More information on what the OpenStack provider can do is
available `here <https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs>`__. 

